<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Attention Is All You Need â€” äº¤äº’å¼é˜…è¯»æŒ‡å—</title>
<style>
*{margin:0;padding:0;box-sizing:border-box;}
body{font-family:-apple-system,"Noto Sans SC","PingFang SC",sans-serif;background:#f1f5f9;height:100vh;overflow:hidden;display:flex;}

/* LEFT: Paper viewer */
.paper-pane{width:42%;height:100vh;background:#525659;overflow-y:auto;position:relative;flex-shrink:0;}
.paper-pane::-webkit-scrollbar{width:8px;}
.paper-pane::-webkit-scrollbar-thumb{background:#888;border-radius:4px;}
.paper-hint{color:#94a3b8;text-align:center;padding:40px 20px;font-size:14px;line-height:1.8;}
.page-wrapper{position:relative;margin:8px auto;width:95%;}
.page-wrapper img{width:100%;display:block;border-radius:4px;box-shadow:0 2px 8px rgba(0,0,0,0.3);}
.page-label{position:absolute;top:8px;right:12px;background:rgba(0,0,0,0.5);color:#fff;font-size:11px;padding:2px 8px;border-radius:4px;z-index:2;}
.highlight-overlay{position:absolute;left:0;width:100%;border:3px solid;border-radius:6px;pointer-events:none;z-index:1;transition:all 0.3s;}

/* RIGHT: Guide */
.guide-pane{flex:1;height:100vh;overflow-y:auto;padding:20px 24px;background:linear-gradient(135deg,#f8fafc 0%,#f0f4ff 50%,#faf5ff 100%);}
.guide-pane::-webkit-scrollbar{width:8px;}
.guide-pane::-webkit-scrollbar-thumb{background:#cbd5e1;border-radius:4px;}

.header{text-align:center;margin-bottom:16px;}
.header h1{font-size:22px;font-weight:800;color:#1e293b;}
.header p{color:#94a3b8;font-size:12px;margin-top:4px;}

.map-box{background:#fff;border-radius:14px;border:1px solid #e2e8f0;padding:12px;margin-bottom:16px;position:relative;height:360px;box-shadow:0 1px 3px rgba(0,0,0,0.04);}
.map-svg{position:absolute;inset:0;width:100%;height:100%;z-index:0;}
.node-btn{position:absolute;transform:translate(-50%,-50%);z-index:1;background:none;border:none;cursor:pointer;transition:all 0.2s;}
.node-chip{border-radius:14px;padding:7px 14px;box-shadow:0 1px 4px rgba(0,0,0,0.08);display:flex;align-items:center;gap:6px;border:2px solid;transition:all 0.2s;font-size:12px;font-weight:600;white-space:nowrap;}
.node-chip .emoji{font-size:14px;}
.node-btn:hover .node-chip{transform:scale(1.05);}

.detail-panel{border-radius:14px;border:1px solid;box-shadow:0 4px 20px rgba(0,0,0,0.06);overflow:hidden;margin-bottom:16px;background:#fafbfd;animation:slideIn 0.25s ease-out;}
@keyframes slideIn{from{opacity:0;transform:translateY(10px);}to{opacity:1;transform:translateY(0);}}
.detail-header{padding:16px 20px;display:flex;justify-content:space-between;align-items:center;}
.detail-header .info h2{font-size:18px;font-weight:700;display:flex;align-items:center;gap:6px;}
.detail-header .info .section{font-size:11px;color:#94a3b8;margin-top:2px;}
.detail-header .close-btn{font-size:24px;color:#94a3b8;background:none;border:none;cursor:pointer;padding:0 4px;line-height:1;}
.detail-body{padding:0 20px 20px;}
.key-question{border-radius:10px;padding:12px 14px;margin-bottom:14px;}
.key-question .label{font-size:12px;font-weight:600;}
.key-question .text{font-size:13px;color:#334155;margin-top:3px;line-height:1.6;}
.content-area{color:#475569;}
.content-area h3{font-size:14px;font-weight:700;color:#1e293b;margin:14px 0 4px;}
.content-area p{font-size:13px;line-height:1.7;margin-bottom:4px;}
.content-area .bullet{display:flex;gap:6px;font-size:13px;margin-left:6px;margin-bottom:3px;line-height:1.6;}
.content-area .bullet .dot{color:#94a3b8;margin-top:1px;flex-shrink:0;}
.content-area .step-label{font-size:13px;font-weight:600;color:#1e293b;margin-top:8px;}
.content-area strong{color:#1e293b;font-weight:600;}
.think-box{background:#fffbeb;border:1px solid #fde68a;border-radius:10px;padding:12px 14px;margin-top:16px;}
.think-box .title{font-size:12px;font-weight:600;color:#b45309;margin-bottom:8px;}
.qa-question{cursor:pointer;display:flex;gap:6px;align-items:flex-start;padding:6px 0;border-radius:6px;transition:background 0.15s;}
.qa-question:hover{background:rgba(245,158,11,0.08);}
.qa-question .arrow{color:#b45309;flex-shrink:0;font-size:11px;margin-top:3px;width:14px;}
.qa-question .qtext{color:#92400e;font-size:13px;line-height:1.6;}
.qa-detail{margin:0 0 10px 18px;animation:slideIn 0.2s ease-out;}
.qa-block{padding:10px 12px;border-radius:0 8px 8px 0;margin-bottom:8px;border-left:3px solid;}
.qa-block .block-label{font-size:10px;font-weight:700;margin-bottom:4px;text-transform:uppercase;letter-spacing:0.5px;}
.qa-block .block-text{font-size:12px;line-height:1.7;margin:0;}
.paper-ref{margin-top:10px;font-size:11px;color:#94a3b8;font-style:italic;}
.empty-hint{text-align:center;padding:24px;color:#94a3b8;font-size:13px;}

/* Divider handle */
.divider{width:5px;background:#cbd5e1;cursor:col-resize;flex-shrink:0;transition:background 0.2s;}
.divider:hover{background:#94a3b8;}
</style>
</head>
<body>
<div class="paper-pane" id="paperPane">
  <div class="paper-hint" id="paperHint">
    â† é€‰æ‹©å³ä¾§æ¦‚å¿µèŠ‚ç‚¹å<br>è¿™é‡Œä¼šè·³è½¬åˆ°å¯¹åº”çš„è®ºæ–‡åŸæ–‡<br>å¹¶é«˜äº®æ ‡å‡ºç›¸å…³ç« èŠ‚
  </div>
</div>
<div class="divider" id="divider"></div>
<div class="guide-pane" id="guidePane">
  <div class="header">
    <h1>Attention Is All You Need</h1>
    <p>ç‚¹å‡»æ¦‚å¿µèŠ‚ç‚¹ â†’ å·¦ä¾§è·³è½¬åŸæ–‡å¹¶é«˜äº® Â· å³ä¾§å±•å¼€è®²è§£</p>
  </div>
  <div class="map-box" id="mapBox">
    <svg class="map-svg" id="mapSvg"></svg>
  </div>
  <div id="detailContainer"></div>
  <div class="empty-hint" id="emptyHint">ç‚¹å‡»ä¸Šæ–¹æ¦‚å¿µèŠ‚ç‚¹ï¼Œå¼€å§‹æ¢ç´¢</div>
</div>

<script>
const TOTAL_PAGES = 15;
const nodes = [
  {
    "id": "problem",
    "label": "é—®é¢˜èƒŒæ™¯",
    "emoji": "ğŸ¯",
    "color": "#ef4444",
    "x": 50,
    "y": 8,
    "section": "ç¬¬1-2èŠ‚ï¼šIntroduction & Background",
    "pages": [
      1,
      2
    ],
    "scrollTo": 2,
    "highlights": [
      {
        "page": 1,
        "top": 35,
        "height": 60
      },
      {
        "page": 2,
        "top": 5,
        "height": 90
      }
    ],
    "keyQuestion": "RNN åˆ°åº•æœ‰ä»€ä¹ˆé—®é¢˜ï¼Œè®©ä½œè€…æƒ³è¦å®Œå…¨æŠ›å¼ƒå®ƒï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "æ—¶ä»£èƒŒæ™¯ï¼š2017å¹´çš„åºåˆ—å»ºæ¨¡"
      },
      {
        "type": "p",
        "text": "2017å¹´ä¹‹å‰ï¼Œåºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰ä»»åŠ¡çš„ä¸»æµèŒƒå¼æ˜¯ã€ŒEncoder-Decoder + Attentionã€ï¼Œå…¶ä¸­ Encoder å’Œ Decoder å‡ç”± RNN å˜ä½“ï¼ˆLSTM/GRUï¼‰æ„æˆã€‚Google çš„ GNMTï¼ˆ2016ï¼‰ã€Facebook çš„ ConvS2Sï¼ˆ2017ï¼‰åˆ†åˆ«ä»£è¡¨äº†å¾ªç¯å’Œå·ç§¯ä¸¤æ¡æŠ€æœ¯è·¯çº¿çš„å·…å³°ã€‚"
      },
      {
        "type": "h3",
        "text": "RNN çš„æ ¹æœ¬çŸ›ç›¾ï¼šåºåˆ—æ€§ vs å¹¶è¡Œæ€§"
      },
      {
        "type": "p",
        "text": "RNN çš„éšçŠ¶æ€æ›´æ–°å…¬å¼ä¸º <strong>h<sub>t</sub> = f(h<sub>t-1</sub>, x<sub>t</sub>)</strong>ï¼Œæ¯ä¸€æ­¥éƒ½ä¾èµ–å‰ä¸€æ­¥çš„è¾“å‡ºã€‚è¿™æ„å‘³ç€å¯¹äºé•¿åº¦ä¸º n çš„åºåˆ—ï¼Œè®¡ç®—å¿…é¡»ä¸²è¡Œæ‰§è¡Œ n æ­¥ï¼Œæ— æ³•åˆ©ç”¨ GPU çš„å¤§è§„æ¨¡å¹¶è¡Œèƒ½åŠ›ã€‚è®ºæ–‡æŒ‡å‡ºè¿™ä¸€ç‚¹ã€Œåœ¨è¾ƒé•¿åºåˆ—é•¿åº¦ä¸‹å°¤ä¸ºå…³é”®ã€ï¼ˆåŸæ–‡ï¼šbecomes critical at longer sequence lengthsï¼‰ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸‰ä¸ªæ ¸å¿ƒç—›ç‚¹"
      },
      {
        "type": "bullet",
        "text": "<strong>è®¡ç®—æ— æ³•å¹¶è¡Œ</strong>ï¼šh<sub>t</sub> ä¾èµ– h<sub>t-1</sub>ï¼Œå³ä½¿æœ‰ 8 å— GPUï¼Œå¤„ç†å•æ¡åºåˆ—æ—¶ä¹Ÿåªèƒ½ç”¨ 1 å—ï¼Œå…¶ä½™ç©ºè½¬ã€‚è®­ç»ƒååé‡ä¸¥é‡å—é™ï¼Œå°¤å…¶åœ¨é•¿åºåˆ—åœºæ™¯ä¸‹ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>é•¿è·ç¦»ä¾èµ–è¡°å‡</strong>ï¼šä¿¡æ¯ä»ä½ç½® 1 ä¼ åˆ°ä½ç½® n éœ€ç»è¿‡ n-1 æ¬¡çŸ©é˜µä¹˜æ³•ï¼Œæ¢¯åº¦è¦ä¹ˆæ¶ˆå¤±ï¼ˆä¿¡å·è¢«åå¤ç¼©å°ï¼‰è¦ä¹ˆçˆ†ç‚¸ï¼ˆä¿¡å·è¢«åå¤æ”¾å¤§ï¼‰ã€‚LSTM çš„é—¨æ§æœºåˆ¶åªæ˜¯ç¼“è§£è€Œéæ ¹æ²»ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>CNN æ›¿ä»£æ–¹æ¡ˆçš„å±€é™</strong>ï¼šConvS2S é€šè¿‡å·ç§¯å®ç°äº†å¹¶è¡Œè®¡ç®—ï¼Œä½† kernel size ä¸º k çš„å•å±‚å·ç§¯åªèƒ½çœ‹åˆ° k ä¸ªä½ç½®ã€‚è¿æ¥ä»»æ„ä¸¤ä¸ªä½ç½®éœ€è¦ O(log<sub>k</sub>(n)) å±‚å †å ï¼ˆdilated convolutionï¼‰ï¼Œè¿œè·ç¦»ä¾èµ–ä»ç„¶è¾ƒéš¾å­¦ä¹ ã€‚"
      },
      {
        "type": "h3",
        "text": "è®ºæ–‡çš„æ ¸å¿ƒæ´å¯Ÿ"
      },
      {
        "type": "p",
        "text": "å¦‚æœè®©æ¯ä¸ªä½ç½®ç›´æ¥ã€Œçœ‹åˆ°ã€æ‰€æœ‰å…¶ä»–ä½ç½®ï¼ˆè€Œéé€æ­¥ä¼ é€’ï¼‰ï¼Œä¿¡æ¯è·¯å¾„å°±ä» O(n) ç¼©çŸ­åˆ° O(1)ã€‚è¿™å°±æ˜¯ Self-Attention çš„æœ¬è´¨æ€æƒ³ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›å¯ä»¥åŒæ—¶è®¡ç®—â€”â€”ä¸€æ¬¡çŸ©é˜µä¹˜æ³•æå®šï¼Œå¤©ç„¶é€‚åˆ GPU å¹¶è¡Œã€‚"
      },
      {
        "type": "h3",
        "text": "å†å²æ„ä¹‰"
      },
      {
        "type": "p",
        "text": "Transformer ä¸ä»…è¶…è¶Šäº†å½“æ—¶æ‰€æœ‰æ¨¡å‹çš„ç¿»è¯‘è´¨é‡ï¼ˆEN-DE 28.4 BLEU, EN-FR 41.8 BLEUï¼‰ï¼Œè®­ç»ƒæˆæœ¬è¿˜å¤§å¹…é™ä½ï¼ˆä»…ä¸º GNMT çš„ 1/7 FLOPsï¼‰ã€‚æ›´æ·±è¿œçš„å½±å“æ˜¯ï¼šå®ƒè¯æ˜äº†ã€ŒAttention is ALL you needã€â€”â€”æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å®Œå…¨æ›¿ä»£å¾ªç¯å’Œå·ç§¯ï¼Œæˆä¸º GPTã€BERTã€ViT ç­‰åç»­é‡Œç¨‹ç¢‘æ¨¡å‹çš„å…±åŒåŸºçŸ³ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "'The animal didn't cross the street because it was too tired' ä¸­ 'it' æŒ‡ä»€ä¹ˆï¼Ÿæ¨¡å‹éœ€å›å¤´çœ‹å¤šè¿œï¼Ÿ",
        "en": "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.",
        "zh": "ç¬¬ä¸‰ä¸ªè€ƒé‡æ˜¯ç½‘ç»œä¸­é•¿è·ç¦»ä¾èµ–ä¹‹é—´çš„è·¯å¾„é•¿åº¦ã€‚å­¦ä¹ é•¿è·ç¦»ä¾èµ–æ˜¯è®¸å¤šåºåˆ—è½¬æ¢ä»»åŠ¡ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚å½±å“å­¦ä¹ æ­¤ç±»ä¾èµ–èƒ½åŠ›çš„ä¸€ä¸ªå…³é”®å› ç´ ï¼Œæ˜¯å‰å‘å’Œåå‘ä¿¡å·åœ¨ç½‘ç»œä¸­å¿…é¡»ç»è¿‡çš„è·¯å¾„é•¿åº¦ã€‚",
        "a": "'it' æŒ‡ä»£ 'the animal'ï¼Œéš”äº†5ä¸ªè¯ã€‚RNN ä¸­ä¿¡æ¯å¿…é¡»ç»è¿‡5ä¸ªæ—¶é—´æ­¥ï¼Œæ¯æ­¥éƒ½æœ‰è¡°å‡é£é™©ã€‚Self-Attention ä¸­ 'it' ç›´æ¥å…³æ³¨ 'animal'ï¼ˆè·¯å¾„ O(1)ï¼‰ã€‚é™„å½•å›¾4ä¹Ÿå±•ç¤ºäº†æ¨¡å‹å­¦åˆ°çš„æŒ‡ä»£æ¶ˆè§£èƒ½åŠ›ã€‚"
      },
      {
        "q": "å¦‚æœæœ‰8ä¸ªGPUä½†å¿…é¡»æŒ‰é¡ºåºå¤„ç†ï¼Œå’Œ1ä¸ªGPUæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "en": "Recurrent models typically factor computation along the symbol positions of the input and output sequences. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths.",
        "zh": "å¾ªç¯æ¨¡å‹é€šå¸¸æ²¿è¾“å…¥å’Œè¾“å‡ºåºåˆ—çš„ç¬¦å·ä½ç½®è¿›è¡Œåˆ†æ­¥è®¡ç®—ã€‚è¿™ç§å›ºæœ‰çš„åºåˆ—æ€§æœ¬è´¨ä½¿å¾—è®­ç»ƒæ ·æœ¬å†…éƒ¨æ— æ³•å¹¶è¡ŒåŒ–ï¼Œè€Œåœ¨è¾ƒé•¿åºåˆ—é•¿åº¦ä¸‹è¿™ä¸€ç‚¹å˜å¾—å°¤ä¸ºå…³é”®ã€‚",
        "a": "æœ¬è´¨ä¸Šæ²¡åŒºåˆ«â€”â€”RNN çš„ä¸²è¡Œä¾èµ–ï¼ˆh_t ä¾èµ– h_{t-1}ï¼‰ä½¿å¾—å•ä¸ªåºåˆ—åªèƒ½ç”¨1ä¸ªGPUï¼Œå…¶ä½™ç©ºè½¬ã€‚Transformer çš„ Self-Attention æ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—ï¼Œ8ä¸ªGPUçœŸæ­£å¹¶è¡Œã€‚"
      },
      {
        "q": "è®ºæ–‡è¡¨1ï¼ˆTable 1ï¼‰ä¸­ Self-Attention æœ€å¤§è·¯å¾„é•¿åº¦ä¸ºä»€ä¹ˆæ˜¯ O(1)ï¼Ÿ",
        "en": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.",
        "zh": "å¦‚è¡¨1æ‰€è¿°ï¼Œè‡ªæ³¨æ„åŠ›å±‚ä»¥å¸¸æ•°æ¬¡åºåˆ—æ‰§è¡Œæ“ä½œè¿æ¥æ‰€æœ‰ä½ç½®ï¼Œè€Œå¾ªç¯å±‚åˆ™éœ€è¦ O(n) æ¬¡åºåˆ—æ“ä½œã€‚",
        "a": "Self-Attention ä¸­æ¯ä¸ªä½ç½®çš„ Query ç›´æ¥ä¸æ‰€æœ‰ Key åšç‚¹ç§¯â€”â€”ä¸€æ¬¡çŸ©é˜µä¹˜æ³•ï¼Œä¸€æ­¥å®Œæˆã€‚æ— è®ºä¸¤è¯ç›¸è·å¤šè¿œï¼Œéƒ½æ˜¯ç›´æ¥è¿æ¥ã€‚RNN è·¯å¾„ O(n)ï¼ŒCNN è·¯å¾„ O(log_k(n))ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬1é¡µ Abstract + ç¬¬2é¡µ Section 1-2"
  },
  {
    "id": "attention",
    "label": "æ³¨æ„åŠ›æœºåˆ¶",
    "emoji": "ğŸ’¡",
    "color": "#f59e0b",
    "x": 25,
    "y": 30,
    "section": "ç¬¬3.2èŠ‚ï¼šAttention",
    "pages": [
      4
    ],
    "scrollTo": 4,
    "highlights": [
      {
        "page": 3,
        "top": 78,
        "height": 20
      },
      {
        "page": 4,
        "top": 0,
        "height": 95
      }
    ],
    "keyQuestion": "æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼ŸQueryã€Keyã€Value åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "æ ¸å¿ƒç›´è§‰ï¼šå¸¦æƒé‡çš„ä¿¡æ¯æ£€ç´¢"
      },
      {
        "type": "p",
        "text": "æƒ³è±¡ä½ åœ¨å›¾ä¹¦é¦†æ‰¾èµ„æ–™ã€‚ä½ å¸¦ç€ä¸€ä¸ª<strong>é—®é¢˜ï¼ˆQueryï¼‰</strong>ï¼Œä¹¦æ¶ä¸Šæ¯æœ¬ä¹¦éƒ½æœ‰<strong>æ ‡é¢˜ï¼ˆKeyï¼‰</strong>å’Œ<strong>å†…å®¹ï¼ˆValueï¼‰</strong>ã€‚ä½ ç”¨é—®é¢˜åŒ¹é…æ¯ä¸ªæ ‡é¢˜çš„ç›¸å…³åº¦ï¼Œç„¶åæŒ‰ç›¸å…³åº¦é«˜ä½åŠ æƒæå–å„æœ¬ä¹¦çš„å†…å®¹ã€‚è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨â€”â€”<strong>åŸºäºç›¸å…³æ€§çš„åŠ æƒä¿¡æ¯èšåˆ</strong>ã€‚"
      },
      {
        "type": "h3",
        "text": "Scaled Dot-Product Attentionï¼ˆå…¬å¼æ¨å¯¼ï¼‰"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡æå‡ºçš„æ ¸å¿ƒå…¬å¼ï¼š<strong>Attention(Q,K,V) = softmax(QK<sup>T</sup>/âˆšd<sub>k</sub>) Â· V</strong>"
      },
      {
        "type": "p",
        "text": "æ‹†è§£æ¯ä¸€æ­¥çš„å«ä¹‰ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>Step 1ï¼šQK<sup>T</sup>ï¼ˆç›¸å…³æ€§çŸ©é˜µï¼‰</strong>ï¼šQ ç»´åº¦ (nÃ—d<sub>k</sub>)ï¼ŒK<sup>T</sup> ç»´åº¦ (d<sub>k</sub>Ã—n)ï¼Œç›¸ä¹˜å¾—åˆ° (nÃ—n) çš„çŸ©é˜µã€‚çŸ©é˜µä¸­ç¬¬ i è¡Œç¬¬ j åˆ—çš„å€¼è¡¨ç¤ºä½ç½® i å¯¹ä½ç½® j çš„åŸå§‹å…³æ³¨åº¦ã€‚ç‚¹ç§¯è¶Šå¤§ï¼Œä¸¤ä¸ªä½ç½®çš„è¯­ä¹‰è¶Šç›¸å…³ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>Step 2ï¼šÃ· âˆšd<sub>k</sub>ï¼ˆç¼©æ”¾å› å­ï¼‰</strong>ï¼šè¿™æ˜¯è®ºæ–‡çš„å…³é”®è®¾è®¡ã€‚å½“ d<sub>k</sub> è¾ƒå¤§æ—¶ï¼ˆè®ºæ–‡ä¸­ d<sub>k</sub>=64ï¼‰ï¼Œç‚¹ç§¯çš„æœŸæœ›æ–¹å·®ä¸º d<sub>k</sub>ï¼ˆå‡è®¾ Qã€K å„å…ƒç´ ç‹¬ç«‹ä¸”å‡å€¼ä¸º0ã€æ–¹å·®ä¸º1ï¼‰ã€‚æ–¹å·® 64 â†’ æ ‡å‡†å·® 8ï¼Œæ„å‘³ç€ç‚¹ç§¯å€¼åˆ†å¸ƒåœ¨ [-16, 16] å·¦å³ã€‚è¿™ä¹ˆå¤§çš„å€¼è¾“å…¥ softmax ä¼šå¯¼è‡´è¾“å‡ºæ¥è¿‘ one-hotï¼ˆæ¢¯åº¦å‡ ä¹ä¸ºé›¶ï¼Œè®­ç»ƒåœæ»ï¼‰ã€‚é™¤ä»¥ âˆš64=8 åæ–¹å·®æ¢å¤ä¸º 1ï¼Œsoftmax è¾“å‡ºæ‰æœ‰è¶³å¤Ÿçš„æ¢¯åº¦ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>Step 3ï¼šsoftmaxï¼ˆå½’ä¸€åŒ–ï¼‰</strong>ï¼šå°†ç¼©æ”¾åçš„åˆ†æ•°è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆæ¯è¡Œä¹‹å’Œä¸º1ï¼‰ã€‚è¿™æ ·æ¯ä¸ªä½ç½®å¯¹å…¶ä»–æ‰€æœ‰ä½ç½®çš„å…³æ³¨åº¦æ„æˆä¸€ä¸ªåˆæ³•çš„æ¦‚ç‡åˆ†å¸ƒã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>Step 4ï¼šÃ— Vï¼ˆåŠ æƒæ±‚å’Œï¼‰</strong>ï¼šç”¨æ¦‚ç‡åˆ†å¸ƒå¯¹ Value çŸ©é˜µåšåŠ æƒå¹³å‡ã€‚å¦‚æœä½ç½® i å¯¹ä½ç½® j çš„æƒé‡ä¸º 0.8ï¼Œåˆ™ä½ç½® j çš„è¯­ä¹‰ä¿¡æ¯ä¼šå¤§é‡æµå…¥ä½ç½® i çš„è¾“å‡ºã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸ºä»€ä¹ˆé€‰æ‹©ç‚¹ç§¯è€ŒéåŠ æ³•æ³¨æ„åŠ›ï¼Ÿ"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡ Section 3.2.1 å¯¹æ¯”äº†ä¸¤ç§æ³¨æ„åŠ›ï¼š<strong>åŠ æ³•æ³¨æ„åŠ›</strong>ï¼ˆadditive, Bahdanau 2015ï¼‰ä½¿ç”¨å‰é¦ˆç½‘ç»œè®¡ç®—å…¼å®¹æ€§ï¼Œ<strong>ç‚¹ç§¯æ³¨æ„åŠ›</strong>ï¼ˆmultiplicativeï¼‰ç›´æ¥ç”¨å‘é‡å†…ç§¯ã€‚ç†è®ºå¤æ‚åº¦ç›¸è¿‘ï¼Œä½†ç‚¹ç§¯æ³¨æ„åŠ›å¯ä»¥ç”¨é«˜åº¦ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•å®ç°ï¼Œå®è·µä¸­æ˜¾è‘—æ›´å¿«ã€æ›´èŠ‚çœå†…å­˜ã€‚ç¼©æ”¾å› å­ âˆšd<sub>k</sub> å¼¥è¡¥äº†ç‚¹ç§¯åœ¨é«˜ç»´ä¸‹çš„æ•°å€¼é—®é¢˜ï¼Œä½¿å…¶å…¼å…·é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸ä¼ ç»Ÿ Seq2Seq æ³¨æ„åŠ›çš„åŒºåˆ«"
      },
      {
        "type": "p",
        "text": "Bahdanau Attentionï¼ˆ2015ï¼‰ä¸­ Q æ¥è‡ª Decoderã€K/V æ¥è‡ª Encoderï¼Œæ˜¯<strong>è·¨åºåˆ—</strong>æ³¨æ„åŠ›ã€‚Transformer å¼•å…¥çš„ <strong>Self-Attention</strong> åˆ™è®© Qã€Kã€V éƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—â€”â€”æ¯ä¸ªè¯åŒæ—¶ã€Œå®¡è§†ã€åºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»–è¯ï¼Œä»è€Œå»ºæ¨¡è¯ä¸è¯ä¹‹é—´çš„å…¨å±€å…³ç³»ã€‚è¿™æ˜¯ Transformer æœ€æ ¸å¿ƒçš„åˆ›æ–°ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "å¦‚æœæ‰€æœ‰æ³¨æ„åŠ›æƒé‡å‡åŒ€åˆ†å¸ƒï¼Œå’Œä¸ç”¨æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "en": "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.",
        "zh": "è¾“å‡ºè¢«è®¡ç®—ä¸ºå€¼çš„åŠ æƒæ±‚å’Œï¼Œå…¶ä¸­åˆ†é…ç»™æ¯ä¸ªå€¼çš„æƒé‡ç”±æŸ¥è¯¢ä¸ç›¸åº”é”®çš„å…¼å®¹æ€§å‡½æ•°è®¡ç®—å¾—å‡ºã€‚",
        "a": "å‡åŒ€æƒé‡ = æ‰€æœ‰ Value çš„ç®—æœ¯å¹³å‡ï¼Œæ— æ³•åŒºåˆ†é‡è¦ä¿¡æ¯ã€‚æ³¨æ„åŠ›çš„æ ¸å¿ƒåœ¨äºã€ŒåŠ æƒã€â€”â€”é€šè¿‡ Q-K åŒ¹é…åº¦åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚"
      },
      {
        "q": "ä¸é™¤ä»¥ âˆšd_kï¼Œsoftmax è¾“å‡ºä¼šè¶‹è¿‘äºä»€ä¹ˆï¼Ÿ",
        "en": "We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.",
        "zh": "æˆ‘ä»¬æ¨æµ‹ï¼Œå¯¹äºè¾ƒå¤§çš„ d_k å€¼ï¼Œç‚¹ç§¯çš„ç»å¯¹å€¼ä¼šå¢å¤§ï¼Œå°† softmax å‡½æ•°æ¨å…¥æ¢¯åº¦æå°çš„åŒºåŸŸã€‚",
        "a": "è¶‹è¿‘ one-hotï¼ˆæ¥è¿‘ [0,...,1,...,0]ï¼‰ã€‚d_k=64 æ—¶ç‚¹ç§¯æ–¹å·®ä¸º64ï¼Œæ ‡å‡†å·®ä¸º8ï¼Œsoftmax é¥±å’Œï¼Œæ¢¯åº¦â‰ˆ0ï¼Œè®­ç»ƒåœæ»ã€‚é™¤ä»¥ âˆš64=8 åæ–¹å·®å˜ä¸º1ï¼Œæ¢¯åº¦å……è¶³ã€‚"
      },
      {
        "q": "æ‰‹ç®—ï¼š3ä¸ªä½ç½®ï¼Œd_k=2ï¼ŒQ=K=[[1,0],[0,1],[1,1]]",
        "en": "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.",
        "zh": "åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬åŒæ—¶å¯¹ä¸€ç»„æŸ¥è¯¢è®¡ç®—æ³¨æ„åŠ›å‡½æ•°ï¼Œå°†å®ƒä»¬æ‰“åŒ…åˆ°çŸ©é˜µ Q ä¸­ã€‚",
        "a": "QK<sup>T</sup>=[[1,0,1],[0,1,1],[1,1,2]]ï¼Œé™¤ä»¥âˆš2â‰ˆ1.41ï¼Œsoftmax å„è¡Œï¼š[0.39,0.22,0.39]ã€[0.22,0.39,0.39]ã€[0.24,0.24,0.52]ã€‚ä½ç½®3ä¸è‡ªå·±æœ€ç›¸å…³(0.52)ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬3-4é¡µ Section 3.2.1"
  },
  {
    "id": "multihead",
    "label": "å¤šå¤´æ³¨æ„åŠ›",
    "emoji": "ğŸ”®",
    "color": "#8b5cf6",
    "x": 75,
    "y": 30,
    "section": "ç¬¬3.2.2èŠ‚ï¼šMulti-Head Attention",
    "pages": [
      4,
      5
    ],
    "scrollTo": 5,
    "highlights": [
      {
        "page": 4,
        "top": 70,
        "height": 28
      },
      {
        "page": 5,
        "top": 0,
        "height": 50
      }
    ],
    "keyQuestion": "ä¸ºä»€ä¹ˆåˆ†å¤šä¸ªã€Œå¤´ã€ï¼Ÿä¸€ä¸ªå¤§æ³¨æ„åŠ›ä¸å¤Ÿå—ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "åŠ¨æœºï¼šè¯­è¨€ä¸­çš„å¤šé‡å…³ç³»"
      },
      {
        "type": "p",
        "text": "è‡ªç„¶è¯­è¨€ä¸­ï¼Œè¯ä¸è¯ä¹‹é—´åŒæ—¶å­˜åœ¨å¤šç§ç±»å‹çš„å…³ç³»ï¼šå¥æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰ã€è¯­ä¹‰å…³ç³»ï¼ˆåŒä¹‰åä¹‰ï¼‰ã€æŒ‡ä»£å…³ç³»ï¼ˆä»£è¯â†’å…ˆè¡Œè¯ï¼‰ã€ä½ç½®å…³ç³»ï¼ˆç›¸é‚»è¯ï¼‰ç­‰ã€‚ä¸€ä¸ªæ³¨æ„åŠ›å¤´åªèƒ½å­¦ä¹ ä¸€ç§æ³¨æ„åŠ›åˆ†å¸ƒæ¨¡å¼ã€‚å¦‚æœåªç”¨å•å¤´ï¼Œè¿™äº›ä¸åŒç»´åº¦çš„å…³ç³»ä¼šè¢«ã€ŒæŒ¤å‹ã€åˆ°åŒä¸€ä¸ªæ³¨æ„åŠ›çŸ©é˜µä¸­ï¼Œå½¼æ­¤å¹²æ‰°ã€‚"
      },
      {
        "type": "h3",
        "text": "æ ¸å¿ƒå…¬å¼"
      },
      {
        "type": "p",
        "text": "<strong>MultiHead(Q,K,V) = Concat(head<sub>1</sub>, ..., head<sub>h</sub>) Â· W<sup>O</sup></strong>"
      },
      {
        "type": "p",
        "text": "å…¶ä¸­ <strong>head<sub>i</sub> = Attention(QW<sub>i</sub><sup>Q</sup>, KW<sub>i</sub><sup>K</sup>, VW<sub>i</sub><sup>V</sup>)</strong>"
      },
      {
        "type": "p",
        "text": "æ¯ä¸ªå¤´æœ‰ç‹¬ç«‹çš„æŠ•å½±çŸ©é˜µ W<sub>i</sub><sup>Q</sup> âˆˆ â„<sup>d<sub>model</sub>Ã—d<sub>k</sub></sup>, W<sub>i</sub><sup>K</sup> âˆˆ â„<sup>d<sub>model</sub>Ã—d<sub>k</sub></sup>, W<sub>i</sub><sup>V</sup> âˆˆ â„<sup>d<sub>model</sub>Ã—d<sub>v</sub></sup>ï¼Œå°† d<sub>model</sub>=512 ç»´çš„è¾“å…¥æŠ•å½±åˆ° d<sub>k</sub>=d<sub>v</sub>=64 ç»´çš„å­ç©ºé—´ä¸­ã€‚"
      },
      {
        "type": "h3",
        "text": "è®¡ç®—é‡ä¸ºä»€ä¹ˆä¸å¢åŠ ï¼Ÿ"
      },
      {
        "type": "p",
        "text": "å•å¤´æ³¨æ„åŠ›åœ¨ d<sub>model</sub>=512 ç»´ä¸Šåšä¸€æ¬¡æ³¨æ„åŠ›è®¡ç®—ã€‚å¤šå¤´ï¼ˆh=8ï¼‰åœ¨ d<sub>k</sub>=64 ç»´ä¸Šåš8æ¬¡ï¼Œæ€»è®¡ç®—é‡ä¸º 8 Ã— 64 = 512 ç»´â€”â€”<strong>ä¸å•å¤´å‡ ä¹ç›¸åŒ</strong>ã€‚è¿™æ˜¯ä¸€ä¸ªç²¾å¦™çš„è®¾è®¡ï¼šç”¨ã€Œå¤šä¸ªå°ç©ºé—´ã€æ›¿ä»£ã€Œä¸€ä¸ªå¤§ç©ºé—´ã€ï¼Œè·å¾—äº†å¤šæ ·æ€§è€Œä¸å¢åŠ æˆæœ¬ã€‚"
      },
      {
        "type": "h3",
        "text": "æ¯ä¸ªå¤´å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿï¼ˆé™„å½•å›¾3-5ï¼‰"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡é™„å½•çš„å¯è§†åŒ–æ­ç¤ºäº†ä¸åŒå¤´è‡ªåŠ¨åˆ†å·¥çš„ç°è±¡ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>å›¾3</strong>ï¼šæŸäº›å¤´è·Ÿè¸ªé•¿è·ç¦»çš„åŠ¨è¯-å®¾è¯­ä¾èµ–ï¼ˆå¦‚ making...more difficultï¼‰ï¼Œå…³æ³¨è·¨è¶Šå¤šä¸ªè¯çš„å¥æ³•ç»“æ„ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>å›¾4</strong>ï¼šæŸäº›å¤´ä¸“æ³¨äºæŒ‡ä»£æ¶ˆè§£ï¼ˆå¦‚ its â†’ Lawï¼‰ï¼Œæ³¨æ„åŠ›åˆ†å¸ƒéå¸¸å°–é”ï¼Œå‡ ä¹åªæŒ‡å‘è¢«å¼•ç”¨çš„é‚£ä¸ªè¯ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>å›¾5</strong>ï¼šä¸åŒçš„å¤´å­¦åˆ°äº†ä¸åŒçš„å¥æ³•ç»“æ„æ¨¡å¼ï¼Œæœ‰äº›å…³æ³¨å±€éƒ¨ä¾èµ–ï¼Œæœ‰äº›å…³æ³¨é•¿è·ç¦»ä¾èµ–ã€‚"
      },
      {
        "type": "p",
        "text": "è¿™ç§åˆ†å·¥æ˜¯<strong>å®Œå…¨è‡ªåŠ¨å­¦åˆ°çš„</strong>ï¼Œæ²¡æœ‰ä»»ä½•æ˜¾å¼ç›‘ç£ä¿¡å·å‘Šè¯‰æ¨¡å‹ã€Œè¿™ä¸ªå¤´åº”è¯¥å­¦å¥æ³•ã€ã€‚è¿™è¯´æ˜å¤šå¤´æœºåˆ¶ä¸ºæ¨¡å‹æä¾›äº†è¶³å¤Ÿçš„è¡¨è¾¾ç©ºé—´æ¥è‡ªå‘æ¶Œç°å‡ºåŠŸèƒ½åˆ†åŒ–ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸‰ç§ä½¿ç”¨åœºæ™¯"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡ä¸­å¤šå¤´æ³¨æ„åŠ›è¢«ç”¨åœ¨ä¸‰ä¸ªä¸åŒçš„åœ°æ–¹ï¼š<strong>â‘  Encoder Self-Attention</strong>â€”â€”æºå¥å­ä¸­çš„è¯äº’ç›¸å…³æ³¨ï¼›<strong>â‘¡ Decoder Masked Self-Attention</strong>â€”â€”ç›®æ ‡å¥å­ä¸­å·²ç”Ÿæˆçš„è¯äº’ç›¸å…³æ³¨ï¼ˆå¸¦æ©ç é˜²æ­¢çœ‹åˆ°æœªæ¥ï¼‰ï¼›<strong>â‘¢ Encoder-Decoder Attention</strong>â€”â€”Decoder çš„è¯å…³æ³¨ Encoder çš„è¾“å‡ºï¼ˆå®ç°è·¨è¯­è¨€çš„ä¿¡æ¯ä¼ é€’ï¼‰ã€‚è¿™ä¸‰å¤„ä½¿ç”¨åŒä¸€ç§å¤šå¤´æ³¨æ„åŠ›ç»“æ„ï¼Œåªæ˜¯è¾“å…¥çš„ Q/K/V æ¥æºä¸åŒã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "æŠŠ8å¤´æ¢æˆ1å¤´æˆ–32å¤´ä¼šæ€æ ·ï¼Ÿè¡¨3(A)è¡Œæ•°æ®",
        "en": "While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.",
        "zh": "è™½ç„¶å•å¤´æ³¨æ„åŠ›æ¯”æœ€ä½³è®¾ç½®å·®0.9ä¸ª BLEUï¼Œä½†å¤´æ•°è¿‡å¤šæ—¶è´¨é‡ä¹Ÿä¼šä¸‹é™ã€‚",
        "a": "h=1â†’24.9, h=8â†’25.8(æœ€ä½³), h=16â†’25.8(æŒå¹³), h=32â†’25.4(ä¸‹é™, d_kä»…16)ã€‚ã€Œå¤šè§†è§’ vs æ¯å¤´ç²¾åº¦ã€çš„æœ€ä½³å¹³è¡¡åœ¨8-16å¤´ã€‚"
      },
      {
        "q": "çº¿æ€§æŠ•å½± W_i^Q å’Œç›´æ¥åˆ‡åˆ† Q æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "en": "We found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections.",
        "zh": "æˆ‘ä»¬å‘ç°å°†æŸ¥è¯¢ã€é”®å’Œå€¼åˆ†åˆ«é€šè¿‡ h ç»„ä¸åŒçš„ã€å¯å­¦ä¹ çš„çº¿æ€§æŠ•å½±æ˜ å°„ï¼Œæ•ˆæœæ›´å¥½ã€‚",
        "a": "ç›´æ¥åˆ‡åˆ† = æ¯å¤´åªçœ‹å›ºå®šçš„64ç»´å­é›†ã€‚çº¿æ€§æŠ•å½± = å¯å­¦ä¹ çš„çŸ©é˜µå˜æ¢ï¼Œæ¯å¤´èƒ½è‡ªç”±é€‰æ‹©å…³æ³¨å“ªäº›ç‰¹å¾ç»„åˆï¼Œä¸å—å›ºå®šåˆ†é…é™åˆ¶ã€‚"
      },
      {
        "q": "é™„å½•å›¾3-5ä¸­è§‚å¯Ÿåˆ°å“ªäº›æ¨¡å¼ï¼Ÿ",
        "en": "Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. The heads clearly learned to perform different tasks.",
        "zh": "è®¸å¤šæ³¨æ„åŠ›å¤´è¡¨ç°å‡ºä¼¼ä¹ä¸å¥å­ç»“æ„ç›¸å…³çš„è¡Œä¸ºã€‚è¿™äº›å¤´æ˜æ˜¾å­¦åˆ°äº†æ‰§è¡Œä¸åŒçš„ä»»åŠ¡ã€‚",
        "a": "å›¾3ï¼šè·Ÿè¸ªé•¿è·ç¦»åŠ¨è¯ä¾èµ–ï¼ˆmaking...more difficultï¼‰ã€‚å›¾4ï¼šæŒ‡ä»£æ¶ˆè§£ï¼ˆitsâ†’Lawï¼Œæ³¨æ„åŠ›éå¸¸å°–é”ï¼‰ã€‚å›¾5ï¼šå­¦åˆ°ä¸åŒå¥æ³•ç»“æ„ã€‚è¯æ˜å¤šå¤´åˆ†å·¥æ˜¯è‡ªåŠ¨å­¦åˆ°çš„ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬4-5é¡µ Section 3.2.2 + é™„å½•"
  },
  {
    "id": "architecture",
    "label": "å®Œæ•´æ¶æ„",
    "emoji": "ğŸ—ï¸",
    "color": "#3b82f6",
    "x": 50,
    "y": 52,
    "section": "ç¬¬3èŠ‚ï¼šModel Architecture",
    "pages": [
      3
    ],
    "scrollTo": 3,
    "highlights": [
      {
        "page": 3,
        "top": 0,
        "height": 100
      }
    ],
    "keyQuestion": "Encoder å’Œ Decoder å¦‚ä½•åä½œï¼Ÿä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "æ•´ä½“æ¶æ„ï¼šEncoder-Decoder æ¡†æ¶"
      },
      {
        "type": "p",
        "text": "Transformer æ²¿ç”¨äº†ç»å…¸çš„ Encoder-Decoder ç»“æ„ï¼ˆå‚è§è®ºæ–‡å›¾1ï¼Œè¿™æ˜¯æœ¬æ–‡æœ€é‡è¦çš„å›¾ï¼‰ã€‚Encoder å°†æºåºåˆ— (x<sub>1</sub>,...,x<sub>n</sub>) æ˜ å°„ä¸ºè¿ç»­è¡¨ç¤º z = (z<sub>1</sub>,...,z<sub>n</sub>)ï¼›Decoder ä»¥ z ä¸ºæ¡ä»¶ï¼Œè‡ªå›å½’åœ°ç”Ÿæˆç›®æ ‡åºåˆ— (y<sub>1</sub>,...,y<sub>m</sub>)ï¼Œæ¯æ­¥ç”Ÿæˆä¸€ä¸ª tokenã€‚ä½†ä¸ RNN ä¸åŒçš„æ˜¯ï¼ŒEncoder å’Œ Decoder å†…éƒ¨å®Œå…¨ç”±æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œæ„æˆã€‚"
      },
      {
        "type": "h3",
        "text": "Encoderï¼š6å±‚å †å "
      },
      {
        "type": "p",
        "text": "æ¯å±‚åŒ…å«ä¸¤ä¸ªå­å±‚ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>å­å±‚1ï¼šMulti-Head Self-Attention</strong> â€” åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®å…³æ³¨æ‰€æœ‰å…¶ä»–ä½ç½®ï¼Œæ•æ‰å…¨å±€ä¾èµ–å…³ç³»ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>å­å±‚2ï¼šPosition-wise Feed-Forward Network</strong> â€” å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åœ°åšéçº¿æ€§å˜æ¢ï¼Œå¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚"
      },
      {
        "type": "p",
        "text": "æ¯ä¸ªå­å±‚å¤–å›´éƒ½æœ‰<strong>æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰</strong>å’Œ<strong>å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰</strong>ï¼š<strong>output = LayerNorm(x + Sublayer(x))</strong>ã€‚æ®‹å·®è¿æ¥è®©æ¢¯åº¦å¯ä»¥ã€Œè·³è¿‡ã€å­å±‚ç›´æ¥å›ä¼ ï¼Œæœ‰æ•ˆé˜²æ­¢æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±ï¼›LayerNorm åˆ™ç¨³å®šæ¯å±‚çš„æ•°å€¼èŒƒå›´ï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›ã€‚"
      },
      {
        "type": "h3",
        "text": "Decoderï¼š6å±‚å †å ï¼Œä½†å¤šäº†ä¸€ä¸ªå­å±‚"
      },
      {
        "type": "p",
        "text": "Decoder æ¯å±‚æœ‰<strong>ä¸‰ä¸ª</strong>å­å±‚ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>å­å±‚1ï¼šMasked Multi-Head Self-Attention</strong> â€” ä¸ Encoder ç±»ä¼¼ï¼Œä½†åŠ äº†æ©ç ï¼ˆmaskï¼‰ã€‚å®ç°æ–¹å¼æ˜¯å°†æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µçš„ä¸Šä¸‰è§’éƒ¨åˆ†è®¾ä¸º -âˆï¼Œsoftmax åè¿™äº›ä½ç½®çš„æƒé‡å˜ä¸º0ã€‚è¿™ä¿è¯äº†ç”Ÿæˆç¬¬ t ä¸ªè¯æ—¶ï¼Œæ¨¡å‹åªèƒ½çœ‹åˆ°ä½ç½® 1~t-1 çš„è¯ï¼Œç»´æŒè‡ªå›å½’ç‰¹æ€§ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>å­å±‚2ï¼šEncoder-Decoder Attention</strong> â€” <strong>Query æ¥è‡ª Decoder ä¸Šä¸€å­å±‚çš„è¾“å‡ºï¼ŒKey å’Œ Value æ¥è‡ª Encoder çš„æœ€ç»ˆè¾“å‡º</strong>ã€‚è¿™æ˜¯æºè¯­è¨€ä¿¡æ¯æµå…¥ç›®æ ‡è¯­è¨€çš„å”¯ä¸€é€šé“ï¼ŒåŠŸèƒ½ç±»ä¼¼äºä¼ ç»Ÿ Seq2Seq ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†ç¿»è¯‘ä¸­çš„ã€Œå¯¹é½ã€ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>å­å±‚3ï¼šPosition-wise FFN</strong> â€” ä¸ Encoder ä¸­çš„ FFN ç»“æ„ç›¸åŒã€‚"
      },
      {
        "type": "h3",
        "text": "ç»´åº¦è®¾è®¡çš„ä¸€è‡´æ€§"
      },
      {
        "type": "p",
        "text": "æ•´ä¸ªæ¨¡å‹ä¸­ï¼Œæ‰€æœ‰å­å±‚çš„è¾“å‡ºç»´åº¦ç»Ÿä¸€ä¸º <strong>d<sub>model</sub> = 512</strong>ï¼ˆåŒ…æ‹¬ embedding å±‚ï¼‰ã€‚è¿™ä¸æ˜¯éšæ„é€‰æ‹©ï¼Œè€Œæ˜¯ä¸ºäº†è®©æ®‹å·®è¿æ¥ä¸­çš„ <strong>x + Sublayer(x)</strong> å¯ä»¥ç›´æ¥ç›¸åŠ â€”â€”ä¸¤è€…å¿…é¡»ç»´åº¦ç›¸åŒã€‚è¿™ç§ã€Œç­‰ç»´è®¾è®¡ã€è®©æ¶æ„æå…¶è§„æ•´ï¼Œä¾¿äºå †å ä»»æ„æ•°é‡çš„å±‚ã€‚"
      },
      {
        "type": "h3",
        "text": "Embedding ä¸è¾“å‡ºå±‚çš„æƒé‡å…±äº«"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡ä¸­ï¼ŒEncoder å’Œ Decoder çš„è¾“å…¥ embedding çŸ©é˜µä»¥åŠ Decoder è¾“å‡ºå‰çš„çº¿æ€§å˜æ¢çŸ©é˜µ<strong>å…±äº«åŒä¸€ç»„æƒé‡</strong>ï¼ˆå‚è€ƒæ–‡çŒ® [30]ï¼‰ï¼Œå¹¶ä¹˜ä»¥ âˆšd<sub>model</sub> è¿›è¡Œç¼©æ”¾ã€‚è¿™å¤§å¹…å‡å°‘äº†å‚æ•°é‡ï¼ŒåŒæ—¶åˆ©ç”¨äº†ã€Œè¯çš„è¾“å…¥è¡¨ç¤ºå’Œè¾“å‡ºè¡¨ç¤ºåº”è¯¥åœ¨åŒä¸€è¯­ä¹‰ç©ºé—´ã€çš„å…ˆéªŒå‡è®¾ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "Decoder Masked Attention æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ",
        "en": "We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆ) all values in the input of the softmax which correspond to illegal connections.",
        "zh": "æˆ‘ä»¬é€šè¿‡åœ¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å†…éƒ¨ï¼Œå°† softmax è¾“å…¥ä¸­å¯¹åº”äºéæ³•è¿æ¥çš„æ‰€æœ‰å€¼è®¾ç½®ä¸º -âˆ æ¥å®ç°ã€‚",
        "a": "æ„é€ ä¸Šä¸‰è§’ mask çŸ©é˜µï¼ˆå¯¹è§’çº¿ä»¥ä¸Šå…¨ä¸º -âˆï¼‰ï¼ŒåŠ åˆ° QK<sup>T</sup>/âˆšd_k ä¸Šã€‚æœªæ¥ä½ç½®çš„åˆ†æ•°å˜ä¸º -âˆï¼Œsoftmax åæƒé‡ä¸º0ã€‚ä¿è¯äº†è‡ªå›å½’ç‰¹æ€§ã€‚"
      },
      {
        "q": "å»æ‰æ®‹å·®è¿æ¥æˆ– Layer Norm ä¼šæ€æ ·ï¼Ÿ",
        "en": "We employ a residual connection around each of the two sub-layers, followed by layer normalization. To facilitate these residual connections, all sub-layers produce outputs of dimension d_model = 512.",
        "zh": "æˆ‘ä»¬åœ¨æ¯ä¸¤ä¸ªå­å±‚å‘¨å›´ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œåæ¥å±‚å½’ä¸€åŒ–ã€‚ä¸ºä¾¿äºæ®‹å·®è¿æ¥ï¼Œæ‰€æœ‰å­å±‚è¾“å‡ºç»´åº¦å‡ä¸º d_model=512ã€‚",
        "a": "å»æ‰æ®‹å·®è¿æ¥ï¼š12ä¸ªå­å±‚çš„æ¢¯åº¦éœ€é€å±‚å›ä¼ ï¼Œææ˜“æ¶ˆå¤±/çˆ†ç‚¸ã€‚æ®‹å·®æä¾›ã€Œæ¢¯åº¦é«˜é€Ÿå…¬è·¯ã€ã€‚å»æ‰ LayerNormï¼šæ•°å€¼èŒƒå›´å¤±æ§ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚ç»´åº¦ç»Ÿä¸€ä¸º512æ­£æ˜¯ä¸ºäº†è®©æ®‹å·®çš„ x + Sublayer(x) å¯ç›´æ¥ç›¸åŠ ã€‚"
      },
      {
        "q": "Encoder-Decoder Attention ä¸­ä¸ºä»€ä¹ˆ Q æ¥è‡ª Decoder?",
        "en": "In encoder-decoder attention layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.",
        "zh": "åœ¨ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å±‚ä¸­ï¼ŒæŸ¥è¯¢æ¥è‡ªä¸Šä¸€ä¸ªè§£ç å™¨å±‚ï¼Œè€Œè®°å¿†é”®å’Œå€¼æ¥è‡ªç¼–ç å™¨çš„è¾“å‡ºã€‚",
        "a": "Decoder åœ¨ç”Ÿæˆç›®æ ‡è¯æ—¶å‘å‡ºã€Œæˆ‘éœ€è¦ä»€ä¹ˆä¿¡æ¯ã€çš„æŸ¥è¯¢(Q)ï¼Œå» Encoder çš„è¾“å‡ºä¸­æ£€ç´¢(K)å¹¶æå–(V)ã€‚è¿™æ˜¯æºè¯­è¨€ä¿¡æ¯æµå…¥ç›®æ ‡ç”Ÿæˆçš„å”¯ä¸€é€šé“ï¼Œå®ç°äº†ç¿»è¯‘ä¸­çš„ã€Œå¯¹é½ã€ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬3é¡µ Section 3.1 + å›¾1"
  },
  {
    "id": "ffn",
    "label": "å‰é¦ˆç½‘ç»œ",
    "emoji": "âš™ï¸",
    "color": "#06b6d4",
    "x": 15,
    "y": 52,
    "section": "ç¬¬3.3èŠ‚ï¼šPosition-wise FFN",
    "pages": [
      5
    ],
    "scrollTo": 5,
    "highlights": [
      {
        "page": 5,
        "top": 48,
        "height": 35
      }
    ],
    "keyQuestion": "æ³¨æ„åŠ›åªåšçº¿æ€§åŠ æƒâ€”â€”éçº¿æ€§å˜æ¢å’ŒçŸ¥è¯†å­˜å‚¨ç”±è°è´Ÿè´£ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "å…¬å¼ä¸ç»“æ„"
      },
      {
        "type": "p",
        "text": "<strong>FFN(x) = max(0, xW<sub>1</sub> + b<sub>1</sub>)W<sub>2</sub> + b<sub>2</sub></strong>"
      },
      {
        "type": "p",
        "text": "è¿™æ˜¯ä¸€ä¸ªä¸¤å±‚çš„å…¨è¿æ¥ç½‘ç»œï¼Œä¸­é—´ç”¨ ReLU æ¿€æ´»ã€‚ç»´åº¦å˜åŒ–ï¼š512 â†’ 2048 â†’ 512ã€‚å…ˆã€Œæ‰©å±•ã€åˆ°4å€ç»´åº¦ï¼ˆå¢åŠ è¡¨è¾¾èƒ½åŠ›ï¼‰ï¼Œå†ã€Œå‹ç¼©ã€å›åŸå§‹ç»´åº¦ï¼ˆä¾¿äºæ®‹å·®è¿æ¥ï¼‰ã€‚è®ºæ–‡ç‰¹åˆ«æŒ‡å‡ºï¼Œè¿™ç­‰ä»·äº<strong>ä¸¤ä¸ª kernel size=1 çš„å·ç§¯</strong>ï¼ˆ1Ã—1 convolutionï¼‰ï¼Œå› ä¸ºå®ƒå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹æ“ä½œï¼Œä¸çœ‹ç›¸é‚»ä½ç½®ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸ºä»€ä¹ˆå¿…é¡»æœ‰ FFNï¼Ÿâ€”â€”è§’è‰²åˆ†å·¥"
      },
      {
        "type": "p",
        "text": "Self-Attention å±‚æœ¬è´¨ä¸Šåªåšã€Œçº¿æ€§åŠ æƒå¹³å‡ã€ï¼šoutput = Î£(Î±áµ¢ Â· váµ¢)ã€‚å³ä½¿å †å å¤šå±‚ï¼Œæ²¡æœ‰éçº¿æ€§æ¿€æ´»çš„è¯ï¼Œæ•´ä¸ªç½‘ç»œä»ç„¶æ˜¯çº¿æ€§å˜æ¢ï¼Œè¡¨è¾¾èƒ½åŠ›ä¸¥é‡å—é™ã€‚FFN ä¸­çš„ ReLU æä¾›äº†å…³é”®çš„<strong>éçº¿æ€§</strong>ï¼Œä½¿å¾—æ¯ä¸€å±‚éƒ½èƒ½å­¦ä¹ æ–°çš„ç‰¹å¾å˜æ¢ã€‚"
      },
      {
        "type": "p",
        "text": "æ›´æ·±å±‚çš„ç ”ç©¶ï¼ˆGeva et al., 2021ï¼‰å‘ç°ï¼ŒFFN å®é™…ä¸Šå……å½“äº†<strong>ã€ŒçŸ¥è¯†è®°å¿†ä½“ã€</strong>çš„è§’è‰²ã€‚W<sub>1</sub> çš„æ¯ä¸€è¡Œå¯ä»¥çœ‹ä½œä¸€ä¸ªã€Œæ¨¡å¼æ£€æµ‹å™¨ã€ï¼ˆç±»ä¼¼äº keyï¼‰ï¼ŒW<sub>2</sub> çš„å¯¹åº”åˆ—åˆ™å­˜å‚¨äº†æ£€æµ‹åˆ°è¯¥æ¨¡å¼ååº”è¾“å‡ºçš„ä¿¡æ¯ï¼ˆç±»ä¼¼äº valueï¼‰ã€‚ReLU èµ·åˆ°ç¨€ç–æ¿€æ´»çš„ä½œç”¨â€”â€”å¯¹äºæ¯ä¸ªè¾“å…¥ï¼Œåªæœ‰å°‘æ•°æ¨¡å¼è¢«æ¿€æ´»ã€‚"
      },
      {
        "type": "h3",
        "text": "ã€Œé€ä½ç½®ã€çš„å«ä¹‰"
      },
      {
        "type": "p",
        "text": "<strong>Position-wise</strong> æ„å‘³ç€åŒä¸€å±‚çš„ FFN å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ä½¿ç”¨<strong>ç›¸åŒçš„å‚æ•°</strong>ä½†<strong>ç‹¬ç«‹è®¡ç®—</strong>ã€‚ä½ç½® 1 å’Œä½ç½® n å…±äº« W<sub>1</sub>, W<sub>2</sub>ï¼Œä½†å„è‡ªçš„è¾“å…¥ä¸åŒï¼Œæ‰€ä»¥è¾“å‡ºä¹Ÿä¸åŒã€‚ä¸åŒå±‚ä¹‹é—´çš„ FFN å‚æ•°æ˜¯<strong>ä¸å…±äº«çš„</strong>â€”â€”è¿™ç»™äº†æ¨¡å‹åœ¨ä¸åŒæŠ½è±¡å±‚çº§ä¸Šå­¦ä¹ ä¸åŒå˜æ¢çš„èƒ½åŠ›ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸­é—´ç»´åº¦ d<sub>ff</sub> çš„é€‰æ‹©"
      },
      {
        "type": "p",
        "text": "Base æ¨¡å‹ç”¨ d<sub>ff</sub>=2048ï¼ˆ4Ã—d<sub>model</sub>ï¼‰ï¼ŒBig æ¨¡å‹ç”¨ d<sub>ff</sub>=4096ï¼ˆ8Ã—d<sub>model</sub>ï¼‰ã€‚æ¶ˆèå®éªŒè¡¨3(C)è¡Œæ˜¾ç¤ºï¼šd<sub>ff</sub>=1024 â†’ BLEU 25.4ï¼›2048 â†’ 25.8ï¼›4096 â†’ 26.2ã€‚æ›´å¤§çš„ d<sub>ff</sub> æä¾›äº†æ›´å¤§çš„ã€ŒçŸ¥è¯†å®¹é‡ã€ï¼Œä½†å‚æ•°é‡ä¹Ÿç›¸åº”å¢é•¿ï¼ˆFFN å‚æ•°çº¦å æ€»å‚æ•°çš„ 2/3ï¼‰ã€‚åæ¥çš„ GPT-3 ç­‰å¤§æ¨¡å‹æ™®éé‡‡ç”¨ 4Ã— çš„æ‰©å±•æ¯”ç‡ï¼Œå·²æˆä¸ºæ ‡å‡†é…ç½®ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "å»æ‰ FFN åªä¿ç•™æ³¨æ„åŠ›å±‚ï¼Œæ¨¡å‹è¿˜èƒ½å·¥ä½œå—ï¼Ÿ",
        "en": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.",
        "zh": "é™¤æ³¨æ„åŠ›å­å±‚å¤–ï¼Œç¼–ç å™¨å’Œè§£ç å™¨çš„æ¯ä¸€å±‚è¿˜åŒ…å«ä¸€ä¸ªå…¨è¿æ¥å‰é¦ˆç½‘ç»œï¼Œè¯¥ç½‘ç»œåˆ†åˆ«ä¸”ç›¸åŒåœ°åº”ç”¨äºæ¯ä¸ªä½ç½®ã€‚",
        "a": "Self-Attention æœ¬è´¨æ˜¯çº¿æ€§åŠ æƒå¹³å‡ã€‚æ²¡æœ‰ FFN çš„éçº¿æ€§(ReLU)ï¼Œå¤šå±‚å †å ä»å¯è¢«å•å±‚æ›¿ä»£ï¼Œè¡¨è¾¾åŠ›ä¸¥é‡å—é™ã€‚FFN è¿˜æ‰¿æ‹…çŸ¥è¯†å­˜å‚¨è§’è‰²ã€‚"
      },
      {
        "q": "ä¸ºä»€ä¹ˆ FFN ç­‰ä»·äºä¸¤ä¸ª kernel size=1 çš„å·ç§¯ï¼Ÿ",
        "en": "Another way of describing this is as two convolutions with kernel size 1.",
        "zh": "å¦ä¸€ç§æè¿°æ–¹å¼æ˜¯ä¸¤ä¸ªæ ¸å¤§å°ä¸º1çš„å·ç§¯ã€‚",
        "a": "1Ã—1 å·ç§¯å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åšçº¿æ€§å˜æ¢ï¼ˆä¸çœ‹ç›¸é‚»ä½ç½®ï¼‰ï¼Œä¸ FFN è¡Œä¸ºå®Œå…¨ä¸€è‡´ã€‚ç¬¬ä¸€ä¸ª1Ã—1å·ç§¯ 512â†’2048ï¼Œç» ReLUï¼Œç¬¬äºŒä¸ª 2048â†’512ã€‚"
      },
      {
        "q": "d_ff æ¢æˆ 1024 æˆ– 4096 ä¼šå¦‚ä½•ï¼Ÿè¡¨3(C)è¡Œ",
        "en": "We further observe in rows (C) and (D) that, as expected, bigger models are better.",
        "zh": "æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨(C)å’Œ(D)è¡Œä¸­è§‚å¯Ÿåˆ°ï¼Œæ­£å¦‚é¢„æœŸï¼Œæ›´å¤§çš„æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚",
        "a": "d_ff=1024â†’BLEU 25.4(53Må‚æ•°)ï¼›2048â†’25.8(65M,base)ï¼›4096â†’26.2(90M)ã€‚è¶Šå¤§è¶Šå¥½ï¼Œä½†å‚æ•°é‡ä¹Ÿå¢é•¿ã€‚2048æ˜¯æ•ˆç‡ä¸æ€§èƒ½çš„æŠ˜ä¸­ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬5é¡µ Section 3.3"
  },
  {
    "id": "positional",
    "label": "ä½ç½®ç¼–ç ",
    "emoji": "ğŸ“",
    "color": "#10b981",
    "x": 85,
    "y": 52,
    "section": "ç¬¬3.5èŠ‚ï¼šPositional Encoding",
    "pages": [
      6
    ],
    "scrollTo": 6,
    "highlights": [
      {
        "page": 6,
        "top": 0,
        "height": 52
      }
    ],
    "keyQuestion": "æ³¨æ„åŠ›æ˜¯ã€Œé›†åˆæ“ä½œã€ï¼Œå®Œå…¨æ— åºâ€”â€”å¦‚ä½•è®©æ¨¡å‹æ„ŸçŸ¥è¯åºï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "é—®é¢˜çš„æœ¬è´¨"
      },
      {
        "type": "p",
        "text": "Self-Attention çš„è®¡ç®—è¿‡ç¨‹æ˜¯<strong>ç½®æ¢ä¸å˜çš„ï¼ˆpermutation invariantï¼‰</strong>ï¼šæ‰“ä¹±è¾“å…¥åºåˆ—çš„é¡ºåºï¼Œè¾“å‡ºï¼ˆé™¤äº†é¡ºåºä¹Ÿå˜äº†ä¹‹å¤–ï¼‰ä¸ä¼šæ”¹å˜ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œã€Œæˆ‘çˆ±ä½ ã€å’Œã€Œä½ çˆ±æˆ‘ã€åœ¨ Self-Attention çœ‹æ¥æ˜¯å®Œå…¨ç­‰ä»·çš„ã€‚è¿™å¯¹äºè¯­è¨€å»ºæ¨¡æ˜¯è‡´å‘½çš„â€”â€”è¯åºæºå¸¦äº†å…³é”®çš„è¯­ä¹‰ä¿¡æ¯ã€‚"
      },
      {
        "type": "h3",
        "text": "æ­£å¼¦ä½ç½®ç¼–ç å…¬å¼"
      },
      {
        "type": "p",
        "text": "<strong>PE<sub>(pos,2i)</sub> = sin(pos / 10000<sup>2i/d<sub>model</sub></sup>)</strong>"
      },
      {
        "type": "p",
        "text": "<strong>PE<sub>(pos,2i+1)</sub> = cos(pos / 10000<sup>2i/d<sub>model</sub></sup>)</strong>"
      },
      {
        "type": "p",
        "text": "å…¶ä¸­ pos æ˜¯ä½ç½®ç´¢å¼•ï¼Œi æ˜¯ç»´åº¦ç´¢å¼•ã€‚æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªä¸åŒé¢‘ç‡çš„æ­£å¼¦/ä½™å¼¦æ³¢ï¼Œæ³¢é•¿ä» 2Ï€ï¼ˆç¬¬0ç»´ï¼‰åˆ° 10000Â·2Ï€ï¼ˆæœ€åä¸€ç»´ï¼‰æ„æˆç­‰æ¯”æ•°åˆ—ã€‚"
      },
      {
        "type": "h3",
        "text": "ç›´è§‰ç†è§£ï¼šå¤šé¢‘ç‡ã€ŒæŒ‡çº¹ã€"
      },
      {
        "type": "p",
        "text": "ç±»æ¯”æ—¶é’Ÿï¼šç§’é’ˆè½¬åŠ¨å¿«ï¼ˆä½ç»´ï¼Œé«˜é¢‘ï¼‰ï¼Œåˆ†é’ˆè½¬åŠ¨æ…¢ï¼ˆä¸­ç»´ï¼‰ï¼Œæ—¶é’ˆè½¬åŠ¨æœ€æ…¢ï¼ˆé«˜ç»´ï¼Œä½é¢‘ï¼‰ã€‚é€šè¿‡è¯»å–å„ä¸ªã€ŒæŒ‡é’ˆã€çš„è§’åº¦ï¼Œä½ å¯ä»¥å”¯ä¸€ç¡®å®šå½“å‰æ—¶åˆ»ã€‚åŒç†ï¼Œæ¯ä¸ªä½ç½®çš„ç¼–ç å‘é‡å°±æ˜¯ä¸€ä¸ªç”±å¤šä¸ªé¢‘ç‡å åŠ è€Œæˆçš„å”¯ä¸€ã€ŒæŒ‡çº¹ã€ã€‚ä½ç»´åˆ†é‡å˜åŒ–å¿«ï¼ŒåŒºåˆ†ç›¸é‚»ä½ç½®ï¼›é«˜ç»´åˆ†é‡å˜åŒ–æ…¢ï¼ŒåŒºåˆ†è¿œè·ç¦»ä½ç½®ã€‚"
      },
      {
        "type": "h3",
        "text": "ç›¸å¯¹ä½ç½®çš„çº¿æ€§å¯è¡¨ç¤ºæ€§"
      },
      {
        "type": "p",
        "text": "æ­£å¼¦ç¼–ç çš„ä¸€ä¸ªå…³é”®æ•°å­¦æ€§è´¨ï¼šå¯¹äºä»»æ„å›ºå®šåç§» kï¼Œ<strong>PE<sub>pos+k</sub></strong> å¯ä»¥è¡¨ç¤ºä¸º <strong>PE<sub>pos</sub></strong> çš„çº¿æ€§å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œå­˜åœ¨ä¸€ä¸ªåªä¾èµ– kï¼ˆä¸ä¾èµ– posï¼‰çš„çŸ©é˜µ M<sub>k</sub>ï¼Œä½¿å¾— PE<sub>pos+k</sub> = M<sub>k</sub> Â· PE<sub>pos</sub>ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥é€šè¿‡çº¿æ€§è¿ç®—æ¥æ•æ‰ç›¸å¯¹ä½ç½®å…³ç³»â€”â€”ã€Œè¿™ä¸ªè¯åœ¨é‚£ä¸ªè¯åé¢ç¬¬3ä¸ªä½ç½®ã€ã€‚"
      },
      {
        "type": "h3",
        "text": "åŠ æ³• vs æ‹¼æ¥"
      },
      {
        "type": "p",
        "text": "ä½ç½®ç¼–ç ä¸è¯åµŒå…¥æ˜¯<strong>ç›´æ¥ç›¸åŠ </strong>çš„ï¼šinput = WordEmbedding + PositionalEncodingã€‚é€‰æ‹©åŠ æ³•è€Œéæ‹¼æ¥ï¼ˆconcatenationï¼‰æ˜¯å› ä¸ºï¼šæ‹¼æ¥ä¼šä½¿ç»´åº¦ä» 512 å¢åˆ° 1024ï¼Œæ‰€æœ‰åç»­å±‚çš„å‚æ•°éƒ½è¦ç¿»å€ã€‚åŠ æ³•ä¿æŒäº†ç»´åº¦ä¸å˜ï¼Œè€Œåç»­çš„çº¿æ€§æŠ•å½±ï¼ˆQ/K/V çŸ©é˜µï¼‰å¯ä»¥éšå¼åœ°å­¦ä¹ å¦‚ä½•åˆ†ç¦»ä½ç½®ä¿¡æ¯å’Œè¯­ä¹‰ä¿¡æ¯ã€‚"
      },
      {
        "type": "h3",
        "text": "æ­£å¼¦ç¼–ç  vs å­¦ä¹ ç¼–ç "
      },
      {
        "type": "p",
        "text": "è®ºæ–‡è¿˜å®éªŒäº†<strong>å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥</strong>ï¼ˆæ¯ä¸ªä½ç½®ä¸€ä¸ªå¯è®­ç»ƒå‘é‡ï¼‰ï¼Œç»“æœä¸æ­£å¼¦ç¼–ç å‡ ä¹æ— å·®å¼‚ï¼ˆè¡¨3(E)ï¼š25.7 vs 25.8ï¼‰ã€‚ä½†æ­£å¼¦ç¼–ç æœ‰ä¸€ä¸ªç†è®ºä¼˜åŠ¿ï¼šå®ƒæ˜¯æ•°å­¦å‡½æ•°ï¼Œå¯¹ä»»æ„ pos éƒ½æœ‰å®šä¹‰ï¼Œå› æ­¤å¯èƒ½æ³›åŒ–åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿åºåˆ—ã€‚è€Œå­¦ä¹ ç¼–ç æ˜¯æŸ¥è¡¨æ“ä½œï¼Œè¶…è¿‡æœ€å¤§è®­ç»ƒé•¿åº¦å°±æ²¡æœ‰å¯¹åº”å‘é‡äº†ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆç”¨åŠ æ³•è€Œä¸æ˜¯æ‹¼æ¥ï¼Ÿ",
        "en": "The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed.",
        "zh": "ä½ç½®ç¼–ç çš„ç»´åº¦ä¸åµŒå…¥ç›¸åŒï¼Œå‡ä¸º d_modelï¼Œå› æ­¤ä¸¤è€…å¯ä»¥ç›´æ¥æ±‚å’Œã€‚",
        "a": "æ‹¼æ¥ä¼šä½¿ç»´åº¦ä»512å¢åˆ°1024ï¼Œæ‰€æœ‰åç»­å±‚å‚æ•°ç¿»å€ã€‚åŠ æ³•ä¿æŒ512ç»´ï¼Œåç»­çº¿æ€§å˜æ¢å¯éšå¼åˆ†ç¦»ä½ç½®å’Œè¯­ä¹‰ä¿¡æ¯ã€‚"
      },
      {
        "q": "åºåˆ—è¶…è¿‡è®­ç»ƒæœ€å¤§é•¿åº¦æ—¶ï¼Œæ­£å¼¦ç¼–ç  vs å­¦ä¹ ç¼–ç ï¼Ÿ",
        "en": "We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.",
        "zh": "æˆ‘ä»¬é€‰æ‹©æ­£å¼¦ç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒå¯èƒ½å…è®¸æ¨¡å‹å¤–æ¨åˆ°æ¯”è®­ç»ƒä¸­é‡åˆ°çš„æ›´é•¿çš„åºåˆ—é•¿åº¦ã€‚",
        "a": "æ­£å¼¦ç¼–ç æ˜¯æ•°å­¦å‡½æ•°ï¼Œä»»æ„ pos éƒ½æœ‰å®šä¹‰ï¼Œç†è®ºä¸Šå¯å¤–æ¨ã€‚å­¦ä¹ ç¼–ç æ˜¯æŸ¥è¡¨ï¼Œè¶…è¿‡è®­ç»ƒæœ€å¤§é•¿åº¦å°±æ²¡æœ‰å¯¹åº”å‘é‡äº†ã€‚è¡¨3(E)æ˜¾ç¤ºè®­ç»ƒå†…ä¸¤è€…å‡ ä¹æ— å·®å¼‚(25.8 vs 25.7)ã€‚"
      },
      {
        "q": "ç”»å‡º pos=0,1,2,3 åœ¨å‰å‡ ç»´çš„å€¼ï¼Œæœ‰ä»€ä¹ˆè§„å¾‹ï¼Ÿ",
        "en": "Each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2Ï€ to 10000Â·2Ï€.",
        "zh": "ä½ç½®ç¼–ç çš„æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªæ­£å¼¦æ›²çº¿ã€‚æ³¢é•¿æ„æˆä» 2Ï€ åˆ° 10000Â·2Ï€ çš„ç­‰æ¯”æ•°åˆ—ã€‚",
        "a": "ä½ç»´(i=0)å˜åŒ–å¿«ï¼Œç›¸é‚»ä½ç½®å·®å¼‚å¤§ï¼›é«˜ç»´å˜åŒ–æ…¢ã€‚å¦‚åŒæ—¶é’Ÿï¼šç§’é’ˆ(ä½ç»´)åŒºåˆ†ç²¾ç»†æ—¶åˆ»ï¼Œæ—¶é’ˆ(é«˜ç»´)åŒºåˆ†å¤§è‡´æ—¶æ®µã€‚å¤šé¢‘ç‡å åŠ ä½¿æ¯ä¸ªä½ç½®æœ‰å”¯ä¸€ã€ŒæŒ‡çº¹ã€ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬6é¡µ Section 3.5"
  },
  {
    "id": "training",
    "label": "è®­ç»ƒç­–ç•¥",
    "emoji": "ğŸ”§",
    "color": "#ec4899",
    "x": 30,
    "y": 74,
    "section": "ç¬¬5èŠ‚ï¼šTraining",
    "pages": [
      7,
      8
    ],
    "scrollTo": 7,
    "highlights": [
      {
        "page": 7,
        "top": 48,
        "height": 52
      },
      {
        "page": 8,
        "top": 0,
        "height": 45
      }
    ],
    "keyQuestion": "è¿™ä¹ˆå¤§çš„æ¨¡å‹å¦‚ä½•è®­ç»ƒï¼Ÿæœ‰å“ªäº›ç²¾å·§çš„è®­ç»ƒæŠ€å·§ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "è®­ç»ƒæ•°æ®ä¸ç¡¬ä»¶"
      },
      {
        "type": "p",
        "text": "è‹±â†’å¾·ç¿»è¯‘ä½¿ç”¨ WMT 2014 æ•°æ®é›†ï¼Œçº¦ <strong>450ä¸‡</strong>å¥å¯¹ï¼Œè¯è¡¨é‡‡ç”¨ BPEï¼ˆByte Pair Encodingï¼‰ï¼Œæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€å…±äº«çº¦ 37000 ä¸ª token çš„è¯è¡¨ã€‚è‹±â†’æ³•ç¿»è¯‘ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼Œçº¦ <strong>3600ä¸‡</strong>å¥å¯¹ï¼Œè¯è¡¨ 32000 ä¸ª tokenã€‚"
      },
      {
        "type": "p",
        "text": "ç¡¬ä»¶é…ç½®ï¼šä¸€å°æœºå™¨é…å¤‡ <strong>8 å— NVIDIA P100 GPU</strong>ã€‚Base æ¨¡å‹è®­ç»ƒçº¦ 10ä¸‡æ­¥ï¼Œæ¯æ­¥ 0.4 ç§’ï¼Œå…±çº¦ <strong>12å°æ—¶</strong>ã€‚Big æ¨¡å‹è®­ç»ƒ 30ä¸‡æ­¥ï¼Œæ¯æ­¥ 1.0 ç§’ï¼Œå…±çº¦ <strong>3.5å¤©</strong>ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŒæœŸçš„ GNMT æ¨¡å‹éœ€è¦åœ¨ 96 å— GPU ä¸Šè®­ç»ƒæ•°å‘¨ã€‚"
      },
      {
        "type": "h3",
        "text": "å­¦ä¹ ç‡è°ƒåº¦ï¼šWarmup + Inverse Sqrt Decay"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡ä½¿ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„å­¦ä¹ ç‡å…¬å¼ï¼š<strong>lr = d<sub>model</sub><sup>-0.5</sup> Â· min(step<sup>-0.5</sup>, step Â· warmup_steps<sup>-1.5</sup>)</strong>"
      },
      {
        "type": "p",
        "text": "è¿™ä¸ªå…¬å¼åˆ†ä¸¤ä¸ªé˜¶æ®µï¼šå‰ <strong>4000 æ­¥</strong>çº¿æ€§å‡æ¸©ï¼ˆä»0åˆ°å³°å€¼ï¼‰ï¼Œä¹‹åæŒ‰ step<sup>-0.5</sup> ç¼“æ…¢è¡°å‡ã€‚Warmup çš„å¿…è¦æ€§åœ¨äºï¼šè®­ç»ƒåˆæœŸå‚æ•°æ˜¯éšæœºçš„ï¼Œæ¢¯åº¦æ–¹å‘ä¸å¯é ã€‚å¦‚æœä¸€å¼€å§‹å°±ç”¨å¤§å­¦ä¹ ç‡ï¼Œå‚æ•°ä¼šå‰§çƒˆéœ‡è¡ç”šè‡³å‘æ•£ã€‚å…ˆç”¨å°å­¦ä¹ ç‡ã€Œç¨³ä½ã€æ¨¡å‹ï¼Œå†é€æ­¥æé«˜ï¼Œæœ€åè¡°å‡åšç²¾ç»†è°ƒæ•´ã€‚è¿™ä¸ªè°ƒåº¦ç­–ç•¥åæ¥è¢«å¹¿æ³›é‡‡ç”¨ï¼Œç”šè‡³è¢«ç§°ä¸ºã€ŒTransformer learning rate scheduleã€ã€‚"
      },
      {
        "type": "h3",
        "text": "æ­£åˆ™åŒ–ç­–ç•¥ä¸€ï¼šDropout"
      },
      {
        "type": "p",
        "text": "åœ¨æ¯ä¸ªå­å±‚çš„è¾“å‡ºï¼ˆè¿›å…¥æ®‹å·®è¿æ¥ä¹‹å‰ï¼‰ä»¥åŠ embedding ä¸ä½ç½®ç¼–ç ç›¸åŠ ä¹‹åï¼Œéƒ½æ–½åŠ äº† <strong>P<sub>drop</sub>=0.1</strong> çš„ Dropoutã€‚å³éšæœºå°†10%çš„ç¥ç»å…ƒè¾“å‡ºç½®é›¶ï¼Œè¿«ä½¿æ¨¡å‹ä¸ä¾èµ–äºä»»ä½•ç‰¹å®šçš„æ¿€æ´»è·¯å¾„ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚Big æ¨¡å‹åœ¨å°æ•°æ®é›†ï¼ˆè‹±â†’å¾·ï¼Œ450ä¸‡å¥ï¼‰ä¸Šä½¿ç”¨ P<sub>drop</sub>=0.3ã€‚"
      },
      {
        "type": "h3",
        "text": "æ­£åˆ™åŒ–ç­–ç•¥äºŒï¼šLabel Smoothing"
      },
      {
        "type": "p",
        "text": "ä¼ ç»Ÿäº¤å‰ç†µæŸå¤±çš„ç›®æ ‡æ˜¯ one-hot åˆ†å¸ƒï¼ˆæ­£ç¡®è¯æ¦‚ç‡ä¸º1ï¼Œå…¶ä½™ä¸º0ï¼‰ã€‚Label Smoothing ç”¨ Îµ=0.1 å°†ç›®æ ‡ã€Œè½¯åŒ–ã€ä¸ºï¼šæ­£ç¡®è¯æ¦‚ç‡ 0.9ï¼Œå…¶ä½™è¯å‡åŒ€åˆ†é… 0.1 çš„æ¦‚ç‡ã€‚"
      },
      {
        "type": "p",
        "text": "ä¸€ä¸ªçœ‹ä¼¼çŸ›ç›¾çš„ç»“æœï¼šLabel Smoothing <strong>æŸå®³äº†å›°æƒ‘åº¦ï¼ˆperplexityï¼‰</strong>ï¼Œä½†<strong>æå‡äº† BLEU åˆ†æ•°</strong>ã€‚åŸå› æ˜¯å›°æƒ‘åº¦å¥–åŠ±ã€Œç¡®å®šæ€§ã€ï¼ˆç»™æ­£ç¡®è¯æ›´é«˜æ¦‚ç‡ï¼‰ï¼Œä½†è¿‡åº¦ç¡®å®šä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚Label Smoothing é¼“åŠ±æ¨¡å‹ä¿æŒã€Œåˆç†çš„ä¸ç¡®å®šæ€§ã€ï¼Œç±»ä¼¼äºã€Œç†è§£ > æ­»è®°ç¡¬èƒŒã€ï¼Œç»“æœæ˜¯ç¿»è¯‘è´¨é‡æ›´å¥½ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¼˜åŒ–å™¨"
      },
      {
        "type": "p",
        "text": "ä½¿ç”¨ <strong>Adam ä¼˜åŒ–å™¨</strong>ï¼Œå‚æ•° Î²<sub>1</sub>=0.9, Î²<sub>2</sub>=0.98, Îµ=10<sup>-9</sup>ã€‚Adam ç»“åˆäº†åŠ¨é‡ï¼ˆmomentumï¼‰å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆRMSpropï¼‰ï¼Œç‰¹åˆ«é€‚åˆè®­ç»ƒå…·æœ‰å¤§é‡å‚æ•°çš„ Transformerã€‚Î²<sub>2</sub>=0.98ï¼ˆé€šå¸¸é»˜è®¤0.999ï¼‰æ„å‘³ç€æ›´åŠ å…³æ³¨è¿‘æœŸçš„æ¢¯åº¦å¹³æ–¹ï¼Œé€‚åº” Transformer è®­ç»ƒä¸­æ¢¯åº¦å˜åŒ–è¾ƒå¿«çš„ç‰¹ç‚¹ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆ warmup æ˜¯å¿…è¦çš„ï¼Ÿ",
        "en": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.",
        "zh": "è¿™å¯¹åº”äºåœ¨å‰ warmup_steps ä¸ªè®­ç»ƒæ­¥ä¸­çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œä¹‹åæŒ‰æ­¥æ•°çš„å¹³æ–¹æ ¹å€’æ•°æ¯”ä¾‹é€’å‡ã€‚æˆ‘ä»¬ä½¿ç”¨ warmup_steps=4000ã€‚",
        "a": "åˆæœŸå‚æ•°éšæœºï¼Œæ¢¯åº¦æ–¹å‘ä¸å¯é ã€‚å¤§å­¦ä¹ ç‡ä¼šå¯¼è‡´å‚æ•°å‰§çƒˆéœ‡è¡ç”šè‡³å‘æ•£ã€‚Warmup è®©æ¨¡å‹å…ˆã€Œç¨³ä½è„šè·Ÿã€ï¼Œå†æ­£å¸¸å­¦ä¹ ï¼ŒåæœŸè¡°å‡åšç²¾ç»†è°ƒæ•´ã€‚"
      },
      {
        "q": "Label smoothing ä¸ºä»€ä¹ˆæŸå®³å›°æƒ‘åº¦å´æå‡ BLEUï¼Ÿ",
        "en": "This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.",
        "zh": "è¿™ä¼šæŸå®³å›°æƒ‘åº¦ï¼Œå› ä¸ºæ¨¡å‹å­¦ä¼šäº†æ›´åŠ ä¸ç¡®å®šï¼Œä½†æé«˜äº†å‡†ç¡®ç‡å’Œ BLEU åˆ†æ•°ã€‚",
        "a": "å›°æƒ‘åº¦å¥–åŠ±ç¡®å®šæ€§ï¼ˆç»™æ­£ç¡®ç­”æ¡ˆé«˜æ¦‚ç‡ï¼‰ï¼ŒLabel smoothing å¼ºåˆ¶ä¸ç¡®å®šï¼Œæ‰€ä»¥å›°æƒ‘åº¦å·®ã€‚ä½† BLEU åªçœ‹ç¿»è¯‘è´¨é‡ï¼Œä¸ç¡®å®šçš„æ¨¡å‹åè€Œæ›´é²æ£’ä¸è¿‡æ‹Ÿåˆã€‚å¦‚åŒã€Œç†è§£ > æ­»è®°ç¡¬èƒŒã€ã€‚"
      },
      {
        "q": "Big æ¨¡å‹ä¸ºä»€ä¹ˆéœ€è¦æ›´å¼ºçš„ dropout(0.3)ï¼Ÿ",
        "en": "The Transformer (big) model trained for English-to-French used dropout rate P_drop = 0.1, instead of 0.3.",
        "zh": "ç”¨äºè‹±æ³•ç¿»è¯‘è®­ç»ƒçš„ Transformer (big) æ¨¡å‹ä½¿ç”¨äº† P_drop=0.1 çš„ dropout ç‡ï¼Œè€Œé 0.3ã€‚",
        "a": "Big æ¨¡å‹ 213M å‚æ•°(base 65M)ï¼Œæ›´æ˜“è¿‡æ‹Ÿåˆã€‚æ›´å¼º dropout è¿«ä½¿æ¨¡å‹ä¸ä¾èµ–ç‰¹å®šè·¯å¾„ã€‚ä½†è‹±æ³•æ•°æ®é›†å¤§(3600ä¸‡å¥ vs è‹±å¾·450ä¸‡)ï¼Œè¿‡æ‹Ÿåˆé£é™©ä½ï¼Œæ‰€ä»¥ç”¨0.1å°±å¤Ÿã€‚æœ€ä¼˜ dropout å–å†³äºã€Œæ¨¡å‹å®¹é‡/æ•°æ®é‡ã€æ¯”ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬7-8é¡µ Section 5"
  },
  {
    "id": "results",
    "label": "å®éªŒä¸æ¶ˆè",
    "emoji": "ğŸ“Š",
    "color": "#6366f1",
    "x": 70,
    "y": 74,
    "section": "ç¬¬6èŠ‚ï¼šResults",
    "pages": [
      8,
      9
    ],
    "scrollTo": 8,
    "highlights": [
      {
        "page": 8,
        "top": 10,
        "height": 50
      },
      {
        "page": 9,
        "top": 0,
        "height": 100
      }
    ],
    "keyQuestion": "å®éªŒæ•°æ®æ­ç¤ºäº†å“ªäº›è®¾è®¡æ´å¯Ÿï¼Ÿæ¯ä¸ªç»„ä»¶çš„è´¡çŒ®æœ‰å¤šå¤§ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "ä¸»å®éªŒç»“æœï¼ˆTable 2ï¼‰"
      },
      {
        "type": "p",
        "text": "è‹±â†’å¾·ç¿»è¯‘ï¼šTransformer Big å–å¾— <strong>28.4 BLEU</strong>ï¼Œè¶…è¶Šæ‰€æœ‰å·²å‘è¡¨çš„å•æ¨¡å‹å’Œé›†æˆæ¨¡å‹ï¼ˆåŒ…æ‹¬ GNMT+RL çš„ 26.30ï¼‰ã€‚è‹±â†’æ³•ç¿»è¯‘ï¼š<strong>41.8 BLEU</strong>ï¼ŒåŒæ ·æ˜¯æ–°çš„ SOTAã€‚"
      },
      {
        "type": "p",
        "text": "æ›´æƒŠäººçš„æ˜¯<strong>è®­ç»ƒæ•ˆç‡</strong>ï¼šTransformer Base çš„è®­ç»ƒ FLOPs ä¸º 3.3Ã—10<sup>18</sup>ï¼Œè€Œ GNMT+RL ä¸º 2.3Ã—10<sup>19</sup>ï¼ˆ<strong>7å€</strong>ï¼‰ï¼ŒConvS2S Ensemble ä¸º 7.7Ã—10<sup>19</sup>ï¼ˆ<strong>23å€</strong>ï¼‰ã€‚Transformer åœ¨æ•ˆæœå’Œæ•ˆç‡ä¸Šéƒ½å®ç°äº†è´¨çš„é£è·ƒã€‚"
      },
      {
        "type": "h3",
        "text": "æ¶ˆèå®éªŒè¯¦è§£ï¼ˆTable 3ï¼‰"
      },
      {
        "type": "p",
        "text": "æ¶ˆèå®éªŒæ˜¯è®ºæ–‡æœ€æœ‰ä»·å€¼çš„éƒ¨åˆ†ä¹‹ä¸€ï¼Œæ¯è¡Œæ­ç¤ºäº†ä¸€ä¸ªè®¾è®¡å†³ç­–çš„å½±å“ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>(A) æ³¨æ„åŠ›å¤´æ•°</strong>ï¼šh=1â†’24.9, h=8â†’25.8(æœ€ä½³), h=16â†’25.8(æŒå¹³), h=32â†’25.4(ä¸‹é™)ã€‚æ€»ç»´åº¦æ’å®šï¼Œå¤´è¶Šå¤šæ¯å¤´è¶Šçª„ï¼ˆh=32æ—¶d<sub>k</sub>=16ï¼‰ï¼Œå¤ªçª„çš„å­ç©ºé—´æ— æ³•å»ºæ¨¡æœ‰æ„ä¹‰çš„å…³ç³»ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>(B) æ³¨æ„åŠ› key ç»´åº¦</strong>ï¼šd<sub>k</sub>=16â†’25.3, d<sub>k</sub>=32â†’25.5ã€‚ç›´æ¥è¯æ˜äº†æ³¨æ„åŠ›å…¼å®¹æ€§å‡½æ•°çš„è´¨é‡å–å†³äº key ç©ºé—´çš„ä¸°å¯Œç¨‹åº¦ï¼Œæ›´å¤æ‚çš„å…¼å®¹æ€§å‡½æ•°å¯èƒ½å¸¦æ¥æ›´å¥½æ•ˆæœã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>(C)(D) æ¨¡å‹è§„æ¨¡</strong>ï¼šd<sub>model</sub> å’Œ d<sub>ff</sub> è¶Šå¤§è¶Šå¥½ï¼Œä½†è¾¹é™…é€’å‡ã€‚Big æ¨¡å‹ï¼ˆd<sub>model</sub>=1024, d<sub>ff</sub>=4096, h=16ï¼‰åœ¨è‹±-å¾·ä¸Šè¾¾åˆ° 26.4ã€‚è¿™é¢„ç¤ºäº†åæ¥ã€ŒScaling Lawã€çš„å‘ç°ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>(D) Dropout</strong>ï¼šæ—  dropout â†’ BLEU éª¤é™è‡³ 24.6ï¼Œè¯æ˜æ­£åˆ™åŒ–å¯¹äº Transformer è‡³å…³é‡è¦ï¼ˆæ³¨æ„åŠ›æœºåˆ¶çš„å…¨å±€è¿æ¥ä½¿æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆç‰¹å®šæ¨¡å¼ï¼‰ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>(E) ä½ç½®ç¼–ç </strong>ï¼šå­¦ä¹ ç¼–ç  25.7 vs æ­£å¼¦ç¼–ç  25.8ï¼Œå·®å¼‚å¯å¿½ç•¥ã€‚æ­£å¼¦ç¼–ç çš„å¤–æ¨èƒ½åŠ›æ˜¯é™„åŠ ä¼˜åŠ¿ã€‚"
      },
      {
        "type": "h3",
        "text": "æ³›åŒ–å®éªŒï¼šè‹±è¯­æˆåˆ†å¥æ³•åˆ†æï¼ˆSection 6.3ï¼‰"
      },
      {
        "type": "p",
        "text": "ä¸ºéªŒè¯ Transformer ä¸ä»…ä»…æ˜¯ç¿»è¯‘ä¸“ç”¨æ¶æ„ï¼Œè®ºæ–‡åœ¨<strong>è‹±è¯­æˆåˆ†å¥æ³•åˆ†æ</strong>ï¼ˆconstituency parsingï¼‰ä¸Šè¿›è¡Œäº†å®éªŒã€‚è¿™ä¸ªä»»åŠ¡ä¸ç¿»è¯‘æœ‰å¾ˆå¤§ä¸åŒï¼šè¾“å‡ºæ˜¯ç»“æ„åŒ–çš„æ ‘ï¼ˆæ‹¬å·è¡¨è¾¾å¼ï¼‰ï¼Œæœ‰ä¸¥æ ¼çš„è¯­æ³•çº¦æŸï¼Œè¾“å‡ºé€šå¸¸æ¯”è¾“å…¥é•¿ã€‚"
      },
      {
        "type": "p",
        "text": "ç»“æœï¼šåœ¨ä»…ä½¿ç”¨ WSJ è®­ç»ƒæ•°æ®çš„æ¡ä»¶ä¸‹å–å¾— <strong>91.3 F1</strong>ï¼Œä½¿ç”¨åŠç›‘ç£å­¦ä¹ åè¾¾åˆ° <strong>92.7 F1</strong>ï¼Œè¶…è¶Šäº†æ‰€æœ‰ä¹‹å‰çš„æ¨¡å‹ï¼ˆé™¤ Recurrent Neural Network Grammarï¼‰ã€‚è¿™æœ‰åŠ›è¯æ˜äº† Transformer æ˜¯<strong>é€šç”¨çš„åºåˆ—å»ºæ¨¡æ¶æ„</strong>ï¼Œä¸å±€é™äºç¿»è¯‘ä»»åŠ¡â€”â€”ä¸ºåç»­ GPTã€BERT ç­‰é€šç”¨é¢„è®­ç»ƒæ¨¡å‹å¥ å®šäº†å®éªŒåŸºç¡€ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "Table 2 çš„ FLOPs åˆ—æ­ç¤ºäº†ä»€ä¹ˆï¼Ÿ",
        "en": "We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.",
        "zh": "æˆ‘ä»¬é€šè¿‡å°†è®­ç»ƒæ—¶é—´ã€GPU æ•°é‡ä»¥åŠæ¯ä¸ª GPU æŒç»­å•ç²¾åº¦æµ®ç‚¹è¿ç®—èƒ½åŠ›çš„ä¼°è®¡å€¼ç›¸ä¹˜ï¼Œæ¥ä¼°ç®—è®­ç»ƒæ¨¡å‹æ‰€ç”¨çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚",
        "a": "Transformer base FLOPs 3.3Ã—10<sup>18</sup>ï¼ŒGNMT+RL 2.3Ã—10<sup>19</sup>(7å€)ï¼ŒConvS2S Ensemble 7.7Ã—10<sup>19</sup>(23å€)ã€‚å¹¶è¡Œæ€§è®©8ä¸ªGPUçœŸæ­£å¹¶è¡Œ+å•æ¨¡å‹è¶…è¶Šé›†æˆâ†’æ•ˆç‡æ•ˆæœåŒèµ¢ã€‚"
      },
      {
        "q": "ä¸ºä»€ä¹ˆä¸æ˜¯å¤´è¶Šå¤šè¶Šå¥½ï¼Ÿ",
        "en": "While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.",
        "zh": "è™½ç„¶å•å¤´æ³¨æ„åŠ›æ¯”æœ€ä½³è®¾ç½®å·®0.9ä¸ª BLEUï¼Œä½†å¤´æ•°è¿‡å¤šæ—¶è´¨é‡ä¹Ÿä¼šä¸‹é™ã€‚",
        "a": "æ€»è®¡ç®—é‡æ’å®šï¼Œå¤´è¶Šå¤šæ¯å¤´ç»´åº¦ d_k=512/h è¶Šå°ã€‚h=32 æ—¶ d_k=16ï¼Œæ— æ³•æ•æ‰æœ‰æ„ä¹‰çš„è¯­ä¹‰å…³ç³»ã€‚8-16å¤´æ˜¯ã€Œå¤šæ ·æ€§ vs ç²¾åº¦ã€çš„æœ€ä½³å¹³è¡¡ã€‚"
      },
      {
        "q": "é€‰æ‹© constituency parsing éªŒè¯æ³›åŒ–èƒ½åŠ›çš„æ„ä¹‰ï¼Ÿ",
        "en": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.",
        "zh": "ä¸ºè¯„ä¼° Transformer æ˜¯å¦èƒ½æ³›åŒ–åˆ°å…¶ä»–ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨è‹±è¯­æˆåˆ†å¥æ³•åˆ†æä¸Šè¿›è¡Œäº†å®éªŒã€‚",
        "a": "å¦‚æœåªå±•ç¤ºç¿»è¯‘ç»“æœï¼Œè¯´æœåŠ›æœ‰é™ã€‚å¥æ³•åˆ†æä¸ç¿»è¯‘å·®å¼‚å¤§ï¼ˆå¼ºç»“æ„çº¦æŸã€è¾“å‡ºæ¯”è¾“å…¥é•¿ï¼‰ï¼Œåœ¨æœªåšç‰¹æ®Šè°ƒä¼˜çš„æƒ…å†µä¸‹ä»å–å¾—ä¼˜ç§€ç»“æœ(92.7 F1)ï¼Œè¯æ˜ Transformer æ˜¯é€šç”¨åºåˆ—å»ºæ¨¡æ¶æ„â€”â€”ä¸º GPT/BERT å¥ å®šåŸºç¡€ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬8-10é¡µ Section 6"
  },
  {
    "id": "selfattention",
    "label": "ä¸ºä½•è‡ªæ³¨æ„åŠ›",
    "emoji": "ğŸ§ ",
    "color": "#f97316",
    "x": 50,
    "y": 95,
    "section": "ç¬¬4èŠ‚ï¼šWhy Self-Attention",
    "pages": [
      6,
      7
    ],
    "scrollTo": 6,
    "highlights": [
      {
        "page": 6,
        "top": 48,
        "height": 52
      },
      {
        "page": 7,
        "top": 0,
        "height": 50
      }
    ],
    "keyQuestion": "ä¸ºä»€ä¹ˆååé€‰æ‹© Self-Attentionï¼Ÿå®ƒåœ¨ä¸‰ä¸ªå…³é”®ç»´åº¦ä¸Šå¦‚ä½•ç¢¾å‹ç«äº‰å¯¹æ‰‹ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "è®ºæ–‡çš„æ ¸å¿ƒè®ºè¯æ¡†æ¶"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡ç¬¬4èŠ‚æ˜¯å…¨æ–‡æœ€å…·ç†è®ºæ·±åº¦çš„éƒ¨åˆ†ã€‚ä½œè€…ä»ä¸‰ä¸ªç»´åº¦ç³»ç»Ÿæ¯”è¾ƒäº† Self-Attentionã€RNN å’Œ CNNï¼š<strong>â‘  æ¯å±‚è®¡ç®—å¤æ‚åº¦</strong>ï¼ˆæ€»è¿ç®—é‡ï¼‰ã€<strong>â‘¡ æœ€å°‘ä¸²è¡Œæ“ä½œæ•°</strong>ï¼ˆå†³å®šå¹¶è¡Œèƒ½åŠ›ï¼‰ã€<strong>â‘¢ æœ€å¤§è·¯å¾„é•¿åº¦</strong>ï¼ˆå½±å“é•¿è·ç¦»ä¾èµ–å­¦ä¹ ï¼‰ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸‰ç§æœºåˆ¶çš„å®šé‡å¯¹æ¯”ï¼ˆTable 1ï¼‰"
      },
      {
        "type": "bullet",
        "text": "<strong>Self-Attention</strong>ï¼šè®¡ç®— O(nÂ²Â·d)ï¼Œä¸²è¡Œ O(1)ï¼Œè·¯å¾„ O(1)ã€‚æ‰€æœ‰ä½ç½®å¯¹åœ¨ä¸€æ¬¡çŸ©é˜µä¹˜æ³•ä¸­åŒæ—¶å®Œæˆï¼Œä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´ç›´æ¥ç›¸è¿ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>RNNï¼ˆRecurrentï¼‰</strong>ï¼šè®¡ç®— O(nÂ·dÂ²)ï¼Œä¸²è¡Œ O(n)ï¼Œè·¯å¾„ O(n)ã€‚æ¯æ­¥å¤„ç†ä¸€ä¸ªä½ç½®ï¼Œå¿…é¡»ä»å¤´åˆ°å°¾ä¾æ¬¡è®¡ç®—ã€‚ä¿¡æ¯ä»ä½ç½®1ä¼ åˆ°ä½ç½®nè¦ç»è¿‡n-1æ­¥ã€‚"
      },
      {
        "type": "bullet",
        "text": "<strong>CNNï¼ˆConvolutionalï¼‰</strong>ï¼šè®¡ç®— O(kÂ·nÂ·dÂ²)ï¼Œä¸²è¡Œ O(1)ï¼Œè·¯å¾„ O(log<sub>k</sub>(n))ã€‚å•å±‚å·ç§¯åªçœ‹ k ä¸ªç›¸é‚»ä½ç½®ï¼ˆå±€éƒ¨æ„Ÿå—é‡ï¼‰ï¼Œè¿æ¥ä»»æ„ä¸¤ä¸ªä½ç½®éœ€è¦ O(log<sub>k</sub>(n)) å±‚ï¼ˆdilated convolutionï¼‰æˆ– O(n/k) å±‚ï¼ˆæ™®é€šå·ç§¯ï¼‰ã€‚"
      },
      {
        "type": "h3",
        "text": "å…³é”®æƒè¡¡ï¼šn vs d"
      },
      {
        "type": "p",
        "text": "Self-Attention çš„è®¡ç®—å¤æ‚åº¦æ˜¯ O(nÂ²Â·d)ï¼ŒRNN æ˜¯ O(nÂ·dÂ²)ã€‚å½“ <strong>n &lt; d</strong> æ—¶ï¼ˆè¿™åœ¨ NLP ä¸­éå¸¸å¸¸è§â€”â€”å…¸å‹å¥å­é•¿åº¦ n=20~100ï¼Œè€Œ d<sub>model</sub>=512ï¼‰ï¼ŒSelf-Attention æ›´é«˜æ•ˆã€‚ä½†å½“ <strong>n &gt; d</strong> æ—¶ï¼ˆå¦‚å¤„ç†æ•´ç¯‡æ–‡æ¡£ã€é«˜åˆ†è¾¨ç‡å›¾åƒï¼‰ï¼ŒO(nÂ²) ä¼šæˆä¸ºç“¶é¢ˆã€‚"
      },
      {
        "type": "h3",
        "text": "å—é™è‡ªæ³¨æ„åŠ›ï¼ˆRestricted Self-Attentionï¼‰"
      },
      {
        "type": "p",
        "text": "é’ˆå¯¹è¶…é•¿åºåˆ—ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªå‰ç»æ€§çš„å˜ä½“ï¼šå°†æ¯ä¸ªä½ç½®çš„æ³¨æ„åŠ›èŒƒå›´é™åˆ¶åœ¨å‘¨å›´å¤§å°ä¸º r çš„é‚»åŸŸå†…ã€‚è®¡ç®—é‡é™ä¸º O(rÂ·nÂ·d)ï¼Œä½†æœ€å¤§è·¯å¾„é•¿åº¦å¢åŠ åˆ° O(n/r)ã€‚è¿™æ˜¯<strong>ç²¾åº¦-æ•ˆç‡æƒè¡¡</strong>çš„ä¸€ä¸ªæ—©æœŸæ€è€ƒï¼Œåæ¥å‚¬ç”Ÿäº† Longformerï¼ˆsliding window + globalï¼‰ã€Sparse Transformerã€Linear Attention ç­‰ä¸€ç³»åˆ—é«˜æ•ˆå˜ä½“ã€‚"
      },
      {
        "type": "h3",
        "text": "è¢«ä½ä¼°çš„ä¼˜åŠ¿ï¼šå¯è§£é‡Šæ€§"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡è¿˜æŒ‡å‡º Self-Attention çš„ä¸€ä¸ªé™„å¸¦å¥½å¤„ï¼šæ³¨æ„åŠ›æƒé‡çŸ©é˜µæœ¬èº«å°±æ˜¯ä¸€å¼ ã€Œå…³æ³¨åº¦çƒ­åŠ›å›¾ã€ï¼Œå¯ä»¥ç›´è§‚åœ°çœ‹åˆ°æ¨¡å‹åœ¨å…³æ³¨ä»€ä¹ˆã€‚é™„å½•ä¸­çš„å›¾3-5æ­£æ˜¯åˆ©ç”¨è¿™ä¸€ç‰¹æ€§è¿›è¡Œçš„å¯è§†åŒ–åˆ†æï¼Œæ­ç¤ºäº†ä¸åŒæ³¨æ„åŠ›å¤´è‡ªåŠ¨å­¦åˆ°äº†ä¸åŒçš„è¯­è¨€ç»“æ„ã€‚è¿™ç§å¯è§£é‡Šæ€§åœ¨ RNN å’Œ CNN ä¸­å¾ˆéš¾å®ç°ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä»€ä¹ˆåœºæ™¯ä¸‹ n>dï¼ŒSelf-Attention ä¸å†æœ‰ä¼˜åŠ¿ï¼Ÿ",
        "en": "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence.",
        "zh": "ä¸ºæé«˜æ¶‰åŠè¶…é•¿åºåˆ—ä»»åŠ¡çš„è®¡ç®—æ€§èƒ½ï¼Œè‡ªæ³¨æ„åŠ›å¯ä»¥è¢«é™åˆ¶ä¸ºä»…è€ƒè™‘è¾“å…¥åºåˆ—ä¸­å¤§å°ä¸º r çš„é‚»åŸŸã€‚",
        "a": "å›¾åƒ(256Ã—256=65536åƒç´ )ã€é•¿æ–‡æ¡£(æ•°ä¸‡è¯)ã€éŸ³é¢‘æ³¢å½¢(æ¯ç§’æ•°ä¸‡é‡‡æ ·)ã€‚åç»­ Longformerã€Sparse Transformer éƒ½æ˜¯è§£å†³ O(nÂ²) çš„æ”¹è¿›ã€‚"
      },
      {
        "q": "O(1) è·¯å¾„é•¿åº¦å¯¹è®­ç»ƒæ„å‘³ç€ä»€ä¹ˆï¼Ÿ",
        "en": "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.",
        "zh": "è¾“å…¥å’Œè¾“å‡ºåºåˆ—ä¸­ä»»æ„ä½ç½®ç»„åˆä¹‹é—´çš„è·¯å¾„è¶ŠçŸ­ï¼Œå­¦ä¹ é•¿è·ç¦»ä¾èµ–å°±è¶Šå®¹æ˜“ã€‚",
        "a": "æ¢¯åº¦ä¸€æ­¥ç›´è¾¾ä»»æ„è¾“å…¥ä½ç½®ï¼Œä¸åƒ RNN ç»è¿‡ n-1 æ¬¡çŸ©é˜µä¹˜æ³•(æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸)ã€‚ç»“æœï¼š(1)æœ‰æ•ˆå­¦ä¹ è¿œè·ç¦»ä¾èµ– (2)æ¢¯åº¦ç¨³å®š (3)è®­ç»ƒå¿«æ”¶æ•›ã€‚"
      },
      {
        "q": "å¦‚ä½•ä¸º n=100000 çš„è¶…é•¿åºåˆ—ä¿®æ”¹ Self-Attentionï¼Ÿ",
        "en": "A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.",
        "zh": "å•ä¸ªæ ¸å®½åº¦ k<n çš„å·ç§¯å±‚ä¸èƒ½è¿æ¥æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºä½ç½®å¯¹ã€‚",
        "a": "æ–¹æ¡ˆï¼š(1) ç¨€ç–æ³¨æ„åŠ›(Sparse Transformer)åªç®—éƒ¨åˆ†ä½ç½®å¯¹ (2) çº¿æ€§æ³¨æ„åŠ›ç”¨æ ¸å‡½æ•°è¿‘ä¼¼softmaxâ†’O(nÂ·d) (3) æ»‘åŠ¨çª—å£+å…¨å±€token(Longformer) (4) åˆ†å—å¤„ç†ã€‚è®ºæ–‡æå‡ºçš„ restricted self-attention æ­£æ˜¯æ–¹æ¡ˆ1çš„é›å½¢ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬6-7é¡µ Section 4 + Table 1"
  }
];
const connections = [
  {
    "from": "problem",
    "to": "attention"
  },
  {
    "from": "problem",
    "to": "selfattention"
  },
  {
    "from": "attention",
    "to": "multihead"
  },
  {
    "from": "multihead",
    "to": "architecture"
  },
  {
    "from": "ffn",
    "to": "architecture"
  },
  {
    "from": "positional",
    "to": "architecture"
  },
  {
    "from": "architecture",
    "to": "training"
  },
  {
    "from": "architecture",
    "to": "results"
  },
  {
    "from": "selfattention",
    "to": "architecture"
  }
];
let selectedId = null;
const expandedQs = new Set();
let pagesLoaded = false;

function getNode(id){return nodes.find(n=>n.id===id);}

function loadPages(){
  if(pagesLoaded)return;
  pagesLoaded=true;
  const pane=document.getElementById('paperPane');
  pane.innerHTML='';
  for(let i=1;i<=TOTAL_PAGES;i++){
    const w=document.createElement('div');
    w.className='page-wrapper';
    w.id='page-'+i;
    w.innerHTML='<div class="page-label">Page '+i+'</div><img src="pages/page_'+i+'.png" alt="Page '+i+'" loading="lazy">';
    pane.appendChild(w);
  }
}

function highlightPages(node){
  document.querySelectorAll('.highlight-overlay').forEach(e=>e.remove());
  if(!node||!node.highlights)return;
  node.highlights.forEach(h=>{
    const pw=document.getElementById('page-'+h.page);
    if(!pw)return;
    const ov=document.createElement('div');
    ov.className='highlight-overlay';
    ov.style.top=h.top+'%';
    ov.style.height=h.height+'%';
    ov.style.borderColor=node.color;
    ov.style.background=node.color+'15';
    ov.style.boxShadow='0 0 20px '+node.color+'30';
    pw.appendChild(ov);
  });
  // Scroll to target page
  setTimeout(()=>{
    const target=document.getElementById('page-'+node.scrollTo);
    if(target){target.scrollIntoView({behavior:'smooth',block:'start'});}
  },100);
}

function renderMap(){
  const svg=document.getElementById('mapSvg');
  const box=document.getElementById('mapBox');
  let h='';
  connections.forEach(c=>{
    const f=getNode(c.from),t=getNode(c.to);
    if(!f||!t)return;
    h+='<line x1="'+f.x+'%" y1="'+(f.y+3)+'%" x2="'+t.x+'%" y2="'+t.y+'%" stroke="#cbd5e1" stroke-width="1.5" stroke-dasharray="5 3" opacity="0.5"/>';
  });
  svg.innerHTML=h;
  box.querySelectorAll('.node-btn').forEach(e=>e.remove());
  nodes.forEach(n=>{
    const btn=document.createElement('button');
    btn.className='node-btn';
    btn.style.left=n.x+'%';btn.style.top=n.y+'%';
    btn.onclick=()=>selectNode(n.id);
    const a=selectedId===n.id;
    const bg=a?n.color:'white',fg=a?'white':'#1e293b';
    const sh=a?'0 4px 16px '+n.color+'44':'0 1px 4px rgba(0,0,0,0.08)';
    const sc=a?'scale(1.08)':'scale(1)';
    btn.innerHTML='<div class="node-chip" style="background:'+bg+';border-color:'+n.color+';color:'+fg+';box-shadow:'+sh+';transform:'+sc+';"><span class="emoji">'+n.emoji+'</span><span>'+n.label+'</span></div>';
    box.appendChild(btn);
  });
}

function toggleQ(key){
  if(expandedQs.has(key))expandedQs.delete(key);else expandedQs.add(key);
  renderDetail();
}

function renderDetail(){
  const ct=document.getElementById('detailContainer');
  const hint=document.getElementById('emptyHint');
  if(!selectedId){ct.innerHTML='';hint.style.display='block';return;}
  hint.style.display='none';
  const n=getNode(selectedId);
  let ch='';
  n.content.forEach(i=>{
    if(i.type==='h3')ch+='<h3>'+i.text+'</h3>';
    else if(i.type==='p')ch+='<p>'+i.text+'</p>';
    else if(i.type==='bullet')ch+='<div class="bullet"><span class="dot">â€¢</span><span>'+i.text+'</span></div>';
    else if(i.type==='step')ch+='<p class="step-label">'+i.text+'</p>';
  });
  let th='';
  n.thinkAbout.forEach((item,idx)=>{
    const key=n.id+'-'+idx;
    const open=expandedQs.has(key);
    th+='<div>';
    th+='<div class="qa-question" onclick="toggleQ(\''+key+'\')">';
    th+='<span class="arrow">'+(open?'â–¼':'â–¶')+'</span>';
    th+='<span class="qtext">'+item.q+'</span></div>';
    if(open){
      th+='<div class="qa-detail">';
      th+='<div class="qa-block" style="background:#fef3c7;border-color:#f59e0b;"><div class="block-label" style="color:#92400e;">ORIGINAL TEXT</div><p class="block-text" style="color:#78350f;font-style:italic;">'+item.en+'</p></div>';
      th+='<div class="qa-block" style="background:#ecfdf5;border-color:#10b981;"><div class="block-label" style="color:#065f46;">ä¸­æ–‡å­¦æœ¯ç¿»è¯‘</div><p class="block-text" style="color:#064e3b;">'+item.zh+'</p></div>';
      th+='<div class="qa-block" style="background:#eff6ff;border-color:#3b82f6;"><div class="block-label" style="color:#1e40af;">è§£ç­”</div><div class="block-text" style="color:#1e3a5f;">'+item.a+'</div></div>';
      th+='</div>';
    }
    th+='</div>';
  });
  ct.innerHTML=
    '<div class="detail-panel" style="border-color:'+n.color+'40;">'+
    '<div class="detail-header" style="background:'+n.color+'10;">'+
    '<div class="info"><h2 style="color:'+n.color+';">'+n.emoji+' '+n.label+'</h2><div class="section">'+n.section+'</div></div>'+
    '<button class="close-btn" onclick="selectNode(null)">Ã—</button></div>'+
    '<div class="detail-body">'+
    '<div class="key-question" style="background:'+n.color+'08;border:1px solid '+n.color+'20;"><div class="label" style="color:'+n.color+';">æ ¸å¿ƒé—®é¢˜</div><div class="text">'+n.keyQuestion+'</div></div>'+
    '<div class="content-area">'+ch+'</div>'+
    '<div class="think-box"><div class="title">å¸¦ç€è¿™äº›é—®é¢˜å»è¯»åŸæ–‡ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</div>'+th+'</div>'+
    '<div class="paper-ref">'+n.paperRef+'</div></div></div>';
}

function selectNode(id){
  selectedId=(selectedId===id)?null:id;
  expandedQs.clear();
  loadPages();
  renderMap();
  renderDetail();
  highlightPages(selectedId?getNode(selectedId):null);
  if(selectedId){
    setTimeout(()=>{
      const dc=document.getElementById('detailContainer');
      if(dc&&dc.firstChild)dc.firstChild.scrollIntoView({behavior:'smooth',block:'start'});
    },150);
  }
}

// Draggable divider
(function(){
  const div=document.getElementById('divider');
  const pp=document.getElementById('paperPane');
  let dragging=false;
  div.addEventListener('mousedown',e=>{dragging=true;e.preventDefault();});
  document.addEventListener('mousemove',e=>{
    if(!dragging)return;
    const pct=Math.min(70,Math.max(25,e.clientX/window.innerWidth*100));
    pp.style.width=pct+'%';
  });
  document.addEventListener('mouseup',()=>{dragging=false;});
})();

renderMap();
renderDetail();
</script>
</body>
</html>