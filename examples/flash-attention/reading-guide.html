<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness â€” äº¤äº’å¼é˜…è¯»æŒ‡å—</title>
<style>
*{margin:0;padding:0;box-sizing:border-box;}
body{font-family:-apple-system,"Noto Sans SC","PingFang SC",sans-serif;background:#f1f5f9;height:100vh;overflow:hidden;display:flex;}

/* LEFT: Paper viewer */
.paper-pane{width:42%;height:100vh;background:#525659;overflow-y:auto;position:relative;flex-shrink:0;}
.paper-pane::-webkit-scrollbar{width:8px;}
.paper-pane::-webkit-scrollbar-thumb{background:#888;border-radius:4px;}
.paper-hint{color:#94a3b8;text-align:center;padding:40px 20px;font-size:14px;line-height:1.8;}
.page-wrapper{position:relative;margin:8px auto;width:95%;}
.page-wrapper img{width:100%;display:block;border-radius:4px;box-shadow:0 2px 8px rgba(0,0,0,0.3);}
.page-label{position:absolute;top:8px;right:12px;background:rgba(0,0,0,0.5);color:#fff;font-size:11px;padding:2px 8px;border-radius:4px;z-index:2;}
.highlight-overlay{position:absolute;left:0;width:100%;border:3px solid;border-radius:6px;pointer-events:none;z-index:1;transition:all 0.3s;}

/* RIGHT: Guide */
.guide-pane{flex:1;height:100vh;overflow-y:auto;padding:20px 24px;background:linear-gradient(135deg,#f8fafc 0%,#f0f4ff 50%,#faf5ff 100%);}
.guide-pane::-webkit-scrollbar{width:8px;}
.guide-pane::-webkit-scrollbar-thumb{background:#cbd5e1;border-radius:4px;}

.header{text-align:center;margin-bottom:16px;}
.header h1{font-size:22px;font-weight:800;color:#1e293b;}
.header p{color:#94a3b8;font-size:12px;margin-top:4px;}

.map-box{background:#fff;border-radius:14px;border:1px solid #e2e8f0;padding:12px;margin-bottom:16px;position:relative;height:360px;box-shadow:0 1px 3px rgba(0,0,0,0.04);}
.map-svg{position:absolute;inset:0;width:100%;height:100%;z-index:0;}
.node-btn{position:absolute;transform:translate(-50%,-50%);z-index:1;background:none;border:none;cursor:pointer;transition:all 0.2s;}
.node-chip{border-radius:14px;padding:7px 14px;box-shadow:0 1px 4px rgba(0,0,0,0.08);display:flex;align-items:center;gap:6px;border:2px solid;transition:all 0.2s;font-size:12px;font-weight:600;white-space:nowrap;}
.node-chip .emoji{font-size:14px;}
.node-btn:hover .node-chip{transform:scale(1.05);}

.detail-panel{border-radius:14px;border:1px solid;box-shadow:0 4px 20px rgba(0,0,0,0.06);overflow:hidden;margin-bottom:16px;background:#fafbfd;animation:slideIn 0.25s ease-out;}
@keyframes slideIn{from{opacity:0;transform:translateY(10px);}to{opacity:1;transform:translateY(0);}}
.detail-header{padding:16px 20px;display:flex;justify-content:space-between;align-items:center;}
.detail-header .info h2{font-size:18px;font-weight:700;display:flex;align-items:center;gap:6px;}
.detail-header .info .section{font-size:11px;color:#94a3b8;margin-top:2px;}
.detail-header .close-btn{font-size:24px;color:#94a3b8;background:none;border:none;cursor:pointer;padding:0 4px;line-height:1;}
.detail-body{padding:0 20px 20px;}
.key-question{border-radius:10px;padding:12px 14px;margin-bottom:14px;}
.key-question .label{font-size:12px;font-weight:600;}
.key-question .text{font-size:13px;color:#334155;margin-top:3px;line-height:1.6;}
.content-area{color:#475569;}
.content-area h3{font-size:14px;font-weight:700;color:#1e293b;margin:14px 0 4px;}
.content-area p{font-size:13px;line-height:1.7;margin-bottom:4px;}
.content-area .bullet{display:flex;gap:6px;font-size:13px;margin-left:6px;margin-bottom:3px;line-height:1.6;}
.content-area .bullet .dot{color:#94a3b8;margin-top:1px;flex-shrink:0;}
.content-area .step-label{font-size:13px;font-weight:600;color:#1e293b;margin-top:8px;}
.content-area strong{color:#1e293b;font-weight:600;}
.think-box{background:#fffbeb;border:1px solid #fde68a;border-radius:10px;padding:12px 14px;margin-top:16px;}
.think-box .title{font-size:12px;font-weight:600;color:#b45309;margin-bottom:8px;}
.qa-question{cursor:pointer;display:flex;gap:6px;align-items:flex-start;padding:6px 0;border-radius:6px;transition:background 0.15s;}
.qa-question:hover{background:rgba(245,158,11,0.08);}
.qa-question .arrow{color:#b45309;flex-shrink:0;font-size:11px;margin-top:3px;width:14px;}
.qa-question .qtext{color:#92400e;font-size:13px;line-height:1.6;}
.qa-detail{margin:0 0 10px 18px;animation:slideIn 0.2s ease-out;}
.qa-block{padding:10px 12px;border-radius:0 8px 8px 0;margin-bottom:8px;border-left:3px solid;}
.qa-block .block-label{font-size:10px;font-weight:700;margin-bottom:4px;text-transform:uppercase;letter-spacing:0.5px;}
.qa-block .block-text{font-size:12px;line-height:1.7;margin:0;}
.paper-ref{margin-top:10px;font-size:11px;color:#94a3b8;font-style:italic;}
.empty-hint{text-align:center;padding:24px;color:#94a3b8;font-size:13px;}

/* Divider handle */
.divider{width:5px;background:#cbd5e1;cursor:col-resize;flex-shrink:0;transition:background 0.2s;}
.divider:hover{background:#94a3b8;}
</style>
</head>
<body>
<div class="paper-pane" id="paperPane">
  <div class="paper-hint" id="paperHint">
    â† é€‰æ‹©å³ä¾§æ¦‚å¿µèŠ‚ç‚¹å<br>è¿™é‡Œä¼šè·³è½¬åˆ°å¯¹åº”çš„è®ºæ–‡åŸæ–‡<br>å¹¶é«˜äº®æ ‡å‡ºç›¸å…³ç« èŠ‚
  </div>
</div>
<div class="divider" id="divider"></div>
<div class="guide-pane" id="guidePane">
  <div class="header">
    <h1 id="titleEl"></h1>
    <p>ç‚¹å‡»æ¦‚å¿µèŠ‚ç‚¹ â†’ å·¦ä¾§è·³è½¬åŸæ–‡å¹¶é«˜äº® Â· å³ä¾§å±•å¼€è®²è§£</p>
  </div>
  <div class="map-box" id="mapBox">
    <svg class="map-svg" id="mapSvg"></svg>
  </div>
  <div id="detailContainer"></div>
  <div class="empty-hint" id="emptyHint">ç‚¹å‡»ä¸Šæ–¹æ¦‚å¿µèŠ‚ç‚¹ï¼Œå¼€å§‹æ¢ç´¢</div>
</div>

<script>
const PAPER_TITLE = "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness";
const TOTAL_PAGES = 34;
const PAGES_DIR = "pages";
const nodes = [
  {
    "id": "problem",
    "label": "é—®é¢˜èƒŒæ™¯",
    "emoji": "ğŸ¯",
    "color": "#ef4444",
    "x": 50,
    "y": 8,
    "section": "ç¬¬1èŠ‚ï¼šIntroduction",
    "pages": [
      1,
      2
    ],
    "scrollTo": 1,
    "highlights": [
      {
        "page": 1,
        "top": 15,
        "height": 80
      },
      {
        "page": 2,
        "top": 0,
        "height": 50
      }
    ],
    "keyQuestion": "Attention çœŸæ­£çš„æ€§èƒ½ç“¶é¢ˆä¸æ˜¯è®¡ç®—é‡ï¼Œè€Œæ˜¯ä»€ä¹ˆï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "é—®é¢˜çš„èµ·ç‚¹ï¼šTransformer çš„ O(NÂ²) å›°å¢ƒ"
      },
      {
        "type": "p",
        "text": "Self-Attention çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦éƒ½æ˜¯åºåˆ—é•¿åº¦ N çš„å¹³æ–¹ã€‚å¯¹äº GPT-2ï¼ˆN=1024, d=64ï¼‰ï¼Œæ³¨æ„åŠ›çŸ©é˜µ S=QK<sup>T</sup> çš„å¤§å°ä¸º 1024Ã—1024 = 1M ä¸ªå…ƒç´ ã€‚å½“ N å¢é•¿åˆ° 4Kã€16K ç”šè‡³ 64K æ—¶ï¼ˆå¦‚é•¿æ–‡æ¡£ã€é«˜åˆ†è¾¨ç‡å›¾åƒï¼‰ï¼Œè¿™ä¸ªçŸ©é˜µä¼šçˆ†ç‚¸å¼å¢é•¿ï¼Œæˆä¸ºæ¨¡å‹æ‰©å±•çš„ä¸»è¦éšœç¢ã€‚"
      },
      {
        "type": "h3",
        "text": "ç°æœ‰æ–¹æ¡ˆçš„å¤±è´¥ï¼šè¿‘ä¼¼æ³¨æ„åŠ›"
      },
      {
        "type": "p",
        "text": "2022å¹´ä¹‹å‰ï¼Œå­¦æœ¯ç•Œæå‡ºäº†å¤§é‡è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•ï¼ˆLinformerã€Performerã€Reformer ç­‰ï¼‰ï¼Œè¯•å›¾å°† O(NÂ²) é™åˆ° O(N) æˆ– O(N log N)ã€‚å®ƒä»¬çš„å…±åŒç­–ç•¥æ˜¯<strong>ç‰ºç‰²æ¨¡å‹è´¨é‡æ¢å–è®¡ç®—é‡</strong>ã€‚ä½†è®ºæ–‡æŒ‡å‡ºä¸€ä¸ªå…³é”®äº‹å®ï¼šè¿™äº›æ–¹æ³•è™½ç„¶å‡å°‘äº† FLOPsï¼Œå´<strong>å¾€å¾€æ²¡æœ‰å®ç°çœŸæ­£çš„ wall-clock åŠ é€Ÿ</strong>ã€‚"
      },
      {
        "type": "h3",
        "text": "è¢«å¿½è§†çš„çœŸç›¸ï¼šå†…å­˜è®¿é—®æ‰æ˜¯ç“¶é¢ˆ"
      },
      {
        "type": "p",
        "text": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ´å¯Ÿåœ¨äºï¼šç°ä»£ GPU çš„<strong>è®¡ç®—é€Ÿåº¦è¿œè¶…å†…å­˜å¸¦å®½</strong>ã€‚A100 GPU çš„è®¡ç®—èƒ½åŠ›æ˜¯ 312 TFLOPSï¼Œä½† HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰çš„å¸¦å®½åªæœ‰ 1.5~2.0 TB/sã€‚å¤§éƒ¨åˆ† Transformer æ“ä½œå®é™…ä¸Šæ˜¯<strong>å†…å­˜å—é™çš„ï¼ˆmemory-boundï¼‰</strong>ï¼Œè€Œéè®¡ç®—å—é™çš„ã€‚è®ºæ–‡å¼•ç”¨äº† Ivanov et al. [43] çš„ç ”ç©¶ï¼šã€ŒData movement is all you needã€ã€‚"
      },
      {
        "type": "h3",
        "text": "GPU å†…å­˜å±‚çº§ï¼šHBM vs SRAM"
      },
      {
        "type": "p",
        "text": "GPU æœ‰ä¸¤çº§ä¸»è¦å­˜å‚¨ï¼š<strong>HBMï¼ˆHigh Bandwidth Memoryï¼‰</strong>å®¹é‡å¤§ï¼ˆA100: 40GBï¼‰ä½†å¸¦å®½ç›¸å¯¹ä½ï¼ˆ1.5 TB/sï¼‰ï¼Œ<strong>SRAMï¼ˆç‰‡ä¸Šç¼“å­˜ï¼‰</strong>å®¹é‡æå°ï¼ˆA100: ~20MBï¼‰ä½†å¸¦å®½æé«˜ï¼ˆ19 TB/sï¼‰ã€‚æ ‡å‡†æ³¨æ„åŠ›å®ç°éœ€è¦åå¤åœ¨ HBM ä¸­è¯»å†™å·¨å¤§çš„ NÃ—N çŸ©é˜µï¼Œè€Œè¿™äº›è¯»å†™æ“ä½œæ‰æ˜¯çœŸæ­£çš„æ—¶é—´æ€æ‰‹ã€‚"
      },
      {
        "type": "h3",
        "text": "è®ºæ–‡çš„æ ¸å¿ƒä¸»å¼ "
      },
      {
        "type": "p",
        "text": "ä½œè€…æå‡ºäº†ä¸€ä¸ªè¢«æ·±åº¦å­¦ä¹ ç¤¾åŒºé•¿æœŸå¿½è§†çš„åŸåˆ™ï¼š<strong>è®©æ³¨æ„åŠ›ç®—æ³•å…·æœ‰ IO æ„ŸçŸ¥æ€§ï¼ˆIO-awareï¼‰</strong>â€”â€”åœ¨ç®—æ³•è®¾è®¡ä¸­æ˜¾å¼è€ƒè™‘ä¸åŒå±‚çº§å†…å­˜ä¹‹é—´çš„æ•°æ®è¯»å†™ã€‚è¿™ä¸€åŸåˆ™åœ¨æ•°æ®åº“ã€å›¾åƒå¤„ç†ã€æ•°å€¼çº¿æ€§ä»£æ•°ç­‰é¢†åŸŸæ—©å·²æˆç†Ÿï¼Œä½† PyTorch/TensorFlow ç­‰æ¡†æ¶ç¼ºä¹å¯¹å†…å­˜è®¿é—®çš„ç²¾ç»†æ§åˆ¶ï¼Œå¯¼è‡´æ·±åº¦å­¦ä¹ ç¤¾åŒºä¸€ç›´æ²¡æœ‰é‡‡çº³ã€‚"
      },
      {
        "type": "h3",
        "text": "æˆæœé¢„è§ˆ"
      },
      {
        "type": "p",
        "text": "FlashAttention å®ç°äº†ï¼šBERT-large è®­ç»ƒåŠ é€Ÿ 15%ï¼ˆè¶…è¶Š MLPerf 1.1 è®°å½•ï¼‰ã€GPT-2 è®­ç»ƒåŠ é€Ÿ 3Ã—ã€é•¿åºåˆ—ä»»åŠ¡ï¼ˆLRAï¼‰åŠ é€Ÿ 2.4Ã—ã€‚åŒæ—¶æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡ï¼šGPT-2 æ‰©å±•åˆ° 4K ä¸Šä¸‹æ–‡ï¼ˆå›°æƒ‘åº¦æå‡ 0.7ï¼‰ï¼Œé¦–æ¬¡åœ¨ Path-Xï¼ˆåºåˆ—é•¿åº¦ 16Kï¼‰ä¸Šè¾¾åˆ°ä¼˜äºéšæœºçš„è¡¨ç°ã€‚å…³é”®æ˜¯â€”â€”<strong>ç»“æœå®Œå…¨ç²¾ç¡®ï¼Œä¸æ˜¯è¿‘ä¼¼</strong>ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆå‡å°‘ FLOPs çš„è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•åè€Œæ²¡æœ‰å®ç° wall-clock åŠ é€Ÿï¼Ÿ",
        "en": "Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware.",
        "zh": "è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•è¯•å›¾é€šè¿‡ç‰ºç‰²æ¨¡å‹è´¨é‡æ¥é™ä½è®¡ç®—å¤æ‚åº¦ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å¾€å¾€æœªèƒ½å®ç°å®é™…çš„å¢™é’Ÿæ—¶é—´åŠ é€Ÿã€‚æˆ‘ä»¬è®¤ä¸ºç¼ºå¤±çš„åŸåˆ™æ˜¯ä½¿æ³¨æ„åŠ›ç®—æ³•å…·æœ‰ IO æ„ŸçŸ¥æ€§ã€‚",
        "a": "å› ä¸ºæ³¨æ„åŠ›æ˜¯å†…å­˜å—é™æ“ä½œï¼Œä¸æ˜¯è®¡ç®—å—é™ã€‚å‡å°‘ FLOPs ä½†ä¸å‡å°‘å†…å­˜è®¿é—®ï¼Œå°±åƒä¿®äº†æ›´å®½çš„é«˜é€Ÿå…¬è·¯ä½†æ”¶è´¹ç«™è¿˜æ˜¯ä¸€ä¸ªâ€”â€”ç“¶é¢ˆæ²¡å˜ã€‚FlashAttention ç›´å‡»æ ¹æºï¼šå‡å°‘ HBM è¯»å†™æ¬¡æ•°ã€‚"
      },
      {
        "q": "A100 GPU çš„ SRAM å’Œ HBM å¸¦å®½å·®å¤šå°‘ï¼Ÿè¿™ä¸ªå·®è·æ„å‘³ç€ä»€ä¹ˆï¼Ÿ",
        "en": "GPUs, compute speed has out-paced memory speed, and most operations in Transformers are bottlenecked by memory accesses.",
        "zh": "GPU çš„è®¡ç®—é€Ÿåº¦å·²ç»è¶…è¶Šäº†å†…å­˜é€Ÿåº¦ï¼ŒTransformer ä¸­çš„å¤§éƒ¨åˆ†æ“ä½œéƒ½å—é™äºå†…å­˜è®¿é—®ã€‚",
        "a": "SRAM 19 TB/s vs HBM 1.5 TB/sï¼Œç›¸å·®çº¦ 13 å€ã€‚è¿™æ„å‘³ç€å¦‚æœèƒ½æŠŠè®¡ç®—æ”¾åœ¨ SRAM ä¸­å®Œæˆï¼ˆè€Œéåå¤è¯»å†™ HBMï¼‰ï¼Œç†è®ºä¸Šå¯ä»¥è·å¾—æ•°é‡çº§çš„åŠ é€Ÿã€‚FlashAttention æ­£æ˜¯é€šè¿‡ tiling å°†è®¡ç®—æ¬åˆ° SRAM ä¸­ã€‚"
      },
      {
        "q": "ä¸ºä»€ä¹ˆ PyTorch/TensorFlow ä¸èƒ½è‡ªåŠ¨åšåˆ° IO æ„ŸçŸ¥ï¼Ÿ",
        "en": "However, common Python interfaces to deep learning such as PyTorch and Tensorflow do not allow fine-grained control of memory access.",
        "zh": "ç„¶è€Œï¼ŒPyTorch å’Œ TensorFlow ç­‰å¸¸ç”¨çš„æ·±åº¦å­¦ä¹  Python æ¥å£ä¸å…è®¸å¯¹å†…å­˜è®¿é—®è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚",
        "a": "PyTorch çš„ autograd å°†æ¯ä¸ªæ“ä½œï¼ˆmatmulã€softmaxã€dropoutï¼‰ç¼–è¯‘ä¸ºç‹¬ç«‹çš„ CUDA kernelã€‚æ¯ä¸ª kernel éƒ½ä» HBM è¯»è¾“å…¥ã€å†™è¾“å‡ºã€‚å¤šä¸ª kernel ä¹‹é—´çš„ä¸­é—´ç»“æœå¿…é¡»ç»è¿‡ HBM ä¸­è½¬ã€‚è¦çœŸæ­£å‡å°‘ IOï¼Œéœ€è¦æ‰‹å†™èåˆçš„ CUDA kernelâ€”â€”å°†æ•´ä¸ªæ³¨æ„åŠ›è®¡ç®—åˆå¹¶ä¸ºä¸€ä¸ª kernelï¼Œè¿™æ­£æ˜¯ FlashAttention çš„å®ç°æ–¹å¼ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬1-2é¡µ Abstract + Introduction"
  },
  {
    "id": "standard_attn",
    "label": "æ ‡å‡†æ³¨æ„åŠ›å®ç°",
    "emoji": "ğŸ“‹",
    "color": "#f59e0b",
    "x": 25,
    "y": 30,
    "section": "ç¬¬2.2èŠ‚ï¼šStandard Attention Implementation",
    "pages": [
      4
    ],
    "scrollTo": 4,
    "highlights": [
      {
        "page": 3,
        "top": 70,
        "height": 30
      },
      {
        "page": 4,
        "top": 0,
        "height": 85
      }
    ],
    "keyQuestion": "æ ‡å‡†æ³¨æ„åŠ›å®ç°ä¸ºä»€ä¹ˆæ…¢ï¼Ÿå®ƒçš„ HBM è®¿é—®é‡åˆ°åº•æœ‰å¤šå¤§ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "æ ‡å‡†å®ç°çš„ä¸‰æ­¥æµç¨‹ï¼ˆAlgorithm 0ï¼‰"
      },
      {
        "type": "p",
        "text": "æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—åˆ†ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„æ­¥éª¤ï¼Œæ¯æ­¥éƒ½æ˜¯ä¸€ä¸ª CUDA kernelï¼š"
      },
      {
        "type": "step",
        "text": "â‘  ä» HBM åŠ è½½ Q, Kï¼Œè®¡ç®— S = QK<sup>T</sup> âˆˆ â„<sup>NÃ—N</sup>ï¼Œå°† S å†™å› HBM"
      },
      {
        "type": "step",
        "text": "â‘¡ ä» HBM è¯»å– Sï¼Œè®¡ç®— P = softmax(S) âˆˆ â„<sup>NÃ—N</sup>ï¼Œå°† P å†™å› HBM"
      },
      {
        "type": "step",
        "text": "â‘¢ ä» HBM åŠ è½½ P å’Œ Vï¼Œè®¡ç®— O = PV âˆˆ â„<sup>NÃ—d</sup>ï¼Œå°† O å†™å› HBM"
      },
      {
        "type": "h3",
        "text": "é—®é¢˜ï¼šNÃ—N çŸ©é˜µçš„åå¤è¯»å†™"
      },
      {
        "type": "p",
        "text": "å…³é”®é—®é¢˜åœ¨äºä¸­é—´çŸ©é˜µ S å’Œ P çš„å¤§å°éƒ½æ˜¯ <strong>NÃ—N</strong>ã€‚å¯¹äº N=1024, d=64 çš„ GPT-2ï¼ŒS å’Œ P å„æœ‰ 100 ä¸‡ä¸ªå…ƒç´ ï¼ˆ4MB float32ï¼‰ï¼Œè¿œå¤§äº Qã€Kã€V å„è‡ªçš„ 65536 ä¸ªå…ƒç´ ï¼ˆ256KBï¼‰ã€‚ä¸‰æ­¥æµç¨‹éœ€è¦æŠŠè¿™äº› NÃ—N çŸ©é˜µåå¤å†™å…¥å’Œè¯»å‡º HBMã€‚"
      },
      {
        "type": "h3",
        "text": "IO å¤æ‚åº¦åˆ†æ"
      },
      {
        "type": "p",
        "text": "æ ‡å‡†å®ç°çš„ HBM è®¿é—®é‡ä¸º <strong>Î˜(Nd + NÂ²)</strong>ã€‚å½“ N >> d æ—¶ï¼ˆå…¸å‹åœºæ™¯ï¼‰ï¼ŒNÂ² é¡¹å ä¸»å¯¼â€”â€”è¿™æ˜¯äºŒæ¬¡å¢é•¿çš„æ ¹æºã€‚ä»¥ GPT-2 ä¸ºä¾‹ï¼ˆN=1024, d=64ï¼‰ï¼šNd = 65K è€Œ NÂ² = 1Mï¼ŒHBM è¯»å†™é‡è¢« NÃ—N çŸ©é˜µä¸»å¯¼ã€‚"
      },
      {
        "type": "h3",
        "text": "é¢å¤–å¼€é”€ï¼šé€å…ƒç´ æ“ä½œ"
      },
      {
        "type": "p",
        "text": "å®é™…ä¸­è¿˜æœ‰æ›´å¤š memory-bound æ“ä½œï¼šå¯¹ S æ–½åŠ  <strong>masking</strong>ï¼ˆcausal maskï¼‰ã€å¯¹ P æ–½åŠ  <strong>dropout</strong>ã€‚æ¯ä¸ªæ“ä½œéƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹ kernelï¼Œåˆä¸€è½® HBM è¯»å†™ã€‚Megatron-LM ç­‰æ¡†æ¶å°è¯•å°† masking å’Œ softmax èåˆä¸ºä¸€ä¸ª kernelï¼Œä½†ä»æ— æ³•é¿å… NÃ—N çŸ©é˜µçš„ç‰©åŒ–ã€‚"
      },
      {
        "type": "h3",
        "text": "å®šé‡å¯¹æ¯”"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡ Figure 2 ç»™å‡ºäº†å…·ä½“æ•°æ®ï¼ˆGPT-2 medium, N=1024, d=64, 16 heads, batch=64, A100ï¼‰ï¼šæ ‡å‡†æ³¨æ„åŠ› FLOPs 66.6 GFLOPsï¼ŒHBM è¯»å†™ <strong>40.3 GB</strong>ï¼Œè¿è¡Œæ—¶é—´ 41.7msã€‚FlashAttention FLOPs 75.2 GFLOPsï¼ˆå¤šäº†13%ï¼‰ï¼ŒHBM è¯»å†™ <strong>4.4 GB</strong>ï¼ˆå°‘äº† 9.2Ã—ï¼‰ï¼Œè¿è¡Œæ—¶é—´ 7.3msï¼ˆå¿«äº† <strong>5.7Ã—</strong>ï¼‰ã€‚è®¡ç®—é‡å¢åŠ äº†ä½†æ—¶é—´å¤§å¹…å‡å°‘â€”â€”å®Œç¾éªŒè¯äº†ã€ŒIO æ‰æ˜¯ç“¶é¢ˆã€ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆ softmax ç‰¹åˆ«éš¾ä»¥èåˆï¼Ÿå®ƒä¸çŸ©é˜µä¹˜æ³•æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ",
        "en": "Standard attention implementations materialize the matrices S and P to HBM, which takes O(NÂ²) memory. Often N >> d (e.g., for GPT2, N = 1024 and d = 64).",
        "zh": "æ ‡å‡†æ³¨æ„åŠ›å®ç°å°†çŸ©é˜µ S å’Œ P ç‰©åŒ–åˆ° HBM ä¸­ï¼Œå ç”¨ O(NÂ²) å†…å­˜ã€‚é€šå¸¸ N >> dï¼ˆä¾‹å¦‚ GPT-2 ä¸­ N=1024, d=64ï¼‰ã€‚",
        "a": "çŸ©é˜µä¹˜æ³•å¯ä»¥åˆ†å—ç‹¬ç«‹è®¡ç®—ï¼ˆC çš„æ¯ä¸ªå—åªä¾èµ– Aã€B çš„å¯¹åº”è¡Œ/åˆ—å—ï¼‰ã€‚ä½† softmax éœ€è¦æ•´è¡Œçš„æœ€å¤§å€¼å’Œæ±‚å’Œå€¼â€”â€”å®ƒè€¦åˆäº† K çš„æ‰€æœ‰åˆ—ã€‚å¿…é¡»çœ‹å®Œæ•´è¡Œæ‰èƒ½å½’ä¸€åŒ–ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ ‡å‡†å®ç°å¿…é¡»å…ˆç‰©åŒ–æ•´ä¸ª S çŸ©é˜µã€‚FlashAttention ç”¨åœ¨çº¿ç®—æ³•ï¼ˆonline softmaxï¼‰è§£å†³äº†è¿™ä¸ªè€¦åˆã€‚"
      },
      {
        "q": "FlashAttention çš„ FLOPs åè€Œæ›´å¤šï¼ˆ75.2 vs 66.6ï¼‰ï¼Œä¸ºä»€ä¹ˆè¿˜æ›´å¿«ï¼Ÿ",
        "en": "HBM access is the primary factor affecting runtime.",
        "zh": "HBM è®¿é—®æ˜¯å½±å“è¿è¡Œæ—¶é—´çš„ä¸»è¦å› ç´ ã€‚",
        "a": "å› ä¸º GPU è¿ç®—æ˜¯é«˜åº¦æµæ°´çº¿åŒ–çš„ï¼Œå¤šå‡ºçš„ FLOPs å¯ä»¥è¢«å¹¶è¡Œè®¡ç®—å•å…ƒã€Œå…è´¹ã€å¸æ”¶ã€‚è€Œ HBM è¯»å†™å¿…é¡»ç­‰å¾…æ•°æ®ä¼ è¾“ï¼Œæ²¡æœ‰å…è´¹åˆé¤ã€‚æ ‡å‡†å®ç° 40.3GB IO vs FlashAttention 4.4GB IOâ€”â€”9.2Ã— çš„ IO å‡å°‘è¿œè¶… 13% çš„ FLOPs å¢åŠ ï¼Œå‡€æ•ˆæœæ˜¯ 5.7Ã— åŠ é€Ÿã€‚"
      },
      {
        "q": "Masking å’Œ Dropout ä¸ºä»€ä¹ˆä¼šè¿›ä¸€æ­¥æ¶åŒ– IO é—®é¢˜ï¼Ÿ",
        "en": "This problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to P.",
        "zh": "å…¶ä»–é€å…ƒç´ æ“ä½œè¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚å¯¹ S æ–½åŠ æ©ç æˆ–å¯¹ P æ–½åŠ  dropoutã€‚",
        "a": "æ¯ä¸ªé€å…ƒç´ æ“ä½œéƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹ CUDA kernelã€‚æ¯ä¸ª kernel éœ€è¦ä» HBM è¯»å–å®Œæ•´çš„ NÃ—N çŸ©é˜µï¼Œä¿®æ”¹åå†™å›ã€‚Masking è¯»å†™ä¸€æ¬¡ NÂ²ï¼ŒDropout åˆè¯»å†™ä¸€æ¬¡ NÂ²ã€‚æ€» IO ä» 3NÂ² å¢é•¿åˆ° 5NÂ² ä»¥ä¸Šã€‚FlashAttention å°†æ‰€æœ‰æ“ä½œèåˆè¿›ä¸€ä¸ª kernelï¼Œåªè¯»å†™ä¸€æ¬¡ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬3-4é¡µ Section 2.2 + Algorithm 0"
  },
  {
    "id": "online_softmax",
    "label": "åœ¨çº¿Softmax",
    "emoji": "ğŸ§®",
    "color": "#8b5cf6",
    "x": 75,
    "y": 30,
    "section": "ç¬¬3.1èŠ‚ï¼šTiling with Online Softmax",
    "pages": [
      5
    ],
    "scrollTo": 5,
    "highlights": [
      {
        "page": 5,
        "top": 0,
        "height": 55
      }
    ],
    "keyQuestion": "Softmax éœ€è¦çœ‹å®Œæ•´è¡Œæ‰èƒ½è®¡ç®—â€”â€”å¦‚ä½•ä¸€å—ä¸€å—åœ°å¢é‡å®Œæˆï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "æ ¸å¿ƒæŒ‘æˆ˜ï¼šSoftmax çš„å…¨å±€ä¾èµ–"
      },
      {
        "type": "p",
        "text": "softmax(x)<sub>i</sub> = e<sup>x<sub>i</sub></sup> / Î£<sub>j</sub> e<sup>x<sub>j</sub></sup>ã€‚åˆ†æ¯éœ€è¦å¯¹æ•´è¡Œæ±‚å’Œï¼Œæ„å‘³ç€ä½ å¿…é¡»çœ‹å®Œæ‰€æœ‰å…ƒç´ æ‰èƒ½è¾“å‡ºä»»ä½•ä¸€ä¸ªå…ƒç´ ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ ‡å‡†å®ç°å¿…é¡»å…ˆè®¡ç®—å¹¶å­˜å‚¨å®Œæ•´çš„ NÃ—N çŸ©é˜µâ€”â€”softmax å°† K çš„æ‰€æœ‰åˆ—<strong>è€¦åˆ</strong>åœ¨äº†ä¸€èµ·ã€‚"
      },
      {
        "type": "h3",
        "text": "æ•°å€¼ç¨³å®šçš„ Softmaxï¼šå‡æœ€å¤§å€¼"
      },
      {
        "type": "p",
        "text": "å®é™…å®ç°ä¸­ï¼Œä¸ºé˜²æ­¢æ•°å€¼æº¢å‡ºï¼Œä¼šå…ˆå‡å»è¡Œæœ€å¤§å€¼ï¼šsoftmax(x)<sub>i</sub> = e<sup>x<sub>i</sub>-m(x)</sup> / Î£<sub>j</sub> e<sup>x<sub>j</sub>-m(x)</sup>ï¼Œå…¶ä¸­ m(x) = max(x)ã€‚è¿™å¼•å…¥äº†ä¸¤ä¸ªéœ€è¦å…¨å±€ä¿¡æ¯çš„ç»Ÿè®¡é‡ï¼š<strong>m(x)</strong>ï¼ˆè¡Œæœ€å¤§å€¼ï¼‰å’Œ <strong>â„“(x)</strong>ï¼ˆå½’ä¸€åŒ–åˆ†æ¯ï¼‰ã€‚"
      },
      {
        "type": "h3",
        "text": "å…³é”®çªç ´ï¼šåˆ†å—å¢é‡æ›´æ–°"
      },
      {
        "type": "p",
        "text": "å‡è®¾æˆ‘ä»¬å·²ç»å¤„ç†äº†å‰ä¸€åŠ x<sup>(1)</sup>ï¼Œå¾—åˆ°äº† m(x<sup>(1)</sup>) å’Œ â„“(x<sup>(1)</sup>)ã€‚ç°åœ¨æ¥äº†åä¸€åŠ x<sup>(2)</sup>ã€‚æˆ‘ä»¬å¯ä»¥<strong>å¢é‡æ›´æ–°</strong>ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>æ–°çš„æœ€å¤§å€¼</strong>ï¼šm(x) = max(m(x<sup>(1)</sup>), m(x<sup>(2)</sup>))"
      },
      {
        "type": "bullet",
        "text": "<strong>æ–°çš„åˆ†æ¯</strong>ï¼šâ„“(x) = e<sup>m(x<sup>(1)</sup>)-m(x)</sup>Â·â„“(x<sup>(1)</sup>) + e<sup>m(x<sup>(2)</sup>)-m(x)</sup>Â·â„“(x<sup>(2)</sup>)"
      },
      {
        "type": "p",
        "text": "å…³é”®åœ¨äº<strong>ç¼©æ”¾å› å­</strong> e<sup>m<sub>old</sub>-m<sub>new</sub></sup>ï¼šå½“æ–°å—çš„æœ€å¤§å€¼æ›´å¤§æ—¶ï¼Œæ—§çš„ç´¯ç§¯ç»“æœéœ€è¦ä¹˜ä»¥è¿™ä¸ªè¡°å‡å› å­æ¥ã€Œä¿®æ­£ã€ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬åªéœ€ä¿å­˜ (m, â„“) ä¸¤ä¸ªæ ‡é‡å°±èƒ½æ­£ç¡®åˆå¹¶ä»»æ„æ•°é‡çš„å—ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¸ Output çš„ç»“åˆ"
      },
      {
        "type": "p",
        "text": "ä¸ä»… softmax åˆ†æ¯å¯ä»¥å¢é‡æ›´æ–°ï¼Œ<strong>æœ€ç»ˆè¾“å‡º O ä¹Ÿå¯ä»¥</strong>ã€‚å¤„ç†æ–°å—æ—¶ï¼Œå…ˆç”¨ç¼©æ”¾å› å­ä¿®æ­£æ—§çš„ Oï¼Œå†åŠ ä¸Šæ–°å—çš„è´¡çŒ®ï¼šO<sub>new</sub> = diag(â„“<sub>new</sub>)<sup>-1</sup>(diag(â„“<sub>old</sub>)Â·e<sup>m<sub>old</sub>-m<sub>new</sub></sup>Â·O<sub>old</sub> + e<sup>mÌƒ-m<sub>new</sub></sup>Â·PÌƒV<sub>j</sub>)ã€‚è¿™ä¸ªå…¬å¼æ˜¯ Algorithm 1 ç¬¬12è¡Œçš„æ ¸å¿ƒã€‚"
      },
      {
        "type": "h3",
        "text": "è¿™ä¸ªæŠ€æœ¯çš„æ¸Šæº"
      },
      {
        "type": "p",
        "text": "åœ¨çº¿ softmax çš„æ•°å­¦åŸç†å¹¶é FlashAttention é¦–åˆ›â€”â€”Milakov & Gimelshein [60] æ›¾æå‡ºè¿‡ç±»ä¼¼æ€æƒ³ï¼ŒKitaev et al.ï¼ˆReformerï¼‰å’Œ Rabe & Staats [66] ä¹Ÿä½¿ç”¨è¿‡ã€‚FlashAttention çš„åˆ›æ–°åœ¨äº<strong>å°†å®ƒä¸ IO æ„ŸçŸ¥çš„ tiling ç­–ç•¥ç»“åˆ</strong>ï¼Œä¸ä»…èŠ‚çœå†…å­˜ï¼Œæ›´å¤§å¹…å‡å°‘äº† HBM è®¿é—®ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆåœ¨çº¿ softmax éœ€è¦åŒæ—¶ç»´æŠ¤æœ€å¤§å€¼ m å’Œåˆ†æ¯ â„“ ä¸¤ä¸ªç»Ÿè®¡é‡ï¼Ÿ",
        "en": "Therefore if we keep track of some extra statistics (m(x), â„“(x)), we can compute softmax one block at a time.",
        "zh": "å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬é¢å¤–è®°å½•ä¸€äº›ç»Ÿè®¡é‡ (m(x), â„“(x))ï¼Œå°±å¯ä»¥é€å—è®¡ç®— softmaxã€‚",
        "a": "m(x) ä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼ˆå‡å»æœ€å¤§å€¼é˜²æº¢å‡ºï¼‰ï¼Œâ„“(x) æ˜¯å½’ä¸€åŒ–åˆ†æ¯ã€‚ä¸¤è€…ç¼ºä¸€ä¸å¯ï¼šæ²¡æœ‰ m ä¼šæº¢å‡ºï¼Œæ²¡æœ‰ â„“ æ— æ³•å½’ä¸€åŒ–ã€‚åˆå¹¶æ–°å—æ—¶ï¼Œæ—§çš„ m å¯èƒ½ä¸å†æ˜¯å…¨å±€æœ€å¤§å€¼ï¼Œéœ€è¦ç”¨ e^(m_old - m_new) ä¿®æ­£â€”â€”è¿™å°±æ˜¯å¢é‡æ›´æ–°çš„æ ¸å¿ƒã€‚"
      },
      {
        "q": "å¦‚æœæ–°å—çš„æœ€å¤§å€¼æ¯”æ—§å—å°å¾—å¤šï¼Œç¼©æ”¾å› å­ e^(m_old - m_new) ä¼šæ€æ ·ï¼Ÿ",
        "en": "We can decompose the softmax of the concatenated x as: m(x) = max(m(x(1)), m(x(2))).",
        "zh": "æˆ‘ä»¬å¯ä»¥å°†æ‹¼æ¥åçš„ x çš„ softmax åˆ†è§£ä¸ºï¼šm(x) = max(m(x(1)), m(x(2)))ã€‚",
        "a": "m_old - m_new ä¼šæ˜¯0æˆ–è´Ÿæ•°ï¼ˆå› ä¸º m_new = max(m_old, mÌƒ)ï¼‰ã€‚å¦‚æœæ—§å—æœ€å¤§å€¼æ›´å¤§ï¼Œe^(m_old - m_new) = e^0 = 1ï¼Œæ—§ç»“æœä¸å˜ï¼›æ–°å—çš„è´¡çŒ®ä¼šè¢«è¡°å‡ã€‚å¦‚æœæ–°å—æœ€å¤§å€¼æ›´å¤§ï¼Œæ—§ç»“æœè¢«è¡°å‡ï¼Œæ–°å—è´¡çŒ®æ›´å¤§ã€‚è¿™æ­£æ˜¯ softmax çš„æ­£ç¡®è¯­ä¹‰ï¼šæ›´å¤§çš„å€¼åº”è·å¾—æ›´å¤§çš„æƒé‡ã€‚"
      },
      {
        "q": "è¿™ç§å¢é‡ softmax çš„ç²¾åº¦å’Œä¸€æ¬¡æ€§è®¡ç®—å®Œå…¨ä¸€è‡´å—ï¼Ÿ",
        "en": "We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes.",
        "zh": "æˆ‘ä»¬æå‡º FlashAttentionï¼Œä¸€ç§ IO æ„ŸçŸ¥çš„ç²¾ç¡®æ³¨æ„åŠ›ç®—æ³•ï¼Œä½¿ç”¨åˆ†å—æŠ€æœ¯å‡å°‘å†…å­˜è¯»å†™æ¬¡æ•°ã€‚",
        "a": "å®Œå…¨ä¸€è‡´â€”â€”è¿™æ˜¯æ•°å­¦ä¸Šä¸¥æ ¼çš„ç­‰ä»·å˜æ¢ï¼Œä¸æ˜¯è¿‘ä¼¼ã€‚è®ºæ–‡é™„å½• C ç»™å‡ºäº†å®Œæ•´çš„æ­£ç¡®æ€§è¯æ˜ï¼ˆé€šè¿‡æ•°å­¦å½’çº³æ³•ï¼‰ã€‚e^(m_old - m_new) çš„ç¼©æ”¾å› å­ç¡®ä¿æ¯ä¸€æ­¥çš„æ•°å€¼ç»“æœä¸å…¨å±€è®¡ç®—å®Œå…¨ç›¸åŒã€‚è¿™ä¹Ÿæ˜¯ FlashAttention æ ‡é¢˜ä¸­ã€ŒExact Attentionã€çš„å«ä¹‰ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬5é¡µ Section 3.1"
  },
  {
    "id": "algorithm",
    "label": "æ ¸å¿ƒç®—æ³•",
    "emoji": "âš¡",
    "color": "#3b82f6",
    "x": 50,
    "y": 52,
    "section": "ç¬¬3.1èŠ‚ï¼šAlgorithm 1",
    "pages": [
      5,
      6
    ],
    "scrollTo": 5,
    "highlights": [
      {
        "page": 5,
        "top": 50,
        "height": 50
      },
      {
        "page": 6,
        "top": 0,
        "height": 40
      }
    ],
    "keyQuestion": "FlashAttention çš„åŒé‡å¾ªç¯å¦‚ä½•ç»„ç»‡ï¼Ÿä¸ºä»€ä¹ˆå¤–å±‚éå† K/Vã€å†…å±‚éå† Qï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "ç®—æ³•çš„å®è§‚ç»“æ„"
      },
      {
        "type": "p",
        "text": "FlashAttentionï¼ˆAlgorithm 1ï¼‰æ˜¯ä¸€ä¸ª<strong>åŒå±‚åµŒå¥—å¾ªç¯</strong>ï¼šå¤–å±‚éå† Kã€V çš„å—ï¼ˆçº¢è‰²ç®­å¤´ï¼ŒFigure 1ï¼‰ï¼Œå†…å±‚éå† Q çš„å—ï¼ˆè“è‰²ç®­å¤´ï¼‰ã€‚æ¯æ¬¡è¿­ä»£åœ¨ SRAM ä¸­å®Œæˆä¸€ä¸ªå°å—çš„å…¨éƒ¨è®¡ç®—ï¼ˆmatmul â†’ mask â†’ softmax â†’ dropout â†’ matmulï¼‰ï¼Œç„¶åå°†ç»“æœå†™å› HBMã€‚"
      },
      {
        "type": "h3",
        "text": "åˆ†å—ç­–ç•¥"
      },
      {
        "type": "p",
        "text": "å—å¤§å°ç”± SRAM å®¹é‡å†³å®šï¼š<strong>B<sub>c</sub> = âŒˆM/(4d)âŒ‰</strong>ï¼Œ<strong>B<sub>r</sub> = min(âŒˆM/(4d)âŒ‰, d)</strong>ã€‚å…¶ä¸­ M æ˜¯ SRAM å¤§å°ï¼Œd æ˜¯ head dimensionã€‚è¿™ä¿è¯äº†ä¸€ä¸ª Q å—ï¼ˆB<sub>r</sub>Ã—dï¼‰ã€ä¸€ä¸ª K å—ï¼ˆB<sub>c</sub>Ã—dï¼‰ã€ä¸€ä¸ª V å—ï¼ˆB<sub>c</sub>Ã—dï¼‰ä»¥åŠä¸­é—´ç»“æœï¼ˆB<sub>r</sub>Ã—B<sub>c</sub>ï¼‰èƒ½åŒæ—¶è£…å…¥ SRAMã€‚ä»¥ A100 ä¸ºä¾‹ï¼šMâ‰ˆ100KB, d=64 â†’ B<sub>c</sub>=B<sub>r</sub>â‰ˆ256ã€‚"
      },
      {
        "type": "h3",
        "text": "é€æ­¥æ‰§è¡Œ"
      },
      {
        "type": "step",
        "text": "â‘  åˆå§‹åŒ– O=(0)ã€â„“=(0)ã€m=(-âˆ) åœ¨ HBM ä¸­"
      },
      {
        "type": "step",
        "text": "â‘¡ å¤–å±‚å¾ªç¯ï¼šåŠ è½½ç¬¬ j å— K<sub>j</sub>, V<sub>j</sub> åˆ° SRAM"
      },
      {
        "type": "step",
        "text": "â‘¢ å†…å±‚å¾ªç¯ï¼šåŠ è½½ç¬¬ i å— Q<sub>i</sub>, O<sub>i</sub>, â„“<sub>i</sub>, m<sub>i</sub> åˆ° SRAM"
      },
      {
        "type": "step",
        "text": "â‘£ åœ¨ SRAM ä¸Šè®¡ç®— S<sub>ij</sub> = Q<sub>i</sub>K<sub>j</sub><sup>T</sup>ï¼ˆå°å—çš„æ³¨æ„åŠ›åˆ†æ•°ï¼‰"
      },
      {
        "type": "step",
        "text": "â‘¤ åœ¨ SRAM ä¸Šå®Œæˆ mask â†’ åœ¨çº¿ softmax æ›´æ–° â†’ dropout â†’ ç´¯åŠ è¾“å‡º"
      },
      {
        "type": "step",
        "text": "â‘¥ å°†æ›´æ–°åçš„ O<sub>i</sub>, â„“<sub>i</sub>, m<sub>i</sub> å†™å› HBM"
      },
      {
        "type": "h3",
        "text": "ä¸ºä»€ä¹ˆå¤–å±‚éå† K/Vï¼Ÿ"
      },
      {
        "type": "p",
        "text": "è¿™ä¸ªå¾ªç¯é¡ºåºæ˜¯ç²¾å¿ƒé€‰æ‹©çš„ã€‚å¤–å±‚éå† K/V æ„å‘³ç€æ¯ä¸ª K<sub>j</sub>, V<sub>j</sub> å—åªä» HBM åŠ è½½ä¸€æ¬¡ï¼Œç„¶ååœ¨å†…å±‚å¾ªç¯ä¸­è¢«æ‰€æœ‰ Q å—å¤ç”¨ã€‚å¦‚æœåè¿‡æ¥ï¼ˆå¤–å±‚ Qã€å†…å±‚ K/Vï¼‰ï¼Œæ¯ä¸ª Q å—å¤„ç†æ—¶éƒ½è¦é‡æ–°åŠ è½½æ‰€æœ‰ K/V å—ï¼ŒIO é‡æ›´å¤§ã€‚"
      },
      {
        "type": "h3",
        "text": "Kernel Fusionï¼šå…¨éƒ¨èåˆä¸ºä¸€ä¸ª CUDA kernel"
      },
      {
        "type": "p",
        "text": "åˆ†å—ä½¿å¾—æ•´ä¸ªæ³¨æ„åŠ›è®¡ç®—ï¼ˆä¸¤æ¬¡çŸ©é˜µä¹˜æ³• + softmax + masking + dropoutï¼‰è¢«<strong>èåˆä¸ºå•ä¸ª CUDA kernel</strong>ã€‚è¾“å…¥ä» HBM åŠ è½½ä¸€æ¬¡ï¼Œæ‰€æœ‰è®¡ç®—åœ¨ SRAM å®Œæˆï¼Œç»“æœå†™å›ä¸€æ¬¡ã€‚æ ‡å‡†å®ç°éœ€è¦ 5+ ä¸ªç‹¬ç«‹ kernelï¼Œæ¯ä¸ªéƒ½è§¦å‘ HBM è¯»å†™ã€‚è¿™å°±æ˜¯ FlashAttention çš„é€Ÿåº¦ä¹‹æºã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "å—å¤§å° B_c = âŒˆM/(4d)âŒ‰ ä¸­çš„ 4 æ˜¯æ€ä¹ˆæ¥çš„ï¼Ÿ",
        "en": "Set block sizes B_c = âŒˆM/(4d)âŒ‰, B_r = min(âŒˆM/(4d)âŒ‰, d).",
        "zh": "è®¾ç½®å—å¤§å° B_c = âŒˆM/(4d)âŒ‰ï¼ŒB_r = min(âŒˆM/(4d)âŒ‰, d)ã€‚",
        "a": "SRAM éœ€è¦åŒæ—¶å®¹çº³ 4 ä¸ªçŸ©é˜µå—ï¼šQ å— (B_rÃ—d)ã€K å— (B_cÃ—d)ã€V å— (B_cÃ—d)ã€ä»¥åŠä¸­é—´ç»“æœ S å— (B_rÃ—B_c)ã€‚å½“ B_râ‰ˆB_câ‰ˆM/(4d) æ—¶ï¼Œæ€»å†…å­˜çº¦ä¸º 3Â·(M/4)Â·d + (M/4d)Â² â‰ˆ Mã€‚4d ä¸­çš„ 4 å°±æ˜¯è¿™ 4 ä¸ªçŸ©é˜µçš„è´¡çŒ®ã€‚"
      },
      {
        "q": "å¦‚æœ SRAM æ›´å¤§ï¼ˆæ¯”å¦‚ä¸‹ä¸€ä»£ GPUï¼‰ï¼ŒFlashAttention ä¼šæ›´å¿«å—ï¼Ÿ",
        "en": "We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes.",
        "zh": "æˆ‘ä»¬åˆ†æäº† FlashAttention çš„ IO å¤æ‚åº¦ï¼Œè¯æ˜å®ƒæ¯”æ ‡å‡†æ³¨æ„åŠ›éœ€è¦æ›´å°‘çš„ HBM è®¿é—®ï¼Œå¹¶ä¸”åœ¨ä¸€ç³»åˆ— SRAM å¤§å°èŒƒå›´å†…æ˜¯æœ€ä¼˜çš„ã€‚",
        "a": "æ˜¯çš„ã€‚FlashAttention çš„ HBM è®¿é—®é‡ä¸º Î˜(NÂ²dÂ²/M)ï¼ŒM è¶Šå¤§ï¼Œè®¿é—®æ¬¡æ•°è¶Šå°‘ã€‚æ›´å¤§çš„ SRAM å…è®¸æ›´å¤§çš„å—ï¼Œæ¯æ¬¡åœ¨ç‰‡ä¸Šå®Œæˆæ›´å¤šè®¡ç®—ã€‚Figure 2 ä¸­é—´å›¾éªŒè¯äº†è¿™ä¸€ç‚¹ï¼šå—å¤§å°ä» 64 å¢åˆ° 256ï¼Œè¿è¡Œæ—¶é—´æŒç»­ä¸‹é™ã€‚"
      },
      {
        "q": "ä¸ºä»€ä¹ˆå†…å±‚å¾ªç¯å¤„ç† Q è€Œä¸æ˜¯ K/Vï¼Ÿå¦‚æœåè¿‡æ¥ä¼šæ€æ ·ï¼Ÿ",
        "en": "In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows).",
        "zh": "åœ¨å¤–å±‚å¾ªç¯ä¸­ï¼ˆçº¢è‰²ç®­å¤´ï¼‰ï¼ŒFlashAttention éå† K å’Œ V çŸ©é˜µçš„å—å¹¶åŠ è½½åˆ°å¿«é€Ÿç‰‡ä¸Š SRAMã€‚åœ¨æ¯ä¸ªå—ä¸­ï¼ŒFlashAttention éå† Q çŸ©é˜µçš„å—ï¼ˆè“è‰²ç®­å¤´ï¼‰ã€‚",
        "a": "å…³é”®åœ¨äºåœ¨çº¿ softmax çš„æ›´æ–°æ–¹å‘ã€‚softmax æ²¿ K çš„ç»´åº¦ï¼ˆåˆ—æ–¹å‘ï¼‰å½’ä¸€åŒ–ï¼Œæ‰€ä»¥å¤–å±‚éå† K çš„å—æ„å‘³ç€æ¯å¤„ç†ä¸€ä¸ªæ–°çš„ K å—å°±æ›´æ–°ä¸€æ¬¡ softmax ç»Ÿè®¡é‡ã€‚å¦‚æœåè¿‡æ¥ï¼Œæ¯ä¸ª Q å—éœ€è¦ç­‰æ‰€æœ‰ K å—å¤„ç†å®Œæ‰èƒ½å¾—åˆ°æœ€ç»ˆ softmaxâ€”â€”è¦ä¹ˆå­˜ NÃ—N çŸ©é˜µï¼ˆå›åˆ°æ ‡å‡†å®ç°ï¼‰ï¼Œè¦ä¹ˆå¤šæ¬¡éå† K/Vï¼ˆIO æ›´å¤šï¼‰ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬5-6é¡µ Algorithm 1 + Figure 1"
  },
  {
    "id": "io_complexity",
    "label": "IOå¤æ‚åº¦",
    "emoji": "ğŸ“",
    "color": "#10b981",
    "x": 15,
    "y": 52,
    "section": "ç¬¬3.2èŠ‚ï¼šAnalysis",
    "pages": [
      6
    ],
    "scrollTo": 6,
    "highlights": [
      {
        "page": 6,
        "top": 25,
        "height": 55
      }
    ],
    "keyQuestion": "FlashAttention èŠ‚çœäº†å¤šå°‘ HBM è®¿é—®ï¼Ÿå®ƒæ˜¯æœ€ä¼˜çš„å—ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "Theorem 2ï¼šHBM è®¿é—®é‡çš„ç²¾ç¡®åˆ†æ"
      },
      {
        "type": "p",
        "text": "<strong>æ ‡å‡†æ³¨æ„åŠ›</strong>ï¼ˆAlgorithm 0ï¼‰çš„ HBM è®¿é—®é‡ï¼š<strong>Î˜(Nd + NÂ²)</strong>"
      },
      {
        "type": "p",
        "text": "<strong>FlashAttention</strong>ï¼ˆAlgorithm 1ï¼‰çš„ HBM è®¿é—®é‡ï¼š<strong>Î˜(NÂ²dÂ²/M)</strong>"
      },
      {
        "type": "p",
        "text": "å…¶ä¸­ N æ˜¯åºåˆ—é•¿åº¦ï¼Œd æ˜¯ head dimensionï¼ŒM æ˜¯ SRAM å¤§å°ï¼Œä¸” d â‰¤ M â‰¤ Ndã€‚"
      },
      {
        "type": "h3",
        "text": "ç›´è§‰ç†è§£"
      },
      {
        "type": "p",
        "text": "æ ‡å‡†å®ç°çš„ Î˜(Nd + NÂ²) ä¸­ï¼ŒNÂ² é¡¹æ¥è‡ªç‰©åŒ–æ•´ä¸ªæ³¨æ„åŠ›çŸ©é˜µã€‚FlashAttention çš„ Î˜(NÂ²dÂ²/M) å¯ä»¥è¿™æ ·ç†è§£ï¼šå¤–å±‚å¾ªç¯æœ‰ NÂ·d/M è½®ï¼ˆK/V çš„å—æ•°ï¼‰ï¼Œæ¯è½®å†…å±‚éå†æ‰€æœ‰ Q å—ï¼ŒåŠ è½½ Nd ä¸ªå…ƒç´ ã€‚æ€» IO = (Nd/M) Ã— Nd = NÂ²dÂ²/Mã€‚"
      },
      {
        "type": "h3",
        "text": "åŠ é€Ÿæ¯”"
      },
      {
        "type": "p",
        "text": "åŠ é€Ÿæ¯” = (Nd + NÂ²) / (NÂ²dÂ²/M) â‰ˆ M/dÂ²ï¼ˆå½“ N >> d æ—¶ï¼‰ã€‚å¯¹äº A100ï¼šM â‰ˆ 100KB = 100Ã—1024 bytes, d=64, float16 â†’ dÂ² = 64Ã—64Ã—2 = 8KBã€‚åŠ é€Ÿæ¯” â‰ˆ 100KB/8KB â‰ˆ <strong>12.5Ã—</strong>ã€‚è¿™ä¸å®æµ‹çš„ 7.6Ã— åŠ é€Ÿï¼ˆFigure 1 å³å›¾ï¼‰åœ¨åŒä¸€é‡çº§ï¼Œå·®å¼‚æ¥è‡ªå¸¸æ•°å› å­å’Œ kernel å¼€é”€ã€‚"
      },
      {
        "type": "h3",
        "text": "Proposition 3ï¼šæœ€ä¼˜æ€§è¯æ˜"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡è¿›ä¸€æ­¥è¯æ˜ï¼ˆProposition 3ï¼‰ï¼š<strong>ä¸å­˜åœ¨</strong>ä»»ä½•ç²¾ç¡®æ³¨æ„åŠ›ç®—æ³•åœ¨æ‰€æœ‰ SRAM å¤§å° M âˆˆ [d, Nd] ä¸Šæ¸è¿‘ä¼˜äº FlashAttentionã€‚è¯æ˜æ€è·¯æ˜¯åè¯æ³•ï¼šå¦‚æœ HBM è®¿é—®é‡ = o(NÂ²dÂ²/M)ï¼Œåœ¨ M = Î˜(Nd) æ—¶ä¼šå¾—åˆ° o(Nd)â€”â€”ä½†è¾“å…¥è¾“å‡ºæœ¬èº«å°±æœ‰ Nd å¤§å°ï¼Œè‡³å°‘éœ€è¦ Î©(Nd) æ¬¡è®¿é—®ã€‚çŸ›ç›¾ã€‚"
      },
      {
        "type": "h3",
        "text": "å…³é”®æ´å¯Ÿ"
      },
      {
        "type": "p",
        "text": "è¿™æ˜¯ä¸€ä¸ªä¼˜é›…çš„ä¸‹ç•Œè®ºè¯â€”â€”å®ƒè¯´æ˜ FlashAttention å·²ç»è§¦åŠäº†<strong>ä¿¡æ¯è®ºæé™</strong>ã€‚ä»»ä½•ç²¾ç¡®è®¡ç®—æ³¨æ„åŠ›çš„ç®—æ³•éƒ½ä¸å¯èƒ½æ¯”å®ƒæ›´å°‘åœ°è®¿é—® HBMã€‚è¿™æ„å‘³ç€æœªæ¥çš„ä¼˜åŒ–åªèƒ½æ¥è‡ª<strong>è¿‘ä¼¼æ–¹æ³•</strong>ï¼ˆå¦‚ç¨€ç–æ³¨æ„åŠ›ï¼‰æˆ–<strong>ç¡¬ä»¶æ”¹è¿›</strong>ï¼ˆæ›´å¤§çš„ SRAMï¼‰ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆæœ€ä¼˜æ€§è¯æ˜ç”¨çš„æ˜¯åè¯æ³•ï¼Ÿç›´æ¥è¯ä¸‹ç•Œä¸è¡Œå—ï¼Ÿ",
        "en": "However, the input to attention (matrices Q, K, V) and the output O have size Nd and they start out being in HBM, so if the algorithm computes exact attention it must incur at least Î©(Nd) HBM accesses.",
        "zh": "ç„¶è€Œï¼Œæ³¨æ„åŠ›çš„è¾“å…¥ï¼ˆçŸ©é˜µ Q, K, Vï¼‰å’Œè¾“å‡º O çš„å¤§å°ä¸º Ndï¼Œä¸”åˆå§‹å­˜å‚¨åœ¨ HBM ä¸­ï¼Œå› æ­¤ä»»ä½•è®¡ç®—ç²¾ç¡®æ³¨æ„åŠ›çš„ç®—æ³•è‡³å°‘éœ€è¦ Î©(Nd) æ¬¡ HBM è®¿é—®ã€‚",
        "a": "åè¯æ³•åˆ©ç”¨äº†ä¸€ä¸ªç®€æ´çš„è§‚å¯Ÿï¼šå½“ M è¶³å¤Ÿå¤§ï¼ˆM=Î˜(Nd)ï¼‰æ—¶ï¼ŒFlashAttention çš„å¤æ‚åº¦ç®€åŒ–ä¸º Î˜(Nd)â€”â€”åˆšå¥½ç­‰äºè¯»å†™è¾“å…¥è¾“å‡ºçš„æœ€å°é‡ã€‚å¦‚æœæœ‰æ›´å¥½çš„ç®—æ³•ï¼Œåœ¨è¿™ä¸ªç‰¹æ®Šçš„ M å€¼ä¸‹ä¼šå¾—åˆ° o(Nd)ï¼Œè¿åäº†ç‰©ç†ä¸‹ç•Œã€‚"
      },
      {
        "q": "Î˜(NÂ²dÂ²/M) åœ¨ M å¢å¤§æ—¶ä¼šæ€æ ·å˜åŒ–ï¼Ÿæœ‰æ²¡æœ‰ä¸´ç•Œç‚¹ï¼Ÿ",
        "en": "For typical values of d (64-128) and M (around 100KB), dÂ² is many times smaller than M, and thus FlashAttention requires many times fewer HBM accesses than standard implementation.",
        "zh": "å¯¹äº dï¼ˆ64-128ï¼‰å’Œ Mï¼ˆçº¦100KBï¼‰çš„å…¸å‹å€¼ï¼ŒdÂ² è¿œå°äº Mï¼Œå› æ­¤ FlashAttention æ‰€éœ€çš„ HBM è®¿é—®æ¬¡æ•°è¿œå°‘äºæ ‡å‡†å®ç°ã€‚",
        "a": "å½“ M ä» d å¢é•¿åˆ° Nd æ—¶ï¼ŒIO ä» Î˜(NÂ²d) çº¿æ€§ä¸‹é™åˆ° Î˜(Nd)ã€‚ä¸´ç•Œç‚¹åœ¨ M = Ndï¼ˆæ•´ä¸ª K/V è£…å…¥ SRAMï¼‰ï¼Œæ­¤æ—¶ IO = Nd = æ ‡å‡†å®ç°çš„ä¸‹ç•Œã€‚ä½†å®é™…ä¸­ M << Ndï¼ˆ20MB << 64Ã—1024Ã—2=128KBÂ·1024=128MBï¼‰ï¼Œæ‰€ä»¥ FlashAttention çš„åŠ é€Ÿæ˜¯å®è´¨æ€§çš„ã€‚"
      },
      {
        "q": "å¦‚æœ d å¢å¤§åˆ°æ¥è¿‘ Mï¼ŒFlashAttention çš„ä¼˜åŠ¿ä¼šæ¶ˆå¤±å—ï¼Ÿ",
        "en": "Standard attention (Algorithm 0) requires Î˜(Nd + NÂ²) HBM accesses, while FlashAttention (Algorithm 1) requires Î˜(NÂ²dÂ²Mâ»Â¹) HBM accesses.",
        "zh": "æ ‡å‡†æ³¨æ„åŠ›ï¼ˆAlgorithm 0ï¼‰éœ€è¦ Î˜(Nd + NÂ²) æ¬¡ HBM è®¿é—®ï¼Œè€Œ FlashAttentionï¼ˆAlgorithm 1ï¼‰éœ€è¦ Î˜(NÂ²dÂ²Mâ»Â¹) æ¬¡ HBM è®¿é—®ã€‚",
        "a": "æ˜¯çš„ã€‚åŠ é€Ÿæ¯” â‰ˆ M/dÂ²ï¼Œå½“ dÂ² æ¥è¿‘ M æ—¶åŠ é€Ÿæ¯”è¶‹è¿‘1ã€‚ä½†å®é™…ä¸­ d é€šå¸¸æ˜¯ 64 æˆ– 128ï¼ˆmulti-head attention åˆ‡åˆ†åï¼‰ï¼Œè€Œ M æ˜¯ 100KB çº§åˆ«ï¼ŒdÂ² = 8-32KBï¼Œæ‰€ä»¥ M/dÂ² åœ¨ 3-12Ã— èŒƒå›´å†…ï¼Œä¼˜åŠ¿æ˜¾è‘—ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬6é¡µ Theorem 2 + Proposition 3"
  },
  {
    "id": "recomputation",
    "label": "åå‘é‡è®¡ç®—",
    "emoji": "ğŸ”„",
    "color": "#06b6d4",
    "x": 85,
    "y": 52,
    "section": "ç¬¬3.1èŠ‚ï¼šRecomputation",
    "pages": [
      5
    ],
    "scrollTo": 5,
    "highlights": [
      {
        "page": 5,
        "top": 40,
        "height": 35
      }
    ],
    "keyQuestion": "è®­ç»ƒæ—¶åå‘ä¼ æ’­éœ€è¦ S å’Œ P çŸ©é˜µâ€”â€”ä¸å­˜å‚¨å®ƒä»¬æ€ä¹ˆåŠï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "é—®é¢˜ï¼šåå‘ä¼ æ’­çš„å†…å­˜éœ€æ±‚"
      },
      {
        "type": "p",
        "text": "è®­ç»ƒæ—¶çš„åå‘ä¼ æ’­éœ€è¦ S = QK<sup>T</sup> å’Œ P = softmax(S) æ¥è®¡ç®— Qã€Kã€V çš„æ¢¯åº¦ã€‚æ ‡å‡†å®ç°åœ¨å‰å‘ä¼ æ’­ä¸­ä¿å­˜è¿™ä¸¤ä¸ª NÃ—N çŸ©é˜µï¼Œå†…å­˜å ç”¨ä¸º O(NÂ²)ã€‚å¯¹äºé•¿åºåˆ—ï¼ˆN=16Kï¼‰ï¼Œä»…ä¸€ä¸ªæ³¨æ„åŠ›å±‚çš„ä¸­é—´å˜é‡å°±éœ€è¦ 16KÃ—16KÃ—4 bytes = 1GBã€‚"
      },
      {
        "type": "h3",
        "text": "FlashAttention çš„è§£å†³æ–¹æ¡ˆï¼šä¸å­˜ï¼Œè€Œæ˜¯é‡ç®—"
      },
      {
        "type": "p",
        "text": "FlashAttention çš„å‰å‘ä¼ æ’­åªä¿å­˜è¾“å‡º O å’Œä¸¤ä¸ªè¾…åŠ©å‘é‡ mï¼ˆè¡Œæœ€å¤§å€¼ï¼‰å’Œ â„“ï¼ˆsoftmax åˆ†æ¯ï¼‰ï¼Œæ€»å†…å­˜ O(N)ã€‚åœ¨åå‘ä¼ æ’­æ—¶ï¼Œä» HBM åŠ è½½ Qã€Kã€V çš„å—åˆ° SRAMï¼Œ<strong>é‡æ–°è®¡ç®—</strong> S å’Œ P çš„å¯¹åº”å—ï¼Œç„¶åç”¨å®ƒä»¬è®¡ç®—æ¢¯åº¦ã€‚"
      },
      {
        "type": "h3",
        "text": "è¿™ä¸å°±æ˜¯ Gradient Checkpointing å—ï¼Ÿ"
      },
      {
        "type": "p",
        "text": "è¡¨é¢ä¸Šçœ‹ï¼Œè¿™æ˜¯<strong>é€‰æ‹©æ€§æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆselective gradient checkpointingï¼‰</strong>[10, 34] çš„ä¸€ç§å½¢å¼â€”â€”ç”¨é¢å¤–çš„è®¡ç®—æ¢å–æ›´å°‘çš„å†…å­˜ã€‚ä½†ä¼ ç»Ÿçš„ gradient checkpointing æ€»æ˜¯ã€Œç”¨æ—¶é—´æ¢ç©ºé—´ã€ã€‚FlashAttention çš„åå‘é‡è®¡ç®—å´<strong>åŒæ—¶åŠ é€Ÿäº†åå‘ä¼ æ’­</strong>ï¼"
      },
      {
        "type": "h3",
        "text": "ä¸ºä»€ä¹ˆé‡ç®—åè€Œæ›´å¿«ï¼Ÿ"
      },
      {
        "type": "p",
        "text": "å› ä¸ºæ ‡å‡†åå‘ä¼ æ’­éœ€è¦ä» HBM è¯»å– NÃ—N çš„ S å’Œ P çŸ©é˜µï¼ŒIO é‡ä¸º O(NÂ²)ã€‚FlashAttention çš„é‡ç®—åœ¨ SRAM ä¸­å®Œæˆï¼Œé¿å…äº†è¿™äº› HBM è¯»å–ã€‚<strong>å¤šå‡ºçš„ FLOPsï¼ˆé‡ç®— S å’Œ Pï¼‰è¢« SRAM çš„é«˜å¸¦å®½è½»æ¾å¸æ”¶ï¼Œè€Œçœä¸‹çš„ HBM IO æ‰æ˜¯çœŸæ­£çš„æ—¶é—´èŠ‚çœ</strong>ã€‚åå‘ä¼ æ’­åŒæ ·åªéœ€ Î˜(NÂ²dÂ²/M) æ¬¡ HBM è®¿é—®ã€‚"
      },
      {
        "type": "h3",
        "text": "ä¿å­˜ä»€ä¹ˆã€ä¸¢å¼ƒä»€ä¹ˆ"
      },
      {
        "type": "p",
        "text": "å‰å‘ä¼ æ’­éœ€è¦ä¿å­˜åˆ° HBM çš„ä¿¡æ¯ï¼š<strong>O âˆˆ â„<sup>NÃ—d</sup></strong>ï¼ˆè¾“å‡ºï¼‰ã€<strong>m âˆˆ â„<sup>N</sup></strong>ï¼ˆè¡Œæœ€å¤§å€¼ï¼‰ã€<strong>â„“ âˆˆ â„<sup>N</sup></strong>ï¼ˆsoftmax åˆ†æ¯ï¼‰ã€‚æ€»å†…å­˜ O(Nd)ã€‚ä¸¢å¼ƒçš„ä¿¡æ¯ï¼šS âˆˆ â„<sup>NÃ—N</sup>ã€P âˆˆ â„<sup>NÃ—N</sup>ã€‚å†…å­˜ä» O(Nd + NÂ²) é™åˆ° O(Nd)â€”â€”çº¿æ€§ vs äºŒæ¬¡ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "é‡è®¡ç®—å¼•å…¥äº†å¤šå°‘é¢å¤–çš„ FLOPsï¼Ÿè¿™ä¸ªä»£ä»·å¤§å—ï¼Ÿ",
        "en": "While gradient checkpointing has been suggested to reduce the maximum amount of memory required, all implementations (that we know of) have to trade speed for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to reduced HBM accesses.",
        "zh": "è™½ç„¶æ¢¯åº¦æ£€æŸ¥ç‚¹è¢«å»ºè®®ç”¨äºå‡å°‘æ‰€éœ€çš„æœ€å¤§å†…å­˜é‡ï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ‰€æœ‰å®ç°éƒ½éœ€è¦ä»¥é€Ÿåº¦æ¢å†…å­˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå³ä½¿å¢åŠ äº† FLOPsï¼Œæˆ‘ä»¬çš„é‡è®¡ç®—ä»ç”±äºå‡å°‘äº† HBM è®¿é—®è€ŒåŠ é€Ÿäº†åå‘ä¼ æ’­ã€‚",
        "a": "é‡ç®— S=QKáµ€ å’Œ P=softmax(S) çš„ FLOPs ä¸å‰å‘ä¼ æ’­ç›¸åŒï¼Œå³åå‘ä¼ æ’­çš„æ€» FLOPs å¤§çº¦ç¿»å€ã€‚ä½† IO ä» O(Nd + NÂ²) é™åˆ° O(NÂ²dÂ²/M)â€”â€”å¯¹äº N=1024, d=64, M=100KBï¼Œè¿™æ˜¯ ~10Ã— çš„ IO å‡å°‘ã€‚ç¿»å€çš„è®¡ç®— vs 10Ã— çš„ IO èŠ‚çœï¼Œå‡€æ•ˆæœæ˜¯å¤§å¹…åŠ é€Ÿã€‚"
      },
      {
        "q": "å¦‚æœ GPU çš„ HBM å®¹é‡æ— é™å¤§ï¼Œé‡è®¡ç®—è¿˜æœ‰æ„ä¹‰å—ï¼Ÿ",
        "en": "One of our goals is to not store O(NÂ²) intermediate values for the backward pass.",
        "zh": "æˆ‘ä»¬çš„ç›®æ ‡ä¹‹ä¸€æ˜¯ä¸åœ¨åå‘ä¼ æ’­ä¸­å­˜å‚¨ O(NÂ²) çš„ä¸­é—´å€¼ã€‚",
        "a": "å³ä½¿ HBM æ— é™å¤§ï¼Œé‡è®¡ç®—ä»ç„¶æœ‰æ„ä¹‰ã€‚å› ä¸ºç“¶é¢ˆä¸æ˜¯å†…å­˜å®¹é‡è€Œæ˜¯å†…å­˜å¸¦å®½â€”â€”å³ä½¿æ•°æ®éƒ½åœ¨ HBM ä¸­ï¼Œè¯»å– NÂ² ä¸ªå…ƒç´ çš„æ—¶é—´ä¹Ÿæ¯”åœ¨ SRAM ä¸­é‡ç®—å®ƒä»¬æ›´é•¿ã€‚é‡è®¡ç®—çš„æ ¸å¿ƒä»·å€¼æ˜¯å‡å°‘ HBM è¯»å†™æ¬¡æ•°ï¼Œä¸ä»…ä»…æ˜¯çœå†…å­˜ã€‚"
      },
      {
        "q": "Dropout çš„éšæœºæ€§å¦‚ä½•åœ¨é‡è®¡ç®—ä¸­ä¿æŒä¸€è‡´ï¼Ÿ",
        "en": "Initialize the pseudo-random number generator state R and save to HBM.",
        "zh": "åˆå§‹åŒ–ä¼ªéšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€ R å¹¶ä¿å­˜åˆ° HBMã€‚",
        "a": "FlashAttention åœ¨å‰å‘ä¼ æ’­å¼€å§‹æ—¶ä¿å­˜éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€ Rã€‚åå‘ä¼ æ’­æ—¶æ¢å¤åˆ°åŒä¸€ä¸ªçŠ¶æ€ Rï¼Œè¿™æ ·é‡æ–°ç”Ÿæˆçš„ dropout mask ä¸å‰å‘ä¼ æ’­å®Œå…¨ä¸€è‡´ã€‚è¿™æ˜¯ä¸€ä¸ªç²¾å·§çš„å®ç°ç»†èŠ‚â€”â€”åªéœ€ä¿å­˜ä¸€ä¸ªç§å­ï¼ˆå‡ ä¸ªå­—èŠ‚ï¼‰ï¼Œå°±èƒ½é‡ç°æ•´ä¸ª NÃ—N çš„ dropout maskã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬5é¡µ Recomputation + é™„å½• B"
  },
  {
    "id": "block_sparse",
    "label": "å—ç¨€ç–æ‰©å±•",
    "emoji": "ğŸ§©",
    "color": "#ec4899",
    "x": 30,
    "y": 74,
    "section": "ç¬¬3.3èŠ‚ï¼šBlock-Sparse FlashAttention",
    "pages": [
      7
    ],
    "scrollTo": 7,
    "highlights": [
      {
        "page": 6,
        "top": 75,
        "height": 25
      },
      {
        "page": 7,
        "top": 0,
        "height": 45
      }
    ],
    "keyQuestion": "å¦‚ä½•åœ¨ FlashAttention ä¹‹ä¸Šè¿›ä¸€æ­¥åŠ é€Ÿè¶…é•¿åºåˆ—ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "åŠ¨æœºï¼šå½“ N æå¤§æ—¶"
      },
      {
        "type": "p",
        "text": "FlashAttention çš„ IO å¤æ‚åº¦ Î˜(NÂ²dÂ²/M) ä»ç„¶æ˜¯ N çš„äºŒæ¬¡æ–¹ã€‚å½“ N å¢é•¿åˆ° 16Kã€64K ç”šè‡³æ›´é•¿æ—¶ï¼Œå³ä½¿æœ‰ IO ä¼˜åŒ–ï¼ŒäºŒæ¬¡å¢é•¿ä»ç„¶æˆä¸ºç“¶é¢ˆã€‚Block-Sparse FlashAttention é€šè¿‡<strong>åªè®¡ç®—æ³¨æ„åŠ›çŸ©é˜µä¸­éé›¶çš„å—</strong>æ¥è¿›ä¸€æ­¥é™ä½å¤æ‚åº¦ã€‚"
      },
      {
        "type": "h3",
        "text": "å®ç°åŸç†"
      },
      {
        "type": "p",
        "text": "ç»™å®šä¸€ä¸ªå—ç¨€ç–æ©ç  M âˆˆ {0,1}<sup>(N/B<sub>r</sub>)Ã—(N/B<sub>c</sub>)</sup>ï¼Œæ ‡è®°å“ªäº›å—éœ€è¦è®¡ç®—ã€‚Algorithm 5 ä¸ Algorithm 1 å‡ ä¹ç›¸åŒï¼Œå”¯ä¸€åŒºåˆ«æ˜¯åœ¨å†…å±‚å¾ªç¯ä¸­<strong>è·³è¿‡ M<sub>ij</sub>=0 çš„å—</strong>ã€‚è¿™ç§ä¿®æ”¹éå¸¸è‡ªç„¶ï¼Œå› ä¸º FlashAttention æœ¬èº«å°±æ˜¯é€å—è®¡ç®—çš„ã€‚"
      },
      {
        "type": "h3",
        "text": "IO å¤æ‚åº¦"
      },
      {
        "type": "p",
        "text": "<strong>Proposition 4</strong>ï¼šBlock-Sparse FlashAttention çš„ HBM è®¿é—®é‡ä¸º Î˜(Nd + NÂ²dÂ²s/M)ï¼Œå…¶ä¸­ s æ˜¯éé›¶å—çš„æ¯”ä¾‹ã€‚ç¨€ç–åº¦ç›´æ¥ä½œç”¨äºäºŒæ¬¡é¡¹ï¼Œå¸¦æ¥çº¿æ€§æ¯”ä¾‹çš„åŠ é€Ÿã€‚"
      },
      {
        "type": "h3",
        "text": "ç¨€ç–æ¨¡å¼çš„é€‰æ‹©"
      },
      {
        "type": "p",
        "text": "è®ºæ–‡ä½¿ç”¨äº† <strong>butterfly sparsity pattern</strong>ï¼ˆè¶å½¢ç¨€ç–æ¨¡å¼ï¼‰[17]ï¼Œè¿™æ˜¯ä½œè€…ä¹‹å‰çš„å·¥ä½œã€‚å¯¹äºé•¿åºåˆ—ï¼Œs é€šå¸¸è®¾ä¸º N<sup>-1/2</sup> æˆ– N<sup>-1</sup>log Nï¼Œä½¿ IO å¤æ‚åº¦é™ä¸º Î˜(NâˆšN) æˆ– Î˜(N log N)â€”â€”ä»äºŒæ¬¡é™åˆ°äº†äºšäºŒæ¬¡ã€‚"
      },
      {
        "type": "h3",
        "text": "å®éªŒéªŒè¯"
      },
      {
        "type": "p",
        "text": "Figure 2ï¼ˆå³å›¾ï¼‰éªŒè¯äº†ç¨€ç–åº¦ä¸è¿è¡Œæ—¶é—´çš„çº¿æ€§å…³ç³»ã€‚åœ¨ LRA benchmark ä¸Šï¼ŒBlock-Sparse FlashAttention å®ç°äº† 2.8Ã— åŠ é€Ÿï¼ˆvs æ ‡å‡†æ³¨æ„åŠ›ï¼‰ï¼Œæ€§èƒ½ä¸å…¨æ³¨æ„åŠ›æŒå¹³ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä½¿ Transformer <strong>é¦–æ¬¡</strong>åœ¨ Path-256 ä»»åŠ¡ä¸Šï¼ˆåºåˆ—é•¿åº¦ 64Kï¼‰è¾¾åˆ°äº†ä¼˜äºéšæœºçš„è¡¨ç°ï¼ˆ63.1%ï¼‰â€”â€”æ­¤å‰æ²¡æœ‰ä»»ä½•æ¨¡å‹åšåˆ°ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "å—ç¨€ç–æ¨¡å¼æ˜¯å¦‚ä½•é€‰æ‹©çš„ï¼Ÿä¸ºä»€ä¹ˆç”¨ butterfly patternï¼Ÿ",
        "en": "For downstream experiments, we use the fixed butterfly sparsity pattern, which has been shown to be able to approximate arbitrary sparsity.",
        "zh": "åœ¨ä¸‹æ¸¸å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å›ºå®šçš„è¶å½¢ç¨€ç–æ¨¡å¼ï¼Œè¯¥æ¨¡å¼å·²è¢«è¯æ˜èƒ½å¤Ÿè¿‘ä¼¼ä»»æ„ç¨€ç–æ€§ã€‚",
        "a": "Butterfly pattern æ¥è‡ªå¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰çš„è¶å½¢ç»“æ„ï¼Œå…¼å…·è§„åˆ™æ€§ï¼ˆä¾¿äº GPU å®ç°ï¼‰å’Œè¡¨è¾¾åŠ›ï¼ˆå¯è¿‘ä¼¼ä»»æ„ç¨€ç–æ¨¡å¼ï¼‰ã€‚å®ƒçš„è¿æ¥æ¨¡å¼ç¡®ä¿ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„è·¯å¾„é•¿åº¦ä¸º O(log N)ï¼Œä¿æŒäº†é•¿è·ç¦»ä¾èµ–çš„å­¦ä¹ èƒ½åŠ›ã€‚"
      },
      {
        "q": "Block-Sparse æ˜¯ç²¾ç¡®çš„è¿˜æ˜¯è¿‘ä¼¼çš„ï¼Ÿ",
        "en": "We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method.",
        "zh": "æˆ‘ä»¬è¿˜å°† FlashAttention æ‰©å±•åˆ°å—ç¨€ç–æ³¨æ„åŠ›ï¼Œäº§ç”Ÿäº†ä¸€ç§æ¯”æ‰€æœ‰ç°æœ‰è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•éƒ½æ›´å¿«çš„è¿‘ä¼¼æ³¨æ„åŠ›ç®—æ³•ã€‚",
        "a": "Block-Sparse FlashAttention æ˜¯è¿‘ä¼¼çš„â€”â€”è¢«æ©ç è®¾ä¸º0çš„å—ä¸­çš„æ³¨æ„åŠ›æƒé‡è¢«ä¸¢å¼ƒäº†ã€‚ä½† FlashAttention æœ¬èº«æ˜¯ç²¾ç¡®çš„ã€‚ä¸¤è€…å¯ä»¥æŒ‰éœ€é€‰æ‹©ï¼šçŸ­ä¸­åºåˆ—ç”¨ç²¾ç¡®ç‰ˆï¼Œè¶…é•¿åºåˆ—ç”¨ç¨€ç–ç‰ˆã€‚ç¨€ç–ç‰ˆçš„ç²¾åº¦æŸå¤±å–å†³äºæ©ç é€‰æ‹©çš„è´¨é‡ã€‚"
      },
      {
        "q": "s = N^(-1/2) æ—¶ï¼ŒIO å¤æ‚åº¦å˜æˆä»€ä¹ˆï¼Ÿ",
        "en": "For large sequence lengths N, s is often set to N^(-1/2) or N^(-1) log N, resulting in Î˜(NâˆšN) or Î˜(N log N) IO complexity.",
        "zh": "å¯¹äºè¾ƒå¤§çš„åºåˆ—é•¿åº¦ Nï¼Œs é€šå¸¸è®¾ä¸º N^(-1/2) æˆ– N^(-1) log Nï¼Œä½¿å¾— IO å¤æ‚åº¦åˆ†åˆ«ä¸º Î˜(NâˆšN) æˆ– Î˜(N log N)ã€‚",
        "a": "s = N^(-1/2) æ—¶ï¼ŒIO = Î˜(NÂ²dÂ²Â·N^(-1/2)/M) = Î˜(N^(3/2)Â·dÂ²/M)ã€‚å¯¹äº N=64Kï¼šæ ‡å‡†æ³¨æ„åŠ› âˆ NÂ² = 4Ã—10â¹ï¼ŒFlashAttention âˆ NÂ²/12 = 3.4Ã—10â¸ï¼ŒBlock-Sparse âˆ N^(3/2)/12 = 2.1Ã—10â¶ã€‚ä»åäº¿çº§é™åˆ°ç™¾ä¸‡çº§â€”â€”ä¸‰ä¸ªæ•°é‡çº§çš„å·®è·ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬6-7é¡µ Section 3.3 + Proposition 4"
  },
  {
    "id": "experiments",
    "label": "å®éªŒç»“æœ",
    "emoji": "ğŸ“Š",
    "color": "#6366f1",
    "x": 70,
    "y": 74,
    "section": "ç¬¬4èŠ‚ï¼šExperiments",
    "pages": [
      7,
      8,
      9
    ],
    "scrollTo": 8,
    "highlights": [
      {
        "page": 7,
        "top": 40,
        "height": 60
      },
      {
        "page": 8,
        "top": 0,
        "height": 100
      },
      {
        "page": 9,
        "top": 0,
        "height": 100
      }
    ],
    "keyQuestion": "FlashAttention åœ¨çœŸå®ä»»åŠ¡ä¸­çš„åŠ é€Ÿæ•ˆæœå’Œè´¨é‡æå‡å¦‚ä½•ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "è®­ç»ƒé€Ÿåº¦ï¼ˆTable 2ï¼‰"
      },
      {
        "type": "p",
        "text": "åœ¨ 8Ã—A100 GPU ä¸Šçš„ GPT-2 è®­ç»ƒå¯¹æ¯”ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>GPT-2 small</strong>ï¼šHuggingFace 9.5å¤© â†’ Megatron 4.7å¤©ï¼ˆ2.0Ã—ï¼‰â†’ FlashAttention <strong>2.7å¤©ï¼ˆ3.5Ã—ï¼‰</strong>"
      },
      {
        "type": "bullet",
        "text": "<strong>GPT-2 medium</strong>ï¼šHuggingFace 21.0å¤© â†’ Megatron 11.5å¤©ï¼ˆ1.8Ã—ï¼‰â†’ FlashAttention <strong>6.9å¤©ï¼ˆ3.0Ã—ï¼‰</strong>"
      },
      {
        "type": "p",
        "text": "ä¸‰ç§å®ç°è¾¾åˆ°äº†ç›¸åŒçš„å›°æƒ‘åº¦ï¼ˆPPL 18.2/14.3ï¼‰ï¼Œç¡®è®¤ FlashAttention æ˜¯ç²¾ç¡®çš„â€”â€”ä¸æ”¹å˜æ¨¡å‹å®šä¹‰ï¼Œåªæ”¹å˜è®¡ç®—æ–¹å¼ã€‚"
      },
      {
        "type": "h3",
        "text": "BERT è®­ç»ƒï¼ˆè¶…è¶Š MLPerf è®°å½•ï¼‰"
      },
      {
        "type": "p",
        "text": "BERT-largeï¼ˆåºåˆ—é•¿åº¦ 512ï¼‰è®­ç»ƒæ¯” MLPerf 1.1 çš„é€Ÿåº¦è®°å½•å¿« <strong>15%</strong>ã€‚è¿™æ˜¯åœ¨ä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„å·¥ä¸šçº§åŸºå‡†ä¸Šå–å¾—çš„ï¼Œè¯´æ˜ FlashAttention çš„åŠ é€Ÿä¸æ˜¯ä¸ã€Œæœªä¼˜åŒ–åŸºçº¿ã€æ¯”è¾ƒã€‚"
      },
      {
        "type": "h3",
        "text": "é•¿åºåˆ—è´¨é‡æå‡"
      },
      {
        "type": "p",
        "text": "FlashAttention ä½¿ Transformer èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œå¸¦æ¥äº†æ˜¾è‘—çš„è´¨é‡æå‡ï¼š"
      },
      {
        "type": "bullet",
        "text": "<strong>GPT-2 ä¸Šä¸‹æ–‡ 4K</strong>ï¼šå›°æƒ‘åº¦æ¯” 1K ä¸Šä¸‹æ–‡æå‡ <strong>0.7</strong>ï¼ˆä¸”è®­ç»ƒé€Ÿåº¦ä»å¿«äº Megatron è®­ç»ƒ 1K ä¸Šä¸‹æ–‡çš„ GPT-2ï¼‰"
      },
      {
        "type": "bullet",
        "text": "<strong>é•¿æ–‡æ¡£åˆ†ç±»</strong>ï¼ˆTable 5ï¼‰ï¼šMIMIC-III ä» 52.8ï¼ˆ512é•¿åº¦ï¼‰æå‡åˆ° 57.1ï¼ˆ16Kï¼‰ï¼ŒECtHR ä» 72.2 æå‡åˆ° 80.7ï¼ˆ8Kï¼‰"
      },
      {
        "type": "h3",
        "text": "çªç ´æ€§æˆæœï¼šPath-X å’Œ Path-256"
      },
      {
        "type": "p",
        "text": "Path-Xï¼ˆåºåˆ—é•¿åº¦ 16Kï¼‰å’Œ Path-256ï¼ˆåºåˆ—é•¿åº¦ 64Kï¼‰æ˜¯ Long-Range Arena ä¸­æœ€å›°éš¾çš„ä»»åŠ¡â€”â€”æ­¤å‰<strong>æ²¡æœ‰ä»»ä½• Transformer æ¨¡å‹</strong>ï¼ˆåŒ…æ‹¬æ‰€æœ‰è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•ï¼‰èƒ½è¾¾åˆ°ä¼˜äºéšæœºçš„è¡¨ç°ã€‚FlashAttention é¦–æ¬¡åœ¨ Path-X ä¸Šè¾¾åˆ° <strong>61.4%</strong>ï¼ŒBlock-Sparse åœ¨ Path-256 ä¸Šè¾¾åˆ° <strong>63.1%</strong>ã€‚è¿™çº¯ç²¹æ˜¯å› ä¸ºèƒ½å¤„ç†æ›´é•¿åºåˆ—ï¼Œè€Œéæ¨¡å‹æ¶æ„æ”¹å˜ã€‚"
      },
      {
        "type": "h3",
        "text": "è¿è¡Œæ—¶ä¸å†…å­˜åŸºå‡†ï¼ˆFigure 3ï¼‰"
      },
      {
        "type": "p",
        "text": "FlashAttention åœ¨åºåˆ—é•¿åº¦ 128-2K ä¸Šæ¯”æ ‡å‡†å®ç°å¿« <strong>æœ€é«˜ 3Ã—</strong>ï¼Œå†…å­˜æ•ˆç‡æå‡ <strong>æœ€é«˜ 20Ã—</strong>ã€‚åœ¨ 64K é•¿åº¦ä¸‹ï¼Œé™¤ Linformer å¤–æ‰€æœ‰æ–¹æ³•éƒ½åœ¨ A100 ä¸Š OOMï¼Œè€Œ FlashAttention ä»ç„¶æ­£å¸¸è¿è¡Œä¸”æ¯” Linformer é«˜æ•ˆ 2Ã—ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆ FlashAttention åœ¨çŸ­åºåˆ—ï¼ˆ128-512ï¼‰ä¸Šä¹Ÿæ¯”è¿‘ä¼¼æ–¹æ³•å¿«ï¼Ÿ",
        "en": "FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses.",
        "zh": "ç”±äºæ›´å°‘çš„å†…å­˜è®¿é—®ï¼ŒFlashAttention åœ¨çŸ­åºåˆ—ä¸Šä»ç„¶æ¯”è¿‘ä¼¼å’Œç¨€ç–æ³¨æ„åŠ›è¿è¡Œæ›´å¿«ã€‚",
        "a": "è¿‘ä¼¼æ–¹æ³•è™½ç„¶ FLOPs æ˜¯ O(N)ï¼Œä½†å®ƒä»¬å¼•å…¥äº†é¢å¤–çš„æ“ä½œï¼ˆå¦‚éšæœºæŠ•å½±ã€æ’åºç­‰ï¼‰ï¼Œæ¯ä¸ªæ“ä½œéƒ½æ˜¯ç‹¬ç«‹ kernelï¼Œéœ€è¦é¢å¤–çš„ HBM è¯»å†™ã€‚åœ¨çŸ­åºåˆ—ä¸Š NÂ² æœ¬èº«ä¸å¤§ï¼ŒFlashAttention çš„ kernel fusion å¸¦æ¥çš„ IO èŠ‚çœåè€Œè¶…è¿‡äº†è¿‘ä¼¼æ–¹æ³•çš„ FLOPs ä¼˜åŠ¿ã€‚"
      },
      {
        "q": "Path-X å’Œ Path-256 åˆ°åº•åœ¨æµ‹ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆä¹‹å‰æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼Ÿ",
        "en": "FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X challenge, solely from using a longer sequence length (16K).",
        "zh": "FlashAttention ä½¿å¾— Transformer é¦–æ¬¡åœ¨ Path-X æŒ‘æˆ˜ä¸Šè¾¾åˆ°ä¼˜äºéšæœºçš„è¡¨ç°ï¼Œä»…ä»…é€šè¿‡ä½¿ç”¨æ›´é•¿çš„åºåˆ—é•¿åº¦ï¼ˆ16Kï¼‰å®ç°ã€‚",
        "a": "Path-X è¦æ±‚æ¨¡å‹åœ¨ä¸€å¼  128Ã—128 çš„å›¾åƒä¸­åˆ¤æ–­ä¸¤ä¸ªæ ‡è®°ç‚¹ä¹‹é—´æ˜¯å¦æœ‰è·¯å¾„è¿æ¥ã€‚å°†å›¾åƒå±•å¹³ä¸ºåºåˆ—åé•¿åº¦ä¸º 16384ã€‚å¦‚æœæ¨¡å‹æ— æ³•æœ‰æ•ˆå¤„ç†è¿™ä¹ˆé•¿çš„åºåˆ—ï¼Œå°±æ— æ³•æ•æ‰åˆ°è¿æ¥è·¯å¾„çš„ä¿¡æ¯ã€‚ä¹‹å‰çš„æ¨¡å‹è¦ä¹ˆ OOMï¼Œè¦ä¹ˆæ³¨æ„åŠ›è¿‘ä¼¼ä¸¢å¤±äº†å…³é”®çš„é•¿è·ç¦»ä¿¡æ¯ã€‚FlashAttention ä¸æ”¹å˜æ¨¡å‹ï¼Œåªæ˜¯è®©å®ƒçœŸæ­£èƒ½ã€Œçœ‹åˆ°ã€16K ä¸ª tokenã€‚"
      },
      {
        "q": "Table 5 ä¸­ä¸ºä»€ä¹ˆ MIMIC-III åœ¨ 1K é•¿åº¦åè€Œæ¯” 512 å·®ï¼Ÿ",
        "en": "The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length.",
        "zh": "è¿™äº›å·®å¼‚å¯èƒ½æ˜¯ç”±äºå¾®å¦™çš„åˆ†å¸ƒåç§»ï¼šMIMIC-III åŒ…å«ä¸“ä¸šåŒ»å­¦æ–‡æœ¬ï¼Œå› æ­¤å¯èƒ½æ›´å®¹æ˜“å—åˆ°æ–‡æ¡£é•¿åº¦åˆ†å¸ƒåç§»çš„å½±å“ã€‚",
        "a": "RoBERTa é¢„è®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦æ˜¯ 512ã€‚å½“é‡å¤ä½ç½®åµŒå…¥ä»¥æ”¯æŒæ›´é•¿åºåˆ—æ—¶ï¼Œæ¨¡å‹éœ€è¦é€‚åº”æ–°çš„é•¿åº¦åˆ†å¸ƒã€‚MIMIC-III æ˜¯åŒ»å­¦æ–‡æœ¬ï¼ˆåˆ†å¸ƒä¸é¢„è®­ç»ƒè¯­æ–™å·®å¼‚å¤§ï¼‰ï¼Œåœ¨ 1K é•¿åº¦æ—¶æ¨¡å‹è¿˜æ²¡é€‚åº”è¿‡æ¥ã€‚ä½†ç»§ç»­å¢é•¿åˆ° 4K-16K åï¼Œé•¿ä¸Šä¸‹æ–‡çš„ä¼˜åŠ¿æœ€ç»ˆè¶…è¿‡äº†åˆ†å¸ƒåç§»çš„åŠ£åŠ¿ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬7-10é¡µ Section 4"
  },
  {
    "id": "implications",
    "label": "å½±å“ä¸å±€é™",
    "emoji": "ğŸ”®",
    "color": "#f97316",
    "x": 50,
    "y": 95,
    "section": "ç¬¬5èŠ‚ï¼šLimitations and Future Directions",
    "pages": [
      10
    ],
    "scrollTo": 10,
    "highlights": [
      {
        "page": 10,
        "top": 0,
        "height": 75
      }
    ],
    "keyQuestion": "FlashAttention çš„å±€é™åœ¨å“ªé‡Œï¼Ÿå®ƒå¯¹åç»­ç ”ç©¶äº§ç”Ÿäº†ä»€ä¹ˆå½±å“ï¼Ÿ",
    "content": [
      {
        "type": "h3",
        "text": "å±€é™ä¸€ï¼šéœ€è¦æ‰‹å†™ CUDA kernel"
      },
      {
        "type": "p",
        "text": "FlashAttention çš„å®ç°éœ€è¦ç”¨ CUDA ç¼–å†™é«˜åº¦ä¼˜åŒ–çš„ kernelâ€”â€”æ¯” PyTorch ä½å¾—å¤šçš„æŠ½è±¡å±‚çº§ï¼Œéœ€è¦å¤§é‡å·¥ç¨‹åŠªåŠ›ã€‚æ¯ç§æ–°çš„æ³¨æ„åŠ›å˜ä½“ï¼ˆå¦‚å¸¦æ—‹è½¬ä½ç½®ç¼–ç ã€å¸¦ ALiBi çš„æ³¨æ„åŠ›ï¼‰éƒ½éœ€è¦ç¼–å†™æ–°çš„ kernelã€‚è®ºæ–‡æŒ‡å‡º<strong>ç¼–è¯‘å™¨è¾…åŠ©</strong>æ˜¯æœªæ¥æ–¹å‘ï¼šå¦‚æœæ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ï¼ˆå¦‚ Tritonï¼‰èƒ½è‡ªåŠ¨å°†é«˜å±‚æè¿°ç¼–è¯‘ä¸º IO æ„ŸçŸ¥çš„ kernelï¼Œå°†æå¤§é™ä½é—¨æ§›ã€‚"
      },
      {
        "type": "h3",
        "text": "å±€é™äºŒï¼šIO æ„ŸçŸ¥ä¸ä»…é™äºæ³¨æ„åŠ›"
      },
      {
        "type": "p",
        "text": "FlashAttention åªä¼˜åŒ–äº†æ³¨æ„åŠ›æ¨¡å—ï¼Œä½† Transformer ä¸­è¿˜æœ‰å¾ˆå¤š memory-bound æ“ä½œï¼ˆå¦‚ LayerNormã€GeLUã€æ®‹å·®è¿æ¥ç­‰ï¼‰ã€‚è®ºæ–‡å‘¼åå°† IO æ„ŸçŸ¥åŸåˆ™æ¨å¹¿åˆ°æ•´ä¸ªæ¨¡å‹â€”â€”è¿™åæ¥å‚¬ç”Ÿäº†å¤§é‡ã€Œfused kernelã€çš„å·¥ä½œã€‚"
      },
      {
        "type": "h3",
        "text": "åç»­å½±å“ï¼šFlashAttention-2 å’Œ FlashAttention-3"
      },
      {
        "type": "p",
        "text": "FlashAttention-2ï¼ˆ2023ï¼‰é€šè¿‡æ”¹è¿›å¹¶è¡Œç­–ç•¥å’Œå·¥ä½œåˆ†é…ï¼Œè¿›ä¸€æ­¥å°†é€Ÿåº¦æå‡åˆ°æ¥è¿‘ç¡¬ä»¶ç†è®ºå³°å€¼ï¼ˆA100 ä¸Šè¾¾åˆ° 72% çš„ç†è®º FLOPs åˆ©ç”¨ç‡ï¼‰ã€‚FlashAttention-3ï¼ˆ2024ï¼‰é’ˆå¯¹ Hopper æ¶æ„ï¼ˆH100 GPUï¼‰åšäº†ç¡¬ä»¶ç‰¹åŒ–ä¼˜åŒ–ï¼Œåˆ©ç”¨äº† TMAï¼ˆTensor Memory Acceleratorï¼‰å’Œå¼‚æ­¥æ‰§è¡Œã€‚"
      },
      {
        "type": "h3",
        "text": "è¡Œä¸šå½±å“"
      },
      {
        "type": "p",
        "text": "FlashAttention å·²æˆä¸ºç°ä»£å¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„<strong>æ ‡å‡†åŸºç¡€è®¾æ–½</strong>ã€‚å®ƒè¢«é›†æˆåˆ° PyTorch 2.0ï¼ˆtorch.nn.functional.scaled_dot_product_attentionï¼‰ã€HuggingFace Transformersã€vLLMã€TensorRT-LLM ç­‰ä¸»æµæ¡†æ¶ä¸­ã€‚å‡ ä¹æ‰€æœ‰ 2023 å¹´åå‘å¸ƒçš„å¤§æ¨¡å‹ï¼ˆGPT-4ã€LLaMA 2/3ã€Mistralã€Gemma ç­‰ï¼‰éƒ½ä½¿ç”¨äº† FlashAttention æˆ–å…¶å˜ä½“ã€‚"
      },
      {
        "type": "h3",
        "text": "æ›´æ·±å±‚çš„å¯ç¤º"
      },
      {
        "type": "p",
        "text": "FlashAttention æœ€æ·±è¿œçš„å½±å“ä¸æ˜¯æŸä¸ªå…·ä½“çš„åŠ é€Ÿæ¯”ï¼Œè€Œæ˜¯<strong>æ”¹å˜äº†æ·±åº¦å­¦ä¹ ç¤¾åŒºçœ‹å¾…ä¼˜åŒ–çš„æ–¹å¼</strong>ï¼šä»ã€Œå‡å°‘ FLOPsã€è½¬å‘ã€Œå‡å°‘ IOã€ã€‚å®ƒé‡æ–°è¿æ¥äº†æ·±åº¦å­¦ä¹ ä¸ç³»ç»Ÿ/ä½“ç³»ç»“æ„é¢†åŸŸçš„ç»å…¸æ™ºæ…§â€”â€”å†…å­˜å±‚çº§ã€æ•°æ®å±€éƒ¨æ€§ã€Roofline æ¨¡å‹â€”â€”è¿™äº›åŸåˆ™åœ¨ HPC é¢†åŸŸå·²ç»æ˜¯å¸¸è¯†ï¼Œä½†åœ¨ ML ç¤¾åŒºé•¿æœŸè¢«å¿½è§†ã€‚"
      }
    ],
    "thinkAbout": [
      {
        "q": "ä¸ºä»€ä¹ˆä¸ç”¨ Triton ç­‰ç¼–è¯‘å™¨è‡ªåŠ¨ç”Ÿæˆ IO æ„ŸçŸ¥çš„ kernelï¼Ÿ",
        "en": "Our current approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation. This requires writing the attention algorithm in a considerably lower-level language than PyTorch.",
        "zh": "æˆ‘ä»¬å½“å‰æ„å»º IO æ„ŸçŸ¥æ³¨æ„åŠ›å®ç°çš„æ–¹æ³•éœ€è¦ä¸ºæ¯ç§æ–°çš„æ³¨æ„åŠ›å®ç°ç¼–å†™æ–°çš„ CUDA kernelã€‚è¿™è¦æ±‚ç”¨æ¯” PyTorch ä½å¾—å¤šçš„è¯­è¨€ç¼–å†™æ³¨æ„åŠ›ç®—æ³•ã€‚",
        "a": "äº‹å®ä¸Š FlashAttention-2 çš„ä¸€éƒ¨åˆ†ç¡®å®ç”¨ Triton é‡å†™äº†ã€‚ä½†ç¼–è¯‘å™¨éš¾ä»¥è‡ªåŠ¨åšåˆ°çš„æ˜¯ã€Œåœ¨çº¿ softmax + åˆ†å— + é‡è®¡ç®—ã€è¿™ç§è·¨è¶Šå¤šä¸ªæ“ä½œçš„å…¨å±€ä¼˜åŒ–ã€‚ç¼–è¯‘å™¨æ“…é•¿å±€éƒ¨ kernel fusionï¼Œä½† FlashAttention çš„åˆ›æ–°åœ¨äºç®—æ³•å±‚é¢çš„å…¨å±€é‡ç»„ã€‚æœªæ¥çš„æ–¹å‘æ˜¯è®©ç¼–è¯‘å™¨ç†è§£æ›´é«˜å±‚çš„ç®—æ³•æ¨¡å¼ã€‚"
      },
      {
        "q": "FlashAttention çš„åŸåˆ™èƒ½åº”ç”¨åˆ°æ³¨æ„åŠ›ä»¥å¤–çš„æ¨¡å—å—ï¼Ÿ",
        "en": "We believe that our IO-aware approach can be used to speed up each of these modules.",
        "zh": "æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„ IO æ„ŸçŸ¥æ–¹æ³•å¯ä»¥ç”¨äºåŠ é€Ÿè¿™äº›æ¨¡å—ä¸­çš„æ¯ä¸€ä¸ªã€‚",
        "a": "å®Œå…¨å¯ä»¥ã€‚æ ¸å¿ƒåŸåˆ™æ˜¯ã€Œåˆ†å— + SRAM è®¡ç®— + å‡å°‘ HBM è¯»å†™ã€ã€‚åç»­å·¥ä½œå·²å°†ç±»ä¼¼æ€æƒ³åº”ç”¨åˆ° LayerNormï¼ˆFusedLayerNormï¼‰ã€GeLUã€æ®‹å·®è¿æ¥ï¼ˆFlash-Decoding for inferenceï¼‰ã€ç”šè‡³ MLP å±‚ã€‚PyTorch 2.0 çš„ torch.compile ä¹Ÿåœ¨å°è¯•è‡ªåŠ¨å‘ç°å’Œèåˆè¿™äº›æœºä¼šã€‚"
      },
      {
        "q": "FlashAttention æ”¹å˜äº†å“ªäº›å…³äºæ¨¡å‹è®¾è®¡çš„ã€Œå¸¸è¯†ã€ï¼Ÿ",
        "en": "FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models and entirely new capabilities.",
        "zh": "FlashAttention å’Œå—ç¨€ç– FlashAttention ä½¿ Transformer æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œäº§ç”Ÿäº†æ›´é«˜è´¨é‡çš„æ¨¡å‹å’Œå…¨æ–°çš„èƒ½åŠ›ã€‚",
        "a": "è‡³å°‘æ”¹å˜äº†ä¸‰ä¸ªã€Œå¸¸è¯†ã€ï¼š(1) O(NÂ²) ä¸å†æ˜¯ä¸å¯é€¾è¶Šçš„éšœç¢â€”â€”é€šè¿‡ IO ä¼˜åŒ–å¯ä»¥åœ¨å®è·µä¸­å¤„ç†å¾ˆé•¿åºåˆ—ã€‚(2) è¿‘ä¼¼æ³¨æ„åŠ›ä¸æ˜¯å”¯ä¸€å‡ºè·¯â€”â€”ç²¾ç¡®è®¡ç®—+ç³»ç»Ÿä¼˜åŒ–å¯èƒ½æ›´å¥½ã€‚(3) æ¨¡å‹è´¨é‡å¯ä»¥é€šè¿‡ç³»ç»Ÿä¼˜åŒ–æå‡â€”â€”ä¸æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œåªæ˜¯è®©å®ƒå¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡å°±èƒ½è·å¾—å®è´¨æ€§çš„è´¨é‡æå‡ã€‚"
      }
    ],
    "paperRef": "è®ºæ–‡ç¬¬10é¡µ Section 5"
  }
];
const connections = [
  {
    "from": "problem",
    "to": "standard_attn"
  },
  {
    "from": "problem",
    "to": "online_softmax"
  },
  {
    "from": "standard_attn",
    "to": "algorithm"
  },
  {
    "from": "online_softmax",
    "to": "algorithm"
  },
  {
    "from": "algorithm",
    "to": "io_complexity"
  },
  {
    "from": "algorithm",
    "to": "recomputation"
  },
  {
    "from": "algorithm",
    "to": "block_sparse"
  },
  {
    "from": "algorithm",
    "to": "experiments"
  },
  {
    "from": "block_sparse",
    "to": "experiments"
  },
  {
    "from": "experiments",
    "to": "implications"
  },
  {
    "from": "io_complexity",
    "to": "implications"
  }
];
let selectedId = null;
const expandedQs = new Set();
let pagesLoaded = false;

document.getElementById('titleEl').textContent = PAPER_TITLE;
document.title = PAPER_TITLE + ' â€” äº¤äº’å¼é˜…è¯»æŒ‡å—';

function getNode(id){return nodes.find(n=>n.id===id);}

function loadPages(){
  if(pagesLoaded)return;
  pagesLoaded=true;
  const pane=document.getElementById('paperPane');
  pane.innerHTML='';
  for(let i=1;i<=TOTAL_PAGES;i++){
    const w=document.createElement('div');
    w.className='page-wrapper';
    w.id='page-'+i;
    w.innerHTML='<div class="page-label">Page '+i+'</div><img src="'+PAGES_DIR+'/page_'+i+'.png" alt="Page '+i+'" loading="lazy">';
    pane.appendChild(w);
  }
}

function highlightPages(node){
  document.querySelectorAll('.highlight-overlay').forEach(e=>e.remove());
  if(!node||!node.highlights)return;
  node.highlights.forEach(h=>{
    const pw=document.getElementById('page-'+h.page);
    if(!pw)return;
    const ov=document.createElement('div');
    ov.className='highlight-overlay';
    ov.style.top=h.top+'%';
    ov.style.height=h.height+'%';
    ov.style.borderColor=node.color;
    ov.style.background=node.color+'15';
    ov.style.boxShadow='0 0 20px '+node.color+'30';
    pw.appendChild(ov);
  });
  setTimeout(()=>{
    const target=document.getElementById('page-'+node.scrollTo);
    if(target){target.scrollIntoView({behavior:'smooth',block:'start'});}
  },100);
}

function renderMap(){
  const svg=document.getElementById('mapSvg');
  const box=document.getElementById('mapBox');
  let h='';
  connections.forEach(c=>{
    const f=getNode(c.from),t=getNode(c.to);
    if(!f||!t)return;
    h+='<line x1="'+f.x+'%" y1="'+(f.y+3)+'%" x2="'+t.x+'%" y2="'+t.y+'%" stroke="#cbd5e1" stroke-width="1.5" stroke-dasharray="5 3" opacity="0.5"/>';
  });
  svg.innerHTML=h;
  box.querySelectorAll('.node-btn').forEach(e=>e.remove());
  nodes.forEach(n=>{
    const btn=document.createElement('button');
    btn.className='node-btn';
    btn.style.left=n.x+'%';btn.style.top=n.y+'%';
    btn.onclick=()=>selectNode(n.id);
    const a=selectedId===n.id;
    const bg=a?n.color:'white',fg=a?'white':'#1e293b';
    const sh=a?'0 4px 16px '+n.color+'44':'0 1px 4px rgba(0,0,0,0.08)';
    const sc=a?'scale(1.08)':'scale(1)';
    btn.innerHTML='<div class="node-chip" style="background:'+bg+';border-color:'+n.color+';color:'+fg+';box-shadow:'+sh+';transform:'+sc+';"><span class="emoji">'+n.emoji+'</span><span>'+n.label+'</span></div>';
    box.appendChild(btn);
  });
}

function toggleQ(key){
  if(expandedQs.has(key))expandedQs.delete(key);else expandedQs.add(key);
  renderDetail();
}

function renderDetail(){
  const ct=document.getElementById('detailContainer');
  const hint=document.getElementById('emptyHint');
  if(!selectedId){ct.innerHTML='';hint.style.display='block';return;}
  hint.style.display='none';
  const n=getNode(selectedId);
  let ch='';
  n.content.forEach(i=>{
    if(i.type==='h3')ch+='<h3>'+i.text+'</h3>';
    else if(i.type==='p')ch+='<p>'+i.text+'</p>';
    else if(i.type==='bullet')ch+='<div class="bullet"><span class="dot">â€¢</span><span>'+i.text+'</span></div>';
    else if(i.type==='step')ch+='<p class="step-label">'+i.text+'</p>';
  });
  let th='';
  n.thinkAbout.forEach((item,idx)=>{
    const key=n.id+'-'+idx;
    const open=expandedQs.has(key);
    th+='<div>';
    th+='<div class="qa-question" onclick="toggleQ(\''+key+'\')">';
    th+='<span class="arrow">'+(open?'â–¼':'â–¶')+'</span>';
    th+='<span class="qtext">'+item.q+'</span></div>';
    if(open){
      th+='<div class="qa-detail">';
      th+='<div class="qa-block" style="background:#fef3c7;border-color:#f59e0b;"><div class="block-label" style="color:#92400e;">ORIGINAL TEXT</div><p class="block-text" style="color:#78350f;font-style:italic;">'+item.en+'</p></div>';
      th+='<div class="qa-block" style="background:#ecfdf5;border-color:#10b981;"><div class="block-label" style="color:#065f46;">ä¸­æ–‡å­¦æœ¯ç¿»è¯‘</div><p class="block-text" style="color:#064e3b;">'+item.zh+'</p></div>';
      th+='<div class="qa-block" style="background:#eff6ff;border-color:#3b82f6;"><div class="block-label" style="color:#1e40af;">è§£ç­”</div><div class="block-text" style="color:#1e3a5f;">'+item.a+'</div></div>';
      th+='</div>';
    }
    th+='</div>';
  });
  ct.innerHTML=
    '<div class="detail-panel" style="border-color:'+n.color+'40;">'+
    '<div class="detail-header" style="background:'+n.color+'10;">'+
    '<div class="info"><h2 style="color:'+n.color+';">'+n.emoji+' '+n.label+'</h2><div class="section">'+n.section+'</div></div>'+
    '<button class="close-btn" onclick="selectNode(null)">Ã—</button></div>'+
    '<div class="detail-body">'+
    '<div class="key-question" style="background:'+n.color+'08;border:1px solid '+n.color+'20;"><div class="label" style="color:'+n.color+';">æ ¸å¿ƒé—®é¢˜</div><div class="text">'+n.keyQuestion+'</div></div>'+
    '<div class="content-area">'+ch+'</div>'+
    '<div class="think-box"><div class="title">å¸¦ç€è¿™äº›é—®é¢˜å»è¯»åŸæ–‡ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</div>'+th+'</div>'+
    '<div class="paper-ref">'+n.paperRef+'</div></div></div>';
}

function selectNode(id){
  selectedId=(selectedId===id)?null:id;
  expandedQs.clear();
  loadPages();
  renderMap();
  renderDetail();
  highlightPages(selectedId?getNode(selectedId):null);
  if(selectedId){
    setTimeout(()=>{
      const dc=document.getElementById('detailContainer');
      if(dc&&dc.firstChild)dc.firstChild.scrollIntoView({behavior:'smooth',block:'start'});
    },150);
  }
}

// Draggable divider
(function(){
  const div=document.getElementById('divider');
  const pp=document.getElementById('paperPane');
  let dragging=false;
  div.addEventListener('mousedown',e=>{dragging=true;e.preventDefault();});
  document.addEventListener('mousemove',e=>{
    if(!dragging)return;
    const pct=Math.min(70,Math.max(25,e.clientX/window.innerWidth*100));
    pp.style.width=pct+'%';
  });
  document.addEventListener('mouseup',()=>{dragging=false;});
})();

renderMap();
renderDetail();
</script>
</body>
</html>
